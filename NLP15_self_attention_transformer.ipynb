{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP15_self_attention_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzAj0pOb2oDW6a/6VquhQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP15_self_attention_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM82kL0pWm7G",
        "colab_type": "text"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGkSN3R-0Lnj",
        "colab_type": "text"
      },
      "source": [
        "## drive mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lINHQzKzhFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "a307604a-5d5f-4ece-d4d5-bf15015a40ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLG9NZE30Jrr",
        "colab_type": "text"
      },
      "source": [
        "## konlpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoS4M9Kd0IsK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8664d62-585e-4ca9-b3cd-8cdb2897b47d"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Co\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Co\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Co\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [255 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,857 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [884 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,413 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [9,558 B]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,038 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [27.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [116 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,336 kB]\n",
            "Get:23 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [896 kB]\n",
            "Fetched 8,104 kB in 4s (1,982 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "g++ set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre\n",
            "  openjdk-8-jre-headless x11-utils\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin libnss-mdns\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk openjdk-8-jdk-headless\n",
            "  openjdk-8-jre openjdk-8-jre-headless x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 54 not upgraded.\n",
            "Need to get 40.7 MB of archives.\n",
            "After this operation, 153 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java all 0.33.3-20ubuntu0.1 [34.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java-jni amd64 0.33.3-20ubuntu0.1 [28.3 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u265-b01-0ubuntu2~18.04 [27.5 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u265-b01-0ubuntu2~18.04 [69.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u265-b01-0ubuntu2~18.04 [8,262 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u265-b01-0ubuntu2~18.04 [1,610 kB]\n",
            "Fetched 40.7 MB in 3s (12.4 MB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../1-fonts-dejavu-core_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../2-fonts-dejavu-extra_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../3-x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../4-libatk-wrapper-java_0.33.3-20ubuntu0.1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../5-libatk-wrapper-java-jni_0.33.3-20ubuntu0.1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../6-openjdk-8-jre-headless_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../7-openjdk-8-jre_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../8-openjdk-8-jdk-headless_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-8-jdk_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Setting up fonts-dejavu-core (2.37-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Setting up libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up openjdk-8-jdk:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/f7/a368401e630f0e390dd0e62c39fb928e5b23741b53c2360ee7d376660927/JPype1-1.0.2-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.6MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.4MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, beautifulsoup4, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.0.2 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UIz7B4A01Jb",
        "colab_type": "text"
      },
      "source": [
        "# data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNCPGWJjs54f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "DATA_PATH = '/content/drive/My Drive/NLP/dataset/6-1.ChatBotData.csv'\n",
        "VOCABULARY_PATH = '/content/drive/My Drive/NLP/dataset/6-1.vocabulary.voc'\n",
        "TOKENIZE_AS_MORPH = False       # 형태소 분석 여부\n",
        "ENC_INPUT = 0                   # encoder 입력을 의미함\n",
        "DEC_INPUT = 1                   # decoder 입력을 의미함\n",
        "DEC_TARGET = 2                  # decoder 출력을 의미함\n",
        "MAX_SEQUENCE_LEN = 10           # 단어 시퀀스 길이\n",
        "\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "PAD = \"<PADDING>\"\n",
        "STD = \"<START>\"\n",
        "END = \"<END>\"\n",
        "UNK = \"<UNKNOWN>\"\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "# 판다스를 통해서 데이터를 불러와 학습 셋과 평가 셋으로 나누어 그 값을 리턴한다.\n",
        "def load_data():\n",
        "    data_df = pd.read_csv(DATA_PATH, header=0)\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "    \n",
        "    train_input, eval_input, train_label, eval_label = \\\n",
        "        train_test_split(question, answer, test_size=0.1, random_state=42)\n",
        "        \n",
        "    return train_input, train_label, eval_input, eval_label\n",
        "\n",
        "# 형태소 분석\n",
        "# 감성분석이나 문서 분류에는 형태소 분석이 필요하다. 하지만 답변 데이터에 형태소 분석을 \n",
        "# 적용하면 형태소로 답변하게 된다.\n",
        "def prepro_like_morphlized(data):\n",
        "    morph_analyzer = Okt()\n",
        "    result_data = list()\n",
        "    for seq in tqdm(data):\n",
        "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
        "        result_data.append(morphlized_seq)\n",
        "\n",
        "    return result_data\n",
        "\n",
        "# 인코더, 디코더의 입력과 출력 데이터를 생성한다.\n",
        "# 디코더 입력과 타켓에는 앞 뒤에 STD, END가 들어간다.\n",
        "#\n",
        "# 예시:\n",
        "# DEFINES.max_sequence_length = 10 인 경우\n",
        "# 인코더 입력 : \"가끔 궁금해\" -> [9310, 17707, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "# 디코더 입력 : \"그 사람도 그럴 거예요\" -> [STD, 20190, 4221, 13697, 14552, 0, ...]\n",
        "# 디코더 타켓 : [20190, 4221, 13697, 14552, END, 0, ...]\n",
        "def data_processing(value, dictionary, pType):\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if TOKENIZE_AS_MORPH:\n",
        "        value = prepro_like_morphlized(value)\n",
        "\n",
        "    sequences_input_index = []\n",
        "    for sequence in value:\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        \n",
        "        if pType == DEC_INPUT:\n",
        "            # 디코더 입력은 <START>로 시작한다.\n",
        "            sequence_index = [dictionary[STD]]\n",
        "        else:\n",
        "            sequence_index = []\n",
        "        \n",
        "        for word in sequence.split():\n",
        "            # word가 딕셔너리에 없으면 UNK (out of vacabulary)를 넣는다.\n",
        "            if dictionary.get(word) is not None:\n",
        "                sequence_index.append(dictionary[word])\n",
        "            else:\n",
        "                sequence_index.append(dictionary[UNK])\n",
        "        \n",
        "            # 문장의 단어수를  제한한다.\n",
        "            if len(sequence_index) >= MAX_SEQUENCE_LEN:\n",
        "                break\n",
        "        \n",
        "        # 디코더 출력은 <END>로 끝난다.\n",
        "        if pType == DEC_TARGET:\n",
        "            if len(sequence_index) < MAX_SEQUENCE_LEN:\n",
        "                sequence_index.append(dictionary[END])\n",
        "            else:\n",
        "                sequence_index[len(sequence_index)-1] = dictionary[END]\n",
        "                \n",
        "        # max_sequence_length보다 문장 길이가 작으면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE_LEN - len(sequence_index)) * [dictionary[PAD]]\n",
        "        sequences_input_index.append(sequence_index)\n",
        "\n",
        "    return np.asarray(sequences_input_index)\n",
        "\n",
        "# 토크나이징\n",
        "def data_tokenizer(data):\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
        "        for word in sentence.split():\n",
        "            words.append(word)\n",
        "    return [word for word in words if word]\n",
        "\n",
        "# 사전 파일을 만든다\n",
        "def make_vocabulary():\n",
        "    data_df = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "    \n",
        "    # 질문과 응답 문장의 단어를 형태소로 바꾼다\n",
        "    if TOKENIZE_AS_MORPH:  \n",
        "        question = prepro_like_morphlized(question)\n",
        "        answer = prepro_like_morphlized(answer)\n",
        "        \n",
        "    data = []\n",
        "    data.extend(question)\n",
        "    data.extend(answer)\n",
        "    words = data_tokenizer(data)\n",
        "    words = list(set(words)) # 전처리로 좋은 방법이 아니다. 빈도가 높은 순서 쓰려면 - fit_on_texts()\n",
        "    words[:0] = MARKER\n",
        "\n",
        "    word2idx = {word: idx for idx, word in enumerate(words)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(words)}\n",
        "    \n",
        "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다. \n",
        "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
        "    return word2idx, idx2word\n",
        "\n",
        "# 질문과 응답 문장 전체의 단어 목록 dict를 만든다.\n",
        "word2idx, idx2word = make_vocabulary()\n",
        "\n",
        "# 질문과 응답 문장을 학습 데이터와 시험 데이터로 분리한다.\n",
        "train_input, train_label, eval_input, eval_label = load_data()\n",
        "\n",
        "# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "train_input_enc = data_processing(train_input, word2idx, ENC_INPUT)\n",
        "train_input_dec = data_processing(train_label, word2idx, DEC_INPUT)\n",
        "train_target_dec = data_processing(train_label, word2idx, DEC_TARGET)\n",
        "\t\n",
        "# 평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "eval_input_enc = data_processing(eval_input, word2idx, ENC_INPUT)\n",
        "eval_input_dec = data_processing(eval_label, word2idx, DEC_INPUT)\n",
        "eval_target_dec = data_processing(eval_label, word2idx, DEC_TARGET)\n",
        "\n",
        "# 결과를 저장한다.\n",
        "with open('/content/drive/My Drive/NLP/dataset/6-1.vocabulary.pickle', 'wb') as f:\n",
        "    pickle.dump([word2idx, idx2word], f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/My Drive/NLP/dataset/6-1.train_data.pickle', 'wb') as f:\n",
        "    pickle.dump([train_input_enc, train_input_dec, train_target_dec], f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/My Drive/NLP/dataset/6-1.eval_data.pickle', 'wb') as f:\n",
        "    pickle.dump([eval_input_enc, eval_input_dec, eval_target_dec], f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgxLUwCN0_HR",
        "colab_type": "text"
      },
      "source": [
        "# train - 수업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvo7IpyTevFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dot\n",
        "from tensorflow.keras.layers import Activation, Concatenate\n",
        "from tensorflow.keras.layers import Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDMcax4nzzrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Attention을 이용한 ChatBot : 학습 모듈\n",
        "#\n",
        "# 2020.06.04 : 조성현 (blog.naver.com/chunjein)\n",
        "# ---------------------------------------------\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dot\n",
        "from tensorflow.keras.layers import Activation, Concatenate\n",
        "from tensorflow.keras.layers import Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# 단어 목록 dict를 읽어온다.\n",
        "with open('./dataset/6-1.vocabulary.pickle', 'rb') as f:\n",
        "    word2idx,  idx2word = pickle.load(f)\n",
        "    \n",
        "# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.\n",
        "with open('./dataset/6-1.train_data.pickle', 'rb') as f:\n",
        "    trainXE, trainXD, trainYD = pickle.load(f)\n",
        "\t\n",
        "# 평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "with open('./dataset/6-1.eval_data.pickle', 'rb') as f:\n",
        "    testXE, testXD, testYD = pickle.load(f)\n",
        "\n",
        "VOCAB_SIZE = len(idx2word)\n",
        "EMB_SIZE = 128\n",
        "LSTM_HIDDEN = 128\n",
        "MODEL_PATH = './dataset/6-6.Attention.h5'\n",
        "LOAD_MODEL = True\n",
        "\n",
        "# Encoder 출력과 decoder 출력으로 attention value를 생성하고,\n",
        "# decoder 출력 + attention value (concatenate)를 리턴한다.\n",
        "# x : encoder 출력, y : decoder 출력\n",
        "# LSTM time step = 4, SMB_SIZE = 3 이라면 각 텐서의 dimension은\n",
        "# 아래 주석과 같다.\n",
        "def Attention(x, y):\n",
        "    # step-1:\n",
        "    # decoder의 매 시점마다 encoder의 전체 시점과 dot-product을 수행한다.\n",
        "    score = Dot(axes=(2, 2))([y, x])                   # (1, 4, 4)\n",
        "    \n",
        "    # step-2:\n",
        "    # dot-product 결과를 확률분포로 만든다 (softmax)\n",
        "    # 이것이 attention score이다.\n",
        "    dist = Activation('softmax')(score)                # (1, 4, 4)\n",
        "\n",
        "    # step-3:    \n",
        "    # encoder의 전체 시점에 위의 확률 분포를 적용해서 가중 평균한다.\n",
        "    # 직접 계산이 어렵기 때문에 dist를 확장하고, 열을 복제해서\n",
        "    # Dot 연산이 가능하도록 trick을 쓴다.\n",
        "    # 이것이 attention value이다.\n",
        "    # dist_exp = K.expand_dims(dist, 2)                   # (1, 4, 1, 4)\n",
        "    # dist_rep = K.repeat_elements(dist_exp, EMB_SIZE, 2) # (1, 4, 3, 4)                                       \n",
        "    # dist_dot = Dot(axes=(3, 1))([dist_rep, x])          # (1, 4, 3, 3)\n",
        "    # attention = K.mean(dist_dot, axis = 2)              # (1, 4, 3)\n",
        "\n",
        "    # step-4:\n",
        "    # 교재의 step-3을 계산하지 않고 step-4를 직접 계산했다.\n",
        "    attention = Dot(axes=(2, 1))([dist, x])\n",
        "    \n",
        "    # step-5:\n",
        "    # decoder 출력과 attention을 concatenate 한다.\n",
        "    return Concatenate()([y, attention])    # (1, 4, 6)\n",
        "    \n",
        "# 워드 임베딩 레이어. Encoder와 decoder에서 공동으로 사용한다.\n",
        "K.clear_session()\n",
        "wordEmbedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)\n",
        "\n",
        "# Encoder\n",
        "# -------\n",
        "# many-to-many로 구성한다. Attention value를 계산하기 위해 중간 출력이 필요하고\n",
        "# (return_sequences=True), decoder로 전달할 h와 c도 필요하다 (return_state = True)\n",
        "encoderX = Input(batch_shape=(None, trainXE.shape[1]))\n",
        "encEMB = wordEmbedding(encoderX)\n",
        "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "encLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "ey1, eh1, ec1 = encLSTM1(encEMB)    # LSTM 1층 \n",
        "ey2, eh2, ec2 = encLSTM2(ey1)       # LSTM 2층\n",
        "\n",
        "# Decoder\n",
        "# -------\n",
        "# many-to-many로 구성한다. target을 학습하고 Attention을 위해서는 중간 출력이 \n",
        "# 필요하다. 그리고 초기 h와 c는 encoder에서 출력한 값을 사용한다 (initial_state)\n",
        "# 최종 출력은 vocabulary의 인덱스인 one-hot 인코더이다.\n",
        "decoderX = Input(batch_shape=(None, trainXD.shape[1]))\n",
        "decEMB = wordEmbedding(decoderX)\n",
        "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
        "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
        "att_dy2 = Attention(ey2, dy2)\n",
        "decOutput = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "outputY = decOutput(att_dy2)\n",
        "\n",
        "# Model\n",
        "# -----\n",
        "# target이 one-hot encoding되어 있으면 categorical_crossentropy\n",
        "# target이 integer로 되어 있으면 sparse_categorical_crossentropy를 쓴다.\n",
        "# sparse_categorical_entropy는 integer인 target을 one-hot으로 바꾼 후에\n",
        "# categorical_entropy를 수행한다.\n",
        "model = Model([encoderX, decoderX], outputY)\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001), \n",
        "              loss='sparse_categorical_crossentropy')\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    model.load_weights(MODEL_PATH)\n",
        "\n",
        "# 학습 (teacher forcing)\n",
        "# ----------------------\n",
        "# loss = sparse_categorical_crossentropy이기 때문에 target을 one-hot으로 변환할\n",
        "# 필요 없이 integer인 trainYD를 그대로 넣어 준다. trainYD를 one-hot으로 변환해서\n",
        "# categorical_crossentropy로 처리하면 out-of-memory 문제가 발생할 수 있다.\n",
        "hist = model.fit([trainXE, trainXD], trainYD, batch_size = 300, \n",
        "                 epochs=1, shuffle=True,\n",
        "                 validation_data = ([testXE, testXD], testYD))\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 학습 결과를 저장한다\n",
        "model.save_weights(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OA8zb3EW8BN",
        "colab_type": "text"
      },
      "source": [
        "# input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EroNbt8a2Bv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/NLP/dataset/6-1.vocabulary.pickle', 'rb') as f:\n",
        "    word2idx,  idx2word = pickle.load(f)\n",
        "    \n",
        "# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.\n",
        "with open('/content/drive/My Drive/NLP/dataset/6-1.train_data.pickle', 'rb') as f:\n",
        "    trainXE, trainXD, trainYD = pickle.load(f)\n",
        "\t\n",
        "# 평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "with open('/content/drive/My Drive/NLP/dataset/6-1.eval_data.pickle', 'rb') as f:\n",
        "    testXE, testXD, testYD = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXRvvJ7d2RPA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2021c090-ba0a-4336-9777-fd7dbc599fcb"
      },
      "source": [
        "len(word2idx), len(idx2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20705, 20705)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckEcMMeVC3dI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62b073aa-83d8-4afd-fc3c-7ba426fb245a"
      },
      "source": [
        "idx2word[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'좋아하시니까'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1tJehDC60X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9473d2b-43e7-4ca5-d3bc-81af0338bc9f"
      },
      "source": [
        "word2idx['좋아하시니까']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABG-YhTO2ceY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d718533-637d-4d73-b8b9-d31dc5e715f2"
      },
      "source": [
        "type(trainXE), trainXE.shape, trainXD.shape, trainYD.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, (10640, 10), (10640, 10), (10640, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgrDvrr62fDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "127acf5b-7b7a-4410-fdc3-106d2eff0020"
      },
      "source": [
        "testXE.shape, testXD.shape, testYD.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1183, 10), (1183, 10), (1183, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcLRTZTQ2u96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f425722b-53df-40c6-fb2c-0484286f92e1"
      },
      "source": [
        "trainXE[0], trainYD[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([6010, 6177, 7923, 5727,    0,    0,    0,    0,    0,    0]),\n",
              " array([ 9684, 17124,  9424,  7956,  3637,  2566,   523,     2,     0,\n",
              "            0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASijJGDKpZBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5447b81c-6776-427b-e13b-8613cd2e9e3e"
      },
      "source": [
        "trainXE[0].reshape(1,-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6010, 6177, 7923, 5727,    0,    0,    0,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f15VppFjpTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9f3f3c05-b310-4ca7-ecda-2b26dd11f11e"
      },
      "source": [
        "qqq = list(trainXE[0])\n",
        "kkk = list(trainYD[0])\n",
        "qqq,kkk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([6010, 6177, 7923, 5727, 0, 0, 0, 0, 0, 0],\n",
              " [9684, 17124, 9424, 7956, 3637, 2566, 523, 2, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlm-MCrfi_4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "f7b99269-6d34-4752-b5b2-470b41373ede"
      },
      "source": [
        "q = []\n",
        "k = []\n",
        "for x, y in zip(qqq,kkk):\n",
        "    q.append(idx2word[x])\n",
        "    k.append(idx2word[y])\n",
        "q, k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['나한테',\n",
              "  '질리면',\n",
              "  '어쩌지',\n",
              "  '걱정돼',\n",
              "  '<PADDING>',\n",
              "  '<PADDING>',\n",
              "  '<PADDING>',\n",
              "  '<PADDING>',\n",
              "  '<PADDING>',\n",
              "  '<PADDING>'],\n",
              " ['당신의',\n",
              "  '겉모습이',\n",
              "  '아닌',\n",
              "  '진정한',\n",
              "  '내면의',\n",
              "  '모습을',\n",
              "  '보여주세요',\n",
              "  '<END>',\n",
              "  '<PADDING>',\n",
              "  '<PADDING>'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUhxvjEijRBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aXSQvZe_bZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dd5c2901-3446-447a-ecee-d573e97a7ba8"
      },
      "source": [
        "trainXE.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-6ESmz2WBBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMB_SIZE = 128\n",
        "MAX_SEQUENCE_LEN = trainXE.shape[1] # 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOn1w90DXAKc",
        "colab_type": "text"
      },
      "source": [
        "# model build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTh6EY6vEnlG",
        "colab_type": "text"
      },
      "source": [
        "## positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kmGzOiPfLFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIxcbdhLEqY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpYiC5s8eXcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6d8b07a-49b5-4079-f9b2-da13e59c607a"
      },
      "source": [
        "pe = positional_encoding(10,128)\n",
        "pe.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Eik2mKyhwKW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "fde67d4b-ff08-4c77-b9d6-53fe52063510"
      },
      "source": [
        "plt.pcolormesh(pe[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0,128))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7vHxRY0Euqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "128, 10 맞음. maximum_position_encoding=10, dimension = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjTts5JmXxOv",
        "colab_type": "text"
      },
      "source": [
        "## masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzPZCO6xh5x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMreVxRC4Xa0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "55a687b3-f3cd-4772-9844-5a4323888aec"
      },
      "source": [
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_5cLUFX4cyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHzMBm4Y4dRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "97091102-70ac-4ff2-d431-55743e863950"
      },
      "source": [
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoQcCge4XOwi",
        "colab_type": "text"
      },
      "source": [
        "## scaled dot product attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHK_D567dIn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask): # (None, seq=10, num=heads=8, False)\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DG3Vls7iixp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnDyUiA0ip5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "287d01b0-f620-40f4-d175-5d43ab04143d"
      },
      "source": [
        "print_out(trainXE[0].reshape(1,-1),trainXD[0].reshape(1,-1),trainXD[0].reshape(1,-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0mr_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__r%s__\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m           (dtype.name, value.dtype.name, value))\n\u001b[0m\u001b[1;32m   1476\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype float32 for Tensor with dtype int64: <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[249468778]])>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-36c112b5e5ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainXE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainXD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainXD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-f2beecdd1da3>\u001b[0m in \u001b[0;36mprint_out\u001b[0;34m(q, k, v)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   temp_out, temp_attn = scaled_dot_product_attention(\n\u001b[0;32m----> 3\u001b[0;31m       q, k, v, None)\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Attention weights are:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-d4bc96e7f9bb>\u001b[0m in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(q, k, v, mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# scale matmul_qk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mdk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mscaled_attention_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul_qk\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m# add the mask to the scaled tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mtruediv\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m   \"\"\"\n\u001b[0;32m-> 1297\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_truediv_python3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_truediv_python3\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m       raise TypeError(\"x and y must have the same dtype, got %r != %r\" %\n\u001b[0;32m-> 1228\u001b[0;31m                       (x_dtype, y_dtype))\n\u001b[0m\u001b[1;32m   1229\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TRUEDIV_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_dtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: x and y must have the same dtype, got tf.int64 != tf.float32"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1AQcZg4ilxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "df407ca4-fb4b-4129-fe67-d23470a01834"
      },
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jJvy2fN4xOF",
        "colab_type": "text"
      },
      "source": [
        "### multi head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoY3UbAl4z6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmLoLbek42cq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ea57a33-2c97-45dc-c02a-0914ce74e836"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTA8xBweXROD",
        "colab_type": "text"
      },
      "source": [
        "## point wise feed forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXaV1JLVtTVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wfK0jGgtTrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c3113d7-c5ca-4de8-cecc-94d7d44ced62"
      },
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oghAw2XkXUH8",
        "colab_type": "text"
      },
      "source": [
        "## encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N19jMWXc_Jnf",
        "colab_type": "text"
      },
      "source": [
        "### encoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Uz5SV6_SvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dimemsion 이 임베딩 개수 128\n",
        "num_heads 가 8\n",
        "EncoderLayer(10, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3LxGUFg_K_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vLqNMWB_L3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7415d6bc-1c05-4d0a-e759-bc6aae63f1ab"
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Nuh37SX1c2",
        "colab_type": "text"
      },
      "source": [
        "### decoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q-At9p92M0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder layer\n",
        "# Each decoder layer consists of sublayers:\n",
        "\n",
        "# Masked multi-head attention (with look ahead mask and padding mask)\n",
        "# Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublayer.\n",
        "# Point wise feed forward networks\n",
        "# Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis.\n",
        "\n",
        "# There are N decoder layers in the transformer.\n",
        "\n",
        "# As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtY_GwYc5Dvv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5f8665de-b000-488a-ebcc-94bc47a55837"
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvY3XIn-81cb",
        "colab_type": "text"
      },
      "source": [
        "### encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvrhPPfg840m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFKArayC84ba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9632fce7-17af-4be1-f8ee-bcdb3bfbb573"
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMGTUFS7X4H-",
        "colab_type": "text"
      },
      "source": [
        "### decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re1l30Wh2Wt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS8QQ2Ls2XOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "917d1467-f52b-4625-be45-d325cea25ddc"
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input, \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False,\n",
        "                              look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QL6D7d8X50D",
        "colab_type": "text"
      },
      "source": [
        "## create the transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQKl8YJ_2bNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcHmCT4s2b2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a936b70e-abab-4b7c-d631-ae1936feddc4"
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500, target_vocab_size=8000, \n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 36, 8000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UHCcKhCX7z-",
        "colab_type": "text"
      },
      "source": [
        "## set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96hQyBc22h7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# num_layers = 4\n",
        "# d_model = 128\n",
        "# dff = 512\n",
        "# num_heads = 8\n",
        "\n",
        "# input_vocab_size = tokenizer_pt.vocab_size + 2\n",
        "# target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "# dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ4SdErcX9U3",
        "colab_type": "text"
      },
      "source": [
        "## optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyze2r0w2l2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxybqsvZ2mfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEEYXm3j2pdI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9e4854d7-b5de-453d-8513-d09b2f28f5ed"
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZz2ka4LX-89",
        "colab_type": "text"
      },
      "source": [
        "## lost and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwh-1fs_2sHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYZAovb22scy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEuqrUkF2tum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfOqARg-YAYl",
        "colab_type": "text"
      },
      "source": [
        "## training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ4K0eUR3XVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 2\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "\n",
        "input_vocab_size = 10\n",
        "target_vocab_size = 10\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA0cpYEF3j4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# num_layers = 4\n",
        "# d_model = 128\n",
        "# dff = 512\n",
        "# num_heads = 8\n",
        "\n",
        "# input_vocab_size = tokenizer_pt.vocab_size + 2\n",
        "# target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "# dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvvqslWS2wzT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "4bc615e2-51de-4505-b2f7-9bd270de62e4"
      },
      "source": [
        "test_transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "\n",
        "a, b = test_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "a, b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-b42c33999b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                dec_padding_mask=None)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-86245da83a7c>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m            look_ahead_mask, dec_padding_mask):\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, inp_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-c17e3d03cb78>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# adding embedding and position encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup_v2\u001b[0;34m(params, ids, max_norm, name)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m   \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    136\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         result = _clip(\n\u001b[0;32m--> 138\u001b[0;31m             array_ops.gather(params[0], ids, name=name), ids, max_norm)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4674\u001b[0m     \u001b[0;31m# TODO(apassos) find a less bad way of detecting resource variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4675\u001b[0m     \u001b[0;31m# without introducing a circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4676\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4677\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4678\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36msparse_read\u001b[0;34m(self, indices, name)\u001b[0m\n\u001b[1;32m    686\u001b[0m       \u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m       value = gen_resource_variable_ops.resource_gather(\n\u001b[0;32m--> 688\u001b[0;31m           self._handle, indices, dtype=self._dtype, name=name)\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_gather\u001b[0;34m(resource, indices, dtype, batch_dims, validate_indices, name)\u001b[0m\n\u001b[1;32m    554\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[56,0] = 55 is not in [0, 10) [Op:ResourceGather]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXjFuKwCZfB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdL7wh_kYCzS",
        "colab_type": "text"
      },
      "source": [
        "## evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkiyAQIoYE5V",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}