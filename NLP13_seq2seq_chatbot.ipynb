{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP13_8_11_seq2seq_chatbot.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM1ftX9Ocvuja7EZrVLUkr4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP13_seq2seq_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcDvax2bdFp6",
        "colab_type": "text"
      },
      "source": [
        "# seq2seq(Sequence to sequence) Model\n",
        "\n",
        "- tf-seq2seq is a general-purpose encoder-decoder framework for Tensorflow that can be used for Machine Translation, Text Summarization, Conversational Modeling, Image Captioning, and more.   \n",
        "[read more](https://google.github.io/seq2seq/)\n",
        "\n",
        "\n",
        "- 시퀀스-투-시퀀스(Sequence-to-Sequence)는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델이다.\n",
        "    - 예를 들어 챗봇(Chatbot)과 기계 번역(Machine Translation)이 그러한 대표적인 예인데, 입력 시퀀스와 출력 시퀀스를 각각 질문과 대답으로 구성하면 챗봇으로 만들 수 있고, 입력 시퀀스와 출력 시퀀스를 각각 입력 문장과 번역 문장으로 만들면 번역기로 만들 수 있습니다. 그 외에도 내용 요약(Text Summarization), STT(Speech to Text) 등에서 쓰일 수 있습니다.   \n",
        "[참조](https://wikidocs.net/24996)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI3oLf32eBQZ",
        "colab_type": "text"
      },
      "source": [
        "## 논문 참조\n",
        "\n",
        "**Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation(SMT)** by `Kyunghyun Cho` and `3 others`\n",
        "\n",
        "[click here](https://arxiv.org/abs/1406.1078)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlVvAJwDiIg9",
        "colab_type": "text"
      },
      "source": [
        "### model look-up\n",
        "\n",
        "![nlp2](https://drive.google.com/uc?view=export&id=1FS0Vj5Pnpmy-ndfiZs6S_qv2qA_rq2nT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie0QLwytfjR1",
        "colab_type": "text"
      },
      "source": [
        "# Practice - A Korean Chatbot using seq2seq model\n",
        "\n",
        "csv file can be found [here](https://github.com/seunghyunmoon2/NLP/tree/master/dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OIc21K2f00n",
        "colab_type": "text"
      },
      "source": [
        "## [대화 data](https://github.com/songys/Chatbot_data)를 이용해서 사용자와 대화를 하는 챗봇을 만든다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsipTaNFheH1",
        "colab_type": "text"
      },
      "source": [
        "### exploratory table\n",
        "\n",
        "- **label**은 대화의 상황 카테고리를 나타낸다.\n",
        "    - ex)일상대화 : 0, 카페 주문 상황: 1, ...\n",
        "\n",
        "![nlp1](https://drive.google.com/uc?view=export&id=10ty7a1JKU5tY8VKTzf6Syo_2C9yP6E9t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y01s3pBdgQ9C",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPPH4jfaEHbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "DATA_PATH = './dataset/6-1.ChatBotData.csv'\n",
        "VOCABULARY_PATH = './dataset/6-1.vocabulary.voc'\n",
        "TOKENIZE_AS_MORPH = False       # 형태소 분석 여부\n",
        "ENC_INPUT = 0                   # encoder 입력을 의미함\n",
        "DEC_INPUT = 1                   # decoder 입력을 의미함\n",
        "DEC_TARGET = 2                  # decoder 출력을 의미함\n",
        "MAX_SEQUENCE_LEN = 10           # 단어 시퀀스 길이\n",
        "\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "PAD = \"<PADDING>\"\n",
        "STD = \"<START>\"\n",
        "END = \"<END>\"\n",
        "UNK = \"<UNKNOWN>\"\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "# 판다스를 통해서 데이터를 불러와 학습 셋과 평가 셋으로 나누어 그 값을 리턴한다.\n",
        "def load_data():\n",
        "    data_df = pd.read_csv(DATA_PATH, header=0)\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "    \n",
        "    train_input, eval_input, train_label, eval_label = \\\n",
        "        train_test_split(question, answer, test_size=0.1, random_state=42)\n",
        "        \n",
        "    return train_input, train_label, eval_input, eval_label\n",
        "\n",
        "# 형태소 분석\n",
        "# 감성분석이나 문서 분류에는 형태소 분석이 필요하다. 하지만 답변 데이터에 형태소 분석을 \n",
        "# 적용하면 형태소로 답변하게 된다.\n",
        "def prepro_like_morphlized(data):\n",
        "    morph_analyzer = Okt()\n",
        "    result_data = list()\n",
        "    for seq in tqdm(data):\n",
        "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
        "        result_data.append(morphlized_seq)\n",
        "\n",
        "    return result_data\n",
        "\n",
        "# 인코더, 디코더의 입력과 출력 데이터를 생성한다.\n",
        "# 디코더 입력과 타켓에는 앞 뒤에 STD, END가 들어간다.\n",
        "#\n",
        "# 예시:\n",
        "# DEFINES.max_sequence_length = 10 인 경우\n",
        "# 인코더 입력 : \"가끔 궁금해\" -> [9310, 17707, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "# 디코더 입력 : \"그 사람도 그럴 거예요\" -> [STD, 20190, 4221, 13697, 14552, 0, ...]\n",
        "# 디코더 타켓 : [20190, 4221, 13697, 14552, END, 0, ...]\n",
        "def data_processing(value, dictionary, pType):\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if TOKENIZE_AS_MORPH:\n",
        "        value = prepro_like_morphlized(value)\n",
        "\n",
        "    sequences_input_index = []\n",
        "    for sequence in value:\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        \n",
        "        if pType == DEC_INPUT:\n",
        "            # 디코더 입력은 <START>로 시작한다.\n",
        "            sequence_index = [dictionary[STD]]\n",
        "        else:\n",
        "            sequence_index = []\n",
        "        \n",
        "        for word in sequence.split():\n",
        "            # word가 딕셔너리에 없으면 UNK (out of vacabulary)를 넣는다.\n",
        "            if dictionary.get(word) is not None:\n",
        "                sequence_index.append(dictionary[word])\n",
        "            else:\n",
        "                sequence_index.append(dictionary[UNK])\n",
        "        \n",
        "            # 문장의 단어수를  제한한다.\n",
        "            if len(sequence_index) >= MAX_SEQUENCE_LEN:\n",
        "                break\n",
        "        \n",
        "        # 디코더 출력은 <END>로 끝난다.\n",
        "        if pType == DEC_TARGET:\n",
        "            if len(sequence_index) < MAX_SEQUENCE_LEN:\n",
        "                sequence_index.append(dictionary[END])\n",
        "            else:\n",
        "                sequence_index[len(sequence_index)-1] = dictionary[END]\n",
        "                \n",
        "        # max_sequence_length보다 문장 길이가 작으면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE_LEN - len(sequence_index)) * [dictionary[PAD]]\n",
        "        sequences_input_index.append(sequence_index)\n",
        "\n",
        "    return np.asarray(sequences_input_index)\n",
        "\n",
        "# 토크나이징\n",
        "def data_tokenizer(data):\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
        "        for word in sentence.split():\n",
        "            words.append(word)\n",
        "    return [word for word in words if word]\n",
        "\n",
        "# 사전 파일을 만든다\n",
        "def make_vocabulary():\n",
        "    data_df = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "    \n",
        "    # 질문과 응답 문장의 단어를 형태소로 바꾼다\n",
        "    if TOKENIZE_AS_MORPH:  \n",
        "        question = prepro_like_morphlized(question)\n",
        "        answer = prepro_like_morphlized(answer)\n",
        "        \n",
        "    data = []\n",
        "    data.extend(question)\n",
        "    data.extend(answer)\n",
        "    words = data_tokenizer(data)\n",
        "    words = list(set(words)) # 전처리로 좋은 방법이 아니다. 빈도가 높은 순서 쓰려면 - fit_on_texts()\n",
        "    words[:0] = MARKER\n",
        "\n",
        "    word2idx = {word: idx for idx, word in enumerate(words)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(words)}\n",
        "    \n",
        "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다. \n",
        "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
        "    return word2idx, idx2word\n",
        "\n",
        "# 질문과 응답 문장 전체의 단어 목록 dict를 만든다.\n",
        "word2idx, idx2word = make_vocabulary()\n",
        "\n",
        "# 질문과 응답 문장을 학습 데이터와 시험 데이터로 분리한다.\n",
        "train_input, train_label, eval_input, eval_label = load_data()\n",
        "\n",
        "# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "train_input_enc = data_processing(train_input, word2idx, ENC_INPUT)\n",
        "train_input_dec = data_processing(train_label, word2idx, DEC_INPUT)\n",
        "train_target_dec = data_processing(train_label, word2idx, DEC_TARGET)\n",
        "\t\n",
        "# 평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "eval_input_enc = data_processing(eval_input, word2idx, ENC_INPUT)\n",
        "eval_input_dec = data_processing(eval_label, word2idx, DEC_INPUT)\n",
        "eval_target_dec = data_processing(eval_label, word2idx, DEC_TARGET)\n",
        "\n",
        "# 결과를 저장한다.\n",
        "with open('./dataset/6-1.vocabulary.pickle', 'wb') as f:\n",
        "    pickle.dump([word2idx, idx2word], f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('./dataset/6-1.train_data.pickle', 'wb') as f:\n",
        "    pickle.dump([train_input_enc, train_input_dec, train_target_dec], f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('./dataset/6-1.eval_data.pickle', 'wb') as f:\n",
        "    pickle.dump([eval_input_enc, eval_input_dec, eval_target_dec], f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbU_c5WSirIL",
        "colab_type": "text"
      },
      "source": [
        "* Dataset ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrhgH7wzixLI",
        "colab_type": "text"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVL7vhNAi08u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seq2Seq를 이용한 ChatBot : 학습 모듈\n",
        "# 참고한 자료 :\n",
        "# https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
        "#\n",
        "# 2020.06.04 : 조성현 (blog.naver.com/chunjein)\n",
        "# ------------------------------------------------------------------------\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.layers import Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# 단어 목록 dict를 읽어온다.\n",
        "with open('./dataset/6-1.vocabulary.pickle', 'rb') as f:\n",
        "    word2idx,  idx2word = pickle.load(f)\n",
        "    \n",
        "# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.\n",
        "with open('./dataset/6-1.train_data.pickle', 'rb') as f:\n",
        "    trainXE, trainXD, trainYD = pickle.load(f)\n",
        "\t\n",
        "# 평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "with open('./dataset/6-1.eval_data.pickle', 'rb') as f:\n",
        "    testXE, testXD, testYD = pickle.load(f)\n",
        "\n",
        "VOCAB_SIZE = len(idx2word)\n",
        "EMB_SIZE = 128\n",
        "LSTM_HIDDEN = 128\n",
        "MODEL_PATH = './dataset/6-2.Seq2Seq.h5'\n",
        "LOAD_MODEL = False\n",
        "\n",
        "# 워드 임베딩 레이어. Encoder와 decoder에서 공동으로 사용한다.\n",
        "K.clear_session()\n",
        "wordEmbedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)\n",
        "\n",
        "# Encoder\n",
        "# -------\n",
        "# many-to-one으로 구성한다. 중간 출력은 필요 없고 decoder로 전달할 h와 c만\n",
        "# 필요하다. h와 c를 얻기 위해 return_state = True를 설정한다.\n",
        "encoderX = Input(batch_shape=(None, trainXE.shape[1]))\n",
        "encEMB = wordEmbedding(encoderX)\n",
        "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True) # 1층 중간출력 2층으로 전달할 목적\n",
        "encLSTM2 = LSTM(LSTM_HIDDEN, return_state = True) # 2층 \n",
        "ey1, eh1, ec1 = encLSTM1(encEMB)    # LSTM 1층 \n",
        "_, eh2, ec2 = encLSTM2(ey1)         # LSTM 2층\n",
        "\n",
        "# Decoder\n",
        "# -------\n",
        "# many-to-many로 구성한다. target을 학습하기 위해서는 중간 출력이 필요하다.\n",
        "# 그리고 초기 h와 c는 encoder에서 출력한 값을 사용한다 (initial_state)\n",
        "# 최종 출력은 vocabulary의 인덱스인 one-hot 인코더이다.\n",
        "decoderX = Input(batch_shape=(None, trainXD.shape[1]))\n",
        "decEMB = wordEmbedding(decoderX)\n",
        "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
        "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
        "decOutput = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "outputY = decOutput(dy2)  # 각 층이 VOCAB_SIZE 만큼 원한인코딩되어나온다. ex) 그래서 의 wv : [0,0, ... ,1,,  ,0, 0]\n",
        "\n",
        "# Model\n",
        "# -----\n",
        "# target이 one-hot encoding되어 있으면 categorical_crossentropy\n",
        "# target이 integer로 되어 있으면 sparse_categorical_crossentropy를 쓴다.\n",
        "# sparse_categorical_entropy는 integer인 target을 one-hot으로 바꾼 후에\n",
        "# categorical_entropy를 수행한다.\n",
        "model = Model([encoderX, decoderX], outputY)\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001), \n",
        "              loss='sparse_categorical_crossentropy')\n",
        "# sparse 말고 그냥 CCE쓰려면 outputY를 to_categorical()로 병형해준 후 \"CCE\" 사용한다.\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    model.load_weights(MODEL_PATH)\n",
        "    \n",
        "# 학습 (teacher forcing)\n",
        "# ----------------------\n",
        "# loss = sparse_categorical_crossentropy이기 때문에 target을 one-hot으로 변환할\n",
        "# 필요 없이 integer인 trainYD를 그대로 넣어 준다. trainYD를 one-hot으로 변환해서\n",
        "# categorical_crossentropy로 처리하면 out-of-memory 문제가 발생할 수 있다.\n",
        "hist = model.fit([trainXE, trainXD], trainYD, batch_size = 300, \n",
        "                 epochs=100, shuffle=True,\n",
        "                 validation_data = ([testXE, testXD], testYD))\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 학습 결과를 저장한다\n",
        "model.save_weights(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM7vM3LM51IK",
        "colab_type": "text"
      },
      "source": [
        "- 학습은 잘 시킨다고 치고, 예측은 어떻게 하느냐? decoder에 입력이 없잖아...?\n",
        "-> 입력을 한개씩 해준다. 리커런트가 아니고... 타임시리즈 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA4pBkBEi1fq",
        "colab_type": "text"
      },
      "source": [
        "## chat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U95OZurOi1rK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seq2Seq를 이용한 ChatBot : 채팅 모듈\n",
        "# 참고한 자료 :\n",
        "# https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
        "#\n",
        "# 2020.06.04 : 조성현 (blog.naver.com/chunjein)\n",
        "# ------------------------------------------------------------------------\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.layers import Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# 단어 목록 dict를 읽어온다.\n",
        "with open('./dataset/6-1.vocabulary.pickle', 'rb') as f:\n",
        "    word2idx,  idx2word = pickle.load(f)\n",
        "\n",
        "VOCAB_SIZE = len(idx2word)\n",
        "EMB_SIZE = 128\n",
        "LSTM_HIDDEN = 128\n",
        "MAX_SEQUENCE_LEN = 10            # 단어 시퀀스 길이\n",
        "MODEL_PATH = './dataset/6-2.Seq2Seq.h5'\n",
        "\n",
        "# 워드 임베딩 레이어. Encoder와 decoder에서 공동으로 사용한다.\n",
        "K.clear_session()\n",
        "wordEmbedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)\n",
        "\n",
        "# Encoder\n",
        "# -------\n",
        "encoderX = Input(batch_shape=(None, MAX_SEQUENCE_LEN))\n",
        "encEMB = wordEmbedding(encoderX)\n",
        "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "encLSTM2 = LSTM(LSTM_HIDDEN, return_state = True)\n",
        "ey1, eh1, ec1 = encLSTM1(encEMB)    # LSTM 1층 \n",
        "_, eh2, ec2 = encLSTM2(ey1)         # LSTM 2층\n",
        "\n",
        "# Decoder\n",
        "# -------\n",
        "# Decoder는 1개 단어씩을 입력으로 받는다. 학습 때와 달리 문장 전체를 받아\n",
        "# recurrent하는 것이 아니라, 단어 1개씩 입력 받아서 다음 예상 단어를 확인한다.\n",
        "# chatting()에서 for 문으로 단어 별로 recurrent 시킨다.\n",
        "# 따라서 batch_shape = (None, 1)이다. 즉, time_step = 1이다. 그래도 네트워크\n",
        "# 파라메터는 동일하다.\n",
        "decoderX = Input(batch_shape=(None, 1))\n",
        "decEMB = wordEmbedding(decoderX)\n",
        "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
        "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
        "decOutput = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "outputY = decOutput(dy2)\n",
        "\n",
        "# Model\n",
        "# -----\n",
        "model = Model([encoderX, decoderX], outputY)\n",
        "model.load_weights(MODEL_PATH)\n",
        "\n",
        "# Chatting용 model\n",
        "model_enc = Model(encoderX, [eh1, ec1, eh2, ec2])\n",
        "\n",
        "ih1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ic1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ih2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ic2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "\n",
        "dec_output1, dh1, dc1 = decLSTM1(decEMB, initial_state = [ih1, ic1])\n",
        "dec_output2, dh2, dc2 = decLSTM2(dec_output1, initial_state = [ih2, ic2])\n",
        "\n",
        "dec_output = decOutput(dec_output2)\n",
        "model_dec = Model([decoderX, ih1, ic1, ih2, ic2], \n",
        "                  [dec_output, dh1, dc1, dh2, dc2])\n",
        "\n",
        "# Question을 입력받아 Answer를 생성한다.\n",
        "def genAnswer(question):\n",
        "    question = question[np.newaxis, :]\n",
        "    init_h1, init_c1, init_h2, init_c2 = model_enc.predict(question)\n",
        "\n",
        "    # 시작 단어는 <START>로 한다.\n",
        "    word = np.array(word2idx['<START>']).reshape(1, 1)\n",
        "\n",
        "    answer = []\n",
        "    for i in range(MAX_SEQUENCE_LEN):\n",
        "        dY, next_h1, next_c1, next_h2, next_c2 = \\\n",
        "            model_dec.predict([word, init_h1, init_c1, init_h2, init_c2])\n",
        "        \n",
        "        # 디코더의 출력은 vocabulary에 대응되는 one-hot이다.\n",
        "        # argmax로 해당 단어를 채택한다.\n",
        "        nextWord = np.argmax(dY[0, 0])\n",
        "        \n",
        "        # 예상 단어가 <END>이거나 <PADDING>이면 더 이상 예상할 게 없다.\n",
        "        if nextWord == word2idx['<END>'] or nextWord == word2idx['<PADDING>']:\n",
        "            break\n",
        "        \n",
        "        # 다음 예상 단어인 디코더의 출력을 answer에 추가한다.\n",
        "        answer.append(idx2word[nextWord])\n",
        "        \n",
        "        # 디코더의 다음 recurrent를 위해 입력 데이터와 hidden 값을\n",
        "        # 준비한다. 입력은 word이고, hidden은 h와 c이다.\n",
        "        word = np.array(nextWord).reshape(1,1)\n",
        "    \n",
        "        init_h1 = next_h1\n",
        "        init_c1 = next_c1\n",
        "        init_h2 = next_h2\n",
        "        init_c2 = next_c2\n",
        "        \n",
        "    return ' '.join(answer)\n",
        "\n",
        "# Chatting\n",
        "def chatting(n=100):\n",
        "    for i in range(n):\n",
        "        question = input('Q : ')\n",
        "        \n",
        "        if  question == 'quit':\n",
        "            break\n",
        "        \n",
        "        q_idx = []\n",
        "        for x in question.split(' '):\n",
        "            if x in word2idx:\n",
        "                q_idx.append(word2idx[x])\n",
        "            else:\n",
        "                q_idx.append(word2idx['<UNKNOWN>'])   # out-of-vocabulary (OOV)\n",
        "        \n",
        "        # <PADDING>을 삽입한다.\n",
        "        if len(q_idx) < MAX_SEQUENCE_LEN:\n",
        "            q_idx.extend([word2idx['<PADDING>']] * (MAX_SEQUENCE_LEN - len(q_idx)))\n",
        "        else:\n",
        "            q_idx = q_idx[0:MAX_SEQUENCE_LEN]\n",
        "        \n",
        "        answer = genAnswer(np.array(q_idx))\n",
        "        print('A :', answer)\n",
        "\n",
        "chatting(100)\n",
        "\n",
        "# 대화 내용 예시\n",
        "# 가끔 궁금해 : 그 사람도 그럴 거예요.,0\n",
        "# 가끔 뭐하는지 궁금해 : 그 사람도 그럴 거예요.,0\n",
        "# 가끔은 혼자인게 좋다 : 혼자를 즐기세요.,0\n",
        "# 가난한 자의 설움 : 돈은 다시 들어올 거예요.,0\n",
        "# 가만 있어도 땀난다 : 땀을 식혀주세요.,0\n",
        "# 가상화폐 쫄딱 망함 : 어서 잊고 새출발 하세요.,0\n",
        "# 가스불 켜고 나갔어 : 빨리 집에 돌아가서 끄고 나오세요.,0\n",
        "# 가스불 켜놓고 나온거 같아 : 빨리 집에 돌아가서 끄고 나오세요.,0\n",
        "# 가스비 너무 많이 나왔다. : 다음 달에는 더 절약해봐요.,0\n",
        "# 가스비 비싼데 감기 걸리겠어 : 따뜻하게 사세요!,0\n",
        "# 남자친구 교회 데려가고 싶어 : 마음을 열 때까지 설득해보세요.,0\n",
        "# 남자친구 또 운동 갔어 : 운동을 함께 해보세요.,0\n",
        "# 남자친구 생일인데 뭘 줄까 : 평소에 필요한 것 생각해보세요.,0\n",
        "# 남자친구 승진 선물로 뭐가 좋을까? : 평소에 필요했던 게 좋을 것 같아요.,0\n",
        "# 남자친구 오늘 따라 훈훈해 보인다 : 전생에 나라를 구하셨나요.,0\n",
        "# 남자친구 오늘 좀 질린다. : 결단은 빠를수록 좋아요.,0\n",
        "# 남자친구가 나 안 믿어줘 : 거짓말 적당히 하세요.,0\n",
        "# 남자친구가 너무 바빠 : 너무 집착하지 마세요.,0\n",
        "# 남자친구가 너무 운동만 해 : 운동을 함께 해보세요.,0\n",
        "# 남자친구가 너무 잘생겼어 : 전생에 나라를 구하셨나요.,0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNvuoSCxvfNT",
        "colab_type": "text"
      },
      "source": [
        "### View model by using *plot_model* function\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7rrYtk4wUNM",
        "colab_type": "text"
      },
      "source": [
        "#### example code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEqfgDzwwTCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = tf.keras.Input(shape=(100,), dtype='int32', name='input')\n",
        "x = tf.keras.layers.Embedding(\n",
        "    output_dim=512, input_dim=10000, input_length=100)(input)\n",
        "x = tf.keras.layers.LSTM(32)(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "model = tf.keras.Model(inputs=[input], outputs=[output])\n",
        "dot_img_file = '/tmp/model_1.png'\n",
        "\n",
        "\n",
        "# here\n",
        "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}