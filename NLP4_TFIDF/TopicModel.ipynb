{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP4_7_21_edit_distance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM16rm0nuicMOfQPV4Cu/R2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP4_TFIDF/TopicModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-uohFRxo1O",
        "colab_type": "text"
      },
      "source": [
        "# Edit distance\n",
        "\n",
        "Also known as, **Levenshtein distance**, is a method to calculate the similarity between two scalar of distance of instances.\n",
        "\n",
        "it is used  to convert a string A, which was allegedly meant to represent B, into a string B by various operations such as delete, insert, subsitute.\n",
        "\n",
        "for example, when a string 'appie' is given, isn't it a reasonable to guess that the user meant 'apple'?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jFZTgjVzFK9",
        "colab_type": "text"
      },
      "source": [
        "## examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBp0ZOhbxXyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import\n",
        "from nltk.metrics.distance import edit_distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUFm0hxWzPTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function that returns edit distance and perform sanity check with the library function `nltk.metrics.distance.edit_distance`\n",
        "\n",
        "# fucntion receives two strings and returns int valued 'distance'\n",
        "def my_edit_distance(str1, str2): -> int\n",
        "    # create a table size of m x n\n",
        "    m = len(str1)+1 # HAND\n",
        "    n = len(str2)+1 # AND\n",
        "\n",
        "    # initialize table. only the first row and column\n",
        "    table = {}\n",
        "    for i in range(m): table[i,0] = i\n",
        "    for j in range(n): table[0,j] = j\n",
        "\n",
        "    # fill in the matrix\n",
        "    for i in range(1,m):\n",
        "        for j in range(1,n):\n",
        "            cost = 0 if str1[i-1] == str2[j-1] else 1\n",
        "            table[i,j] = min(table[i,j-1]+1, table[i-1, j]+1, table[i-1, j-1]+ cost)\n",
        "            \n",
        "    return table[i,j]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhN0WzN32xp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('my algorithm : ',my_edit_distance('hand','and'))\n",
        "print('NLTK algorithm : ',edit_distance('hand','and'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8sKVQdi4eTq",
        "colab_type": "text"
      },
      "source": [
        "* output\n",
        "```\n",
        "my algorithm : 1\n",
        "NLTK algorithm : 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADE1H5kHFSa9",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCSVvw5W4q-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build a model that finds the cosine similarity rank of given sentences.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "doc1 = 'gold truck silver'\n",
        "doc2 = 'shipment of gold damaged in a fire'\n",
        "doc3 = 'delivery of silver arrived in a silver truck'\n",
        "doc4 = 'shipment of gold arrived in a truck'\n",
        "\n",
        "Bow1 = doc1.split(' ')\n",
        "Bow2 = doc2.split(' ')\n",
        "Bow3 = doc3.split(' ')\n",
        "Bow4 = doc4.split(' ')\n",
        "\n",
        "Unique = sorted(list(set(Bow1+Bow2+Bow3+Bow4)))\n",
        "\n",
        "fq1 = dict.fromkeys(Unique,0)\n",
        "fq2 = dict.fromkeys(Unique,0)\n",
        "fq3 = dict.fromkeys(Unique,0)\n",
        "fq4 = dict.fromkeys(Unique,0)\n",
        "\n",
        "for word in Bow1:\n",
        "    fq1[word] += 1\n",
        "for word in Bow2:\n",
        "    fq2[word] += 1\n",
        "for word in Bow3:\n",
        "    fq3[word] += 1\n",
        "for word in Bow4:\n",
        "    fq4[word] += 1\n",
        "\n",
        "print(fq1)\n",
        "print(fq2)\n",
        "print(fq3)\n",
        "print(fq4)\n",
        "\n",
        "\n",
        "table_value = np.array([list(fq1.values()),list(fq2.values()),list(fq3.values()),list(fq4.values())]).T\n",
        "table = pd.DataFrame(table_value,index=Unique,columns=['fq1','fq2','fq3','fq4'])\n",
        "table.loc['#words'] = [sum(fq1.values()), sum(fq2.values()), sum(fq3.values()), sum(fq4.values())]\n",
        "table['DF'] = table.sum(axis=1)\n",
        "print(table)\n",
        "\n",
        "\n",
        "#TF\n",
        "TF_value = table_value/np.array(table.loc['#words'][:4])\n",
        "TF = pd.DataFrame(TF_value,index=Unique,columns=['fq1','fq2','fq3','fq4'])\n",
        "print(TF)\n",
        "\n",
        "\n",
        "#IDF\n",
        "IDF = np.array(np.log(len(Unique)/np.array(table['DF'][:-1])))\n",
        "print(IDF)\n",
        "\n",
        "TF_IDF = np.multiply(TF_value.T, IDF).T\n",
        "\n",
        "#Norm - innerdot - cosdistance\n",
        "Norm = np.sqrt(np.sum(np.square(TF_IDF),axis=0))\n",
        "# array([0.75014138, 0.65711979, 0.58844549, 0.53933091])\n",
        "\n",
        "innerdot = np.array([np.inner(Norm[0],Norm[1]),np.inner(Norm[0],Norm[2]),np.inner(Norm[0],Norm[3])])\n",
        "# array([0.49293275, 0.44141731, 0.40457443])\n",
        "\n",
        "# cosine distance\n",
        "cos_dist = np.array([(innerdot[0]/Norm[0]*Norm[1]),(innerdot[1]/Norm[0]*Norm[2]),(innerdot[2]/Norm[0]*Norm[3])])\n",
        "# array([0.43180642, 0.34626809, 0.29087783])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNbFxA6pFZQN",
        "colab_type": "text"
      },
      "source": [
        "## clearer code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70CiTy0uFbUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TFIDF 연습\n",
        "# 2020.07-21\n",
        "# ----------\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# 1. dictionary를 생성한다.\n",
        "def makeVocab(sentences):\n",
        "    words = [word for sentence in sentences for word in sentence.split()]\n",
        "    words = list(set(words))\n",
        "    words.sort()\n",
        "    return {word: idx for idx, word in enumerate(words)}\n",
        "\n",
        "# 2. TF를 생성한다.\n",
        "def makeTF(sentences):\n",
        "    vocab = makeVocab(sentences)\n",
        "    tf = np.zeros((len(vocab), len(sentences)))\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        freq = nltk.FreqDist(nltk.word_tokenize(sentence))\n",
        "        for key in freq.keys():\n",
        "            tf[vocab[key], i] = freq[key] / len(sentence)\n",
        "    return tf\n",
        "\n",
        "# 3. IDF를 생성한다.\n",
        "def makeIDF(sentences, tf):\n",
        "    df = tf.shape[1] - (tf == 0.0).sum(axis=1)\n",
        "    return np.log(tf.shape[1] / (0+df))\n",
        "\n",
        "# 4. TFIDF를 생성한다.\n",
        "def makeTFIDF(sentences):\n",
        "    tf = makeTF(sentences)\n",
        "    idf = makeIDF(sentences, tf)\n",
        "    return np.multiply(tf, idf.reshape(tf.shape[0], 1))\n",
        "\n",
        "sentences = ['gold silver truck', 'shipment of gold damaged in a fire', 'delivery of silver arrived in a silver truck', 'shipment of gold arrived in a truck']\n",
        "tfidf = makeTFIDF(sentences);\n",
        "print(tfidf.round(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xmQTFZyFfWa",
        "colab_type": "text"
      },
      "source": [
        "# Topic Model\n",
        "\n",
        "> **dimension reduction** is conducted by PCA, SVD methods. SVD can be used to find *Topic Model*. \n",
        "\n",
        "> here we have LSA comes out. We deal with two models for Topic Model. \n",
        "1. LSA\n",
        "2. LDA\n",
        "\n",
        "First, LSA takes SVD method where as LDA will be dealt with later.\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxEGRGN2INO6",
        "colab_type": "text"
      },
      "source": [
        "##  LSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0HO2kLEI2LZ",
        "colab_type": "text"
      },
      "source": [
        "### SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcRrMaILI_33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TF-IDF matrix를 SVD로 분해한다.\n",
        "# C = U.S.VT\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "statements = [\n",
        "            'ruled india',\n",
        "            'Chalukyas ruled Badami',\n",
        "            'So many kingdoms ruled India',\n",
        "            'Lalbagh is a botanical garden in India'\n",
        "        ]\n",
        "\n",
        "# TF-IDF matrix를 생성한다.\n",
        "tf_vector = TfidfVectorizer(max_features = 8)\n",
        "tfidf = tf_vector.fit_transform(statements)\n",
        "print(tfidf.shape)\n",
        "\n",
        "# SVD (Singular Vector Decomposition)으로 TF-IDF를 분해한다.\n",
        "# U, S, VT 행렬의 의미 --> Latent Semantic Analysis (LSA)\n",
        "# U 행렬 ~ 차원 = (문서 개수 X topic 개수) : 문서당 topic 분포\n",
        "# S 행렬 ~ 차원 = (topic 개수 X topic 개수)\n",
        "# VT 행렬. 차원 = (topic 개수 X 단어 개수) : topic 당 단어 빈도 (분포)\n",
        "U, S, VT = np.linalg.svd(tfidf.toarray(), full_matrices = True)\n",
        "\n",
        "print(U.round(2), '\\n')\n",
        "print(S.round (2), '\\n')\n",
        "print(VT.round(2), '\\n')\n",
        "\n",
        "# S를 행렬 형태로 변환한다.\n",
        "s = np.zeros(tfidf.shape)\n",
        "s[:S.shape[0], :S.shape[0]] = np.diag(S)\n",
        "print(s.round(2), '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zUOeOjFJCn_",
        "colab_type": "text"
      },
      "source": [
        "### LSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDessuoaGMqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Latent Semantic Analysis (LSA)\n",
        "# ------------------------------\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "#from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# news data를 읽어와서 저장해 둔다.\n",
        "#newsData = fetch_20newsgroups(shuffle=True, \n",
        "#                              random_state=1, \n",
        "#                              remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "#with open('./dataset/news.data', 'wb') as f:\n",
        "#    pickle.dump(newsData , f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# 저장된 news data를 읽어온다.\n",
        "with open('./dataset/news.data', 'rb') as f:\n",
        "    newsData  = pickle.load(f)\n",
        "\n",
        "# 첫 번째 news를 조회해 본다.\n",
        "news = newsData.data\n",
        "print(len(news))\n",
        "print(news[0])\n",
        "\n",
        "# news 별로 분류된 target을 확인해 본다.\n",
        "print(newsData.target_names)\n",
        "print(len(newsData.target_names))\n",
        "\n",
        "# preprocessing.\n",
        "# 1. 영문자가 아닌 문자를 모두 제거한다.\n",
        "news1 = []\n",
        "for doc in news:\n",
        "    news1.append(re.sub(\"[^a-zA-Z]\", \" \", doc))\n",
        "\n",
        "# 2. 불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 \n",
        "# 단어를 제거한다\n",
        "stop_words = stopwords.words('english')\n",
        "news2 = []\n",
        "for doc in news1:\n",
        "    doc1 = []\n",
        "    for w in doc.split():\n",
        "        w = w.lower()\n",
        "        if len(w) > 3 and w not in stop_words:\n",
        "            doc1.append(w)\n",
        "    news2.append(' '.join(doc1))\n",
        "    \n",
        "print(news2[0])\n",
        "\n",
        "# TF-IDF matrix를 생성한다.\n",
        "tf_vector = TfidfVectorizer(max_features = 500)\n",
        "tfidf = tf_vector.fit_transform(news2)\n",
        "print(tfidf.shape)\n",
        "\n",
        "vocab = tf_vector.get_feature_names()\n",
        "print(vocab[:20])\n",
        "\n",
        "# Latent Semantic Analysis (LSA)\n",
        "# ------------------------------\n",
        "svd = TruncatedSVD(n_components = len(newsData.target_names), n_iter=1000)\n",
        "svd.fit(tfidf)\n",
        "\n",
        "U = svd.fit_transform(tfidf) / svd.singular_values_\n",
        "VT = svd.components_\n",
        "S = np.diag(svd.singular_values_)\n",
        "\n",
        "# 문서 별 Topic 번호를 확인한다. (문서 10개만 확인)\n",
        "for i in range(10):\n",
        "    print('문서-{:d} : topic = {:d}'.format(i, np.argmax(U[i:(i+1), :][0])))\n",
        "    \n",
        "# VT 행렬에서 topic 별로 중요 단어를 표시한다\n",
        "for i in range(len(VT)):\n",
        "    idx = np.flipud(VT[i].argsort())[:10]\n",
        "    print('토픽-{:2d} : '.format(i+1), end='')\n",
        "    for n in idx:\n",
        "        print('{:s} '.format(vocab[n]), end='')\n",
        "    print()\n",
        "\n",
        "newsData.keys()\n",
        "\n",
        "# 문서별로 분류된 코드를 확인해 본다.\n",
        "# x, y : 문서 번호\n",
        "def checkTopic(x, y):\n",
        "    print(\"문서 %d의 topic = %s\" % (x, newsData.target_names[newsData.target[x]]))\n",
        "    print(\"문서 %d의 topic = %s\" % (y, newsData.target_names[newsData.target[y]]))\n",
        "\n",
        "checkTopic(1, 6)\n",
        "checkTopic(0, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMV84DloIPvc",
        "colab_type": "text"
      },
      "source": [
        "## LDA\n",
        "\n",
        "> LDA는 결합확률분포로 취급하고(문서, 단어 조합의 관점에서) **Gibbs' sampling**의 샘플링 기법으로 근사치를 구한다.\n",
        "\n",
        "> 두개의 다항분포를 추정하고, 해당 문서를 특정 topic에 할당하는 알고리즘.\n",
        "\n",
        "> 문서는 토픽 집합에 속한 단어들로 구성되고, 한 문서에 등장하는 단어들은 여러 토픽들과 연계되어 있다. -> 토픽 분포\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2pEYfSgIRtt",
        "colab_type": "text"
      },
      "source": [
        "### algorithm explanation\n",
        "\n",
        "> How LDA algorithm workds : from a random document of its topic we want to know, we observe it's word distribution( `w` ). and we can back track (`theta` and `beta`) with `w`. \n",
        "\n",
        "> Where `beta` represents the likelihood of the topic given distribution of words and `theta` represents the likelihood of the topic of the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwQQHfg_RTHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Latent Dirichlet Allocation (LDA)\n",
        "# ---------------------------------\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "#from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# news data를 읽어와서 저장해 둔다.\n",
        "#newsData = fetch_20newsgroups(shuffle=True, \n",
        "#                              random_state=1, \n",
        "#                              remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "#with open('./dataset/news.data', 'wb') as f:\n",
        "#    pickle.dump(newsData , f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# 저장된 news data를 읽어온다.\n",
        "with open('./dataset/news.data', 'rb') as f:\n",
        "    newsData  = pickle.load(f)\n",
        "\n",
        "# 첫 번째 news를 조회해 본다.\n",
        "news = newsData.data\n",
        "print(len(news))\n",
        "print(news[0])\n",
        "\n",
        "# news 별로 분류된 target을 확인해 본다.\n",
        "print(newsData.target_names)\n",
        "print(len(newsData.target_names))\n",
        "\n",
        "# preprocessing.\n",
        "# 1. 영문자가 아닌 문자를 모두 제거한다.\n",
        "news1 = []\n",
        "for doc in news:\n",
        "    news1.append(re.sub(\"[^a-zA-Z]\", \" \", doc))\n",
        "\n",
        "# 2. 불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 \n",
        "# 단어를 제거한다\n",
        "stop_words = stopwords.words('english')\n",
        "news2 = []\n",
        "for doc in news1:\n",
        "    doc1 = []\n",
        "    for w in doc.split():\n",
        "        w = w.lower()\n",
        "        if len(w) > 3 and w not in stop_words:\n",
        "            doc1.append(w)\n",
        "    news2.append(' '.join(doc1))\n",
        "    \n",
        "print(news2[0])\n",
        "\n",
        "# TF-IDF matrix를 생성한다.\n",
        "tf_vector = TfidfVectorizer(max_features = 500)\n",
        "tfidf = tf_vector.fit_transform(news2)\n",
        "print(tfidf.shape)\n",
        "\n",
        "vocab = tf_vector.get_feature_names()\n",
        "print(vocab[:20])\n",
        "\n",
        "# Latent Dirichlet Allocation (LDA)\n",
        "# ---------------------------------\n",
        "# Return 값이 Document-Topic distribution이다.\n",
        "# iteration 횟수가 max_iter까지 가면 아직 수렴하지 않은 것이다.\n",
        "# 아직 수렴하지 않은 경우 mat_iter를 증가시켜야 한다.\n",
        "# mat_iter를 증가시켜도 수렴하지 못하는 경우는 preprocessing 등을 좀 더 정밀하게 해야 한다.\n",
        "# perplexity는 감소해야 한다.\n",
        "# perplexity의 역할 : elbow 같은 역할\n",
        "# n_components는 적절히 바꿔줄 수도 있음. perplexity 변화에 따라서\n",
        "# evaluate_every - 5번 반복마다 성능 평가 & 결과 출력\n",
        "model = LDA(n_components = len(newsData.target_names), \n",
        "            learning_method='online', \n",
        "            evaluate_every=5, \n",
        "            max_iter=1000, \n",
        "            verbose=1)\n",
        "\n",
        "doc_topic = model.fit_transform(tfidf)\n",
        "\n",
        "# 문서 별 Topic 번호를 확인한다. (문서 10개만 확인)  ## theta\n",
        "for i in range(10):\n",
        "    print('문서-{:d} : topic = {:d}'.format(i, np.argmax(doc_topic[i:(i+1), :][0])))\n",
        "\n",
        "# topic_term 행렬에서 topic 별로 중요 단어를 표시한다  ## beta\n",
        "topic_term = model.components_\n",
        "for i in range(len(topic_term)):\n",
        "    idx = np.flipud(topic_term[i].argsort())[:10]\n",
        "    print('토픽-{:2d} : '.format(i+1), end='')\n",
        "    for n in idx:\n",
        "        print('{:s} '.format(vocab[n]), end='')\n",
        "    print()\n",
        "\n",
        "newsData.keys()\n",
        "\n",
        "# 문서별로 분류된 코드를 확인해 본다.\n",
        "# x, y : 문서 번호\n",
        "def checkTopic(x, y):\n",
        "    print(\"문서 %d의 topic = %s\" % (x, newsData.target_names[newsData.target[x]]))\n",
        "    print(\"문서 %d의 topic = %s\" % (y, newsData.target_names[newsData.target[y]]))\n",
        "\n",
        "checkTopic(1, 5)\n",
        "checkTopic(0, 2)\n",
        "checkTopic(4, 7)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}