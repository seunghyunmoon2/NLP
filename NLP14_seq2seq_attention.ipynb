{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP14_8_12_seq2seq_attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNd2DPH7uFmpXRu/arSZ86v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP14_seq2seq_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csgPIFMy3OLb",
        "colab_type": "text"
      },
      "source": [
        "# build a chatbot using seq2seq-Attention model\n",
        "\n",
        "[논문/paper1](https://arxiv.org/abs/1409.0473)   \n",
        "[논문/paper2](https://arxiv.org/abs/1508.04025)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMRxLaH85WeO",
        "colab_type": "text"
      },
      "source": [
        "# Model overview\n",
        "\n",
        "![1](https://drive.google.com/uc?view=export&id=1Vmh8mRzw1HZk4Qtu1yK5n0pKKQ6M4s88)   \n",
        "![2](https://drive.google.com/uc?view=export&id=1_oiXJN2lx7ZvWax6Z2pNMEoxHVlhF4b9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWls4Lz24UjM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "[tensorflow-reference](https://www.tensorflow.org/tutorials/text/nmt_with_attention)   \n",
        "[tensorflow-github](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSjrL1ndDC07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Attention을 이용한 ChatBot : 학습 모듈\n",
        "#\n",
        "# 2020.06.04 : 조성현 (blog.naver.com/chunjein)\n",
        "# ---------------------------------------------\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dot\n",
        "from tensorflow.keras.layers import Activation, Concatenate\n",
        "from tensorflow.keras.layers import Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# 단어 목록 dict를 읽어온다.\n",
        "with open('./dataset/6-1.vocabulary.pickle', 'rb') as f:\n",
        "    word2idx,  idx2word = pickle.load(f)\n",
        "    \n",
        "# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.\n",
        "with open('./dataset/6-1.train_data.pickle', 'rb') as f:\n",
        "    trainXE, trainXD, trainYD = pickle.load(f)\n",
        "\t\n",
        "# 평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "with open('./dataset/6-1.eval_data.pickle', 'rb') as f:\n",
        "    testXE, testXD, testYD = pickle.load(f)\n",
        "\n",
        "VOCAB_SIZE = len(idx2word)\n",
        "EMB_SIZE = 128\n",
        "LSTM_HIDDEN = 128\n",
        "MODEL_PATH = './dataset/6-6.Attention.h5'\n",
        "LOAD_MODEL = True\n",
        "\n",
        "# Encoder 출력과 decoder 출력으로 attention value를 생성하고,\n",
        "# decoder 출력 + attention value (concatenate)를 리턴한다.\n",
        "# x : encoder 출력, y : decoder 출력\n",
        "# LSTM time step = 4, SMB_SIZE = 3 이라면 각 텐서의 dimension은\n",
        "# 아래 주석과 같다.\n",
        "def Attention(x, y):\n",
        "    # step-1:\n",
        "    # decoder의 매 시점마다 encoder의 전체 시점과 dot-product을 수행한다.\n",
        "    score = Dot(axes=(2, 2))([y, x])                   # (1, 4, 4)\n",
        "    \n",
        "    # step-2:\n",
        "    # dot-product 결과를 확률분포로 만든다 (softmax)\n",
        "    # 이것이 attention score이다.\n",
        "    dist = Activation('softmax')(score)                # (1, 4, 4)\n",
        "\n",
        "    # step-3:    \n",
        "    # encoder의 전체 시점에 위의 확률 분포를 적용해서 가중 평균한다.\n",
        "    # 직접 계산이 어렵기 때문에 dist를 확장하고, 열을 복제해서\n",
        "    # Dot 연산이 가능하도록 trick을 쓴다.\n",
        "    # 이것이 attention value이다.\n",
        "    # dist_exp = K.expand_dims(dist, 2)                   # (1, 4, 1, 4)\n",
        "    # dist_rep = K.repeat_elements(dist_exp, EMB_SIZE, 2) # (1, 4, 3, 4)                                       \n",
        "    # dist_dot = Dot(axes=(3, 1))([dist_rep, x])          # (1, 4, 3, 3)\n",
        "    # attention = K.mean(dist_dot, axis = 2)              # (1, 4, 3)\n",
        "\n",
        "    # step-4:\n",
        "    # 교재의 step-3을 계산하지 않고 step-4를 직접 계산했다.\n",
        "    attention = Dot(axes=(2, 1))([dist, x])\n",
        "    \n",
        "    # step-5:\n",
        "    # decoder 출력과 attention을 concatenate 한다.\n",
        "    return Concatenate()([y, attention])    # (1, 4, 6)\n",
        "    \n",
        "# 워드 임베딩 레이어. Encoder와 decoder에서 공동으로 사용한다.\n",
        "K.clear_session()\n",
        "wordEmbedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)\n",
        "\n",
        "# Encoder\n",
        "# -------\n",
        "# many-to-many로 구성한다. Attention value를 계산하기 위해 중간 출력이 필요하고\n",
        "# (return_sequences=True), decoder로 전달할 h와 c도 필요하다 (return_state = True)\n",
        "encoderX = Input(batch_shape=(None, trainXE.shape[1]))\n",
        "encEMB = wordEmbedding(encoderX)\n",
        "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "encLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "ey1, eh1, ec1 = encLSTM1(encEMB)    # LSTM 1층 \n",
        "ey2, eh2, ec2 = encLSTM2(ey1)       # LSTM 2층\n",
        "\n",
        "# Decoder\n",
        "# -------\n",
        "# many-to-many로 구성한다. target을 학습하고 Attention을 위해서는 중간 출력이 \n",
        "# 필요하다. 그리고 초기 h와 c는 encoder에서 출력한 값을 사용한다 (initial_state)\n",
        "# 최종 출력은 vocabulary의 인덱스인 one-hot 인코더이다.\n",
        "decoderX = Input(batch_shape=(None, trainXD.shape[1]))\n",
        "decEMB = wordEmbedding(decoderX)\n",
        "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
        "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
        "att_dy2 = Attention(ey2, dy2)\n",
        "decOutput = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "outputY = decOutput(att_dy2)\n",
        "\n",
        "# Model\n",
        "# -----\n",
        "# target이 one-hot encoding되어 있으면 categorical_crossentropy\n",
        "# target이 integer로 되어 있으면 sparse_categorical_crossentropy를 쓴다.\n",
        "# sparse_categorical_entropy는 integer인 target을 one-hot으로 바꾼 후에\n",
        "# categorical_entropy를 수행한다.\n",
        "model = Model([encoderX, decoderX], outputY)\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001), \n",
        "              loss='sparse_categorical_crossentropy')\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    model.load_weights(MODEL_PATH)\n",
        "\n",
        "# 학습 (teacher forcing)\n",
        "# ----------------------\n",
        "# loss = sparse_categorical_crossentropy이기 때문에 target을 one-hot으로 변환할\n",
        "# 필요 없이 integer인 trainYD를 그대로 넣어 준다. trainYD를 one-hot으로 변환해서\n",
        "# categorical_crossentropy로 처리하면 out-of-memory 문제가 발생할 수 있다.\n",
        "hist = model.fit([trainXE, trainXD], trainYD, batch_size = 300, \n",
        "                 epochs=1, shuffle=True,\n",
        "                 validation_data = ([testXE, testXD], testYD))\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 학습 결과를 저장한다\n",
        "model.save_weights(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqJE6g6q3IRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Attention을 이용한 ChatBot : 채팅 모듈\n",
        "#\n",
        "# 2020.06.04 : 조성현 (blog.naver.com/chunjein)\n",
        "# ---------------------------------------------\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dot\n",
        "from tensorflow.keras.layers import Activation, Concatenate\n",
        "from tensorflow.keras.layers import Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import sys\n",
        "sys.getdefaultencoding() \n",
        "\n",
        "# 단어 목록 dict를 읽어온다.\n",
        "with open('./dataset/6-1.vocabulary.pickle', 'rb') as f:\n",
        "    word2idx,  idx2word = pickle.load(f)\n",
        "    \n",
        "VOCAB_SIZE = len(idx2word)\n",
        "EMB_SIZE = 128\n",
        "LSTM_HIDDEN = 128\n",
        "MAX_SEQUENCE_LEN = 10            # 단어 시퀀스 길이\n",
        "MODEL_PATH = './dataset/6-6.Attention.h5'\n",
        "\n",
        "# Encoder 출력과 decoder 출력으로 attention value를 생성하고,\n",
        "# decoder 출력 + attention value (concatenate)를 리턴한다.\n",
        "# x : encoder 출력, y : decoder 출력\n",
        "# LSTM time step = 4, SMB_SIZE = 3 이라면 각 텐서의 dimension은\n",
        "# 아래 주석과 같다.\n",
        "def Attention(x, y):\n",
        "    # step-1:\n",
        "    # decoder의 매 시점마다 encoder의 전체 시점과 dot-product을 수행한다.\n",
        "    score = Dot(axes=(2, 2))([y, x])                   # (1, 4, 4)\n",
        "    \n",
        "    # step-2:\n",
        "    # dot-product 결과를 확률분포로 만든다 (softmax)\n",
        "    # 이것이 attention score이다.\n",
        "    dist = Activation('softmax')(score)                # (1, 4, 4)\n",
        "\n",
        "    # step-3:    \n",
        "    # encoder의 전체 시점에 위의 확률 분포를 적용해서 가중 평균한다.\n",
        "    # 직접 계산이 어렵기 때문에 dist를 확장하고, 열을 복제해서\n",
        "    # Dot 연산이 가능하도록 trick을 쓴다.\n",
        "    # 이것이 attention value이다.\n",
        "    # dist_exp = K.expand_dims(dist, 2)                   # (1, 4, 1, 4)\n",
        "    # dist_rep = K.repeat_elements(dist_exp, EMB_SIZE, 2) # (1, 4, 3, 4)                                       \n",
        "    # dist_dot = Dot(axes=(3, 1))([dist_rep, x])          # (1, 4, 3, 3)\n",
        "    # attention = K.mean(dist_dot, axis = 2)              # (1, 4, 3)\n",
        "\n",
        "    # step-4:\n",
        "    # 교재의 step-3을 계산하지 않고 step-4를 직접 계산했다.\n",
        "    attention = Dot(axes=(2, 1))([dist, x])\n",
        "    \n",
        "    # step-5:\n",
        "    # decoder 출력과 attention을 concatenate 한다.\n",
        "    return Concatenate()([y, attention])    # (1, 4, 6)\n",
        "\n",
        "# 워드 임베딩 레이어. Encoder와 decoder에서 공동으로 사용한다.\n",
        "K.clear_session()\n",
        "wordEmbedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)\n",
        "\n",
        "# Encoder\n",
        "# -------\n",
        "# many-to-many로 구성한다. Attention value를 계산하기 위해 중간 출력이 필요하고\n",
        "# (return_sequences=True), decoder로 전달할 h와 c도 필요하다 (return_state = True)\n",
        "encoderX = Input(batch_shape=(None, MAX_SEQUENCE_LEN))\n",
        "encEMB = wordEmbedding(encoderX)\n",
        "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "encLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "ey1, eh1, ec1 = encLSTM1(encEMB)    # LSTM 1층 \n",
        "ey2, eh2, ec2 = encLSTM2(ey1)       # LSTM 2층\n",
        "\n",
        "# Decoder\n",
        "# -------\n",
        "# Decoder는 1개 단어씩을 입력으로 받는다. 학습 때와 달리 문장 전체를 받아\n",
        "# recurrent하는 것이 아니라, 단어 1개씩 입력 받아서 다음 예상 단어를 확인한다.\n",
        "# chatting()에서 for 문으로 단어 별로 recurrent 시킨다.\n",
        "# 따라서 batch_shape = (None, 1)이다. 즉, time_step = 1이다. 그래도 네트워크\n",
        "# 파라메터는 동일하다.\n",
        "decoderX = Input(batch_shape=(None, 1))\n",
        "decEMB = wordEmbedding(decoderX)\n",
        "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
        "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
        "att_dy2 = Attention(ey2, dy2)\n",
        "decOutput = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "outputY = decOutput(att_dy2)\n",
        "\n",
        "# Model\n",
        "# -----\n",
        "model = Model([encoderX, decoderX], outputY)\n",
        "model.load_weights(MODEL_PATH)\n",
        "\n",
        "# Chatting용 model\n",
        "model_enc = Model(encoderX, [eh1, ec1, eh2, ec2, ey2])\n",
        "\n",
        "ih1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ic1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ih2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ic2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ey = Input(batch_shape = (None, MAX_SEQUENCE_LEN, LSTM_HIDDEN))\n",
        "\n",
        "dec_output1, dh1, dc1 = decLSTM1(decEMB, initial_state = [ih1, ic1])\n",
        "dec_output2, dh2, dc2 = decLSTM2(dec_output1, initial_state = [ih2, ic2])\n",
        "dec_attention = Attention(ey, dec_output2)\n",
        "dec_output = decOutput(dec_attention)\n",
        "model_dec = Model([decoderX, ih1, ic1, ih2, ic2, ey], \n",
        "                  [dec_output, dh1, dc1, dh2, dc2])\n",
        "\n",
        "# Question을 입력받아 Answer를 생성한다.\n",
        "def genAnswer(question):\n",
        "    question = question[np.newaxis, :]\n",
        "    init_h1, init_c1, init_h2, init_c2, enc_y = model_enc.predict(question)\n",
        "\n",
        "    # 시작 단어는 <START>로 한다.\n",
        "    word = np.array(word2idx['<START>']).reshape(1, 1)\n",
        "\n",
        "    answer = []\n",
        "    for i in range(MAX_SEQUENCE_LEN):\n",
        "        dY, next_h1, next_c1, next_h2, next_c2 = \\\n",
        "            model_dec.predict([word, init_h1, init_c1, init_h2, init_c2, enc_y])\n",
        "        \n",
        "        # 디코더의 출력은 vocabulary에 대응되는 one-hot이다.\n",
        "        # argmax로 해당 단어를 채택한다.\n",
        "        nextWord = np.argmax(dY[0, 0])\n",
        "        \n",
        "        # 예상 단어가 <END>이거나 <PADDING>이면 더 이상 예상할 게 없다.\n",
        "        if nextWord == word2idx['<END>'] or nextWord == word2idx['<PADDING>']:\n",
        "            break\n",
        "        \n",
        "        # 다음 예상 단어인 디코더의 출력을 answer에 추가한다.\n",
        "        answer.append(idx2word[nextWord])\n",
        "        \n",
        "        # 디코더의 다음 recurrent를 위해 입력 데이터와 hidden 값을\n",
        "        # 준비한다. 입력은 word이고, hidden은 h와 c이다.\n",
        "        word = np.array(nextWord).reshape(1,1)\n",
        "    \n",
        "        init_h1 = next_h1\n",
        "        init_c1 = next_c1\n",
        "        init_h2 = next_h2\n",
        "        init_c2 = next_c2\n",
        "        \n",
        "    return ' '.join(answer)\n",
        "\n",
        "# Chatting\n",
        "def chatting(n=100):\n",
        "    for i in range(n):\n",
        "        question = input('Q : ')\n",
        "        \n",
        "        if  question == 'quit':\n",
        "            break\n",
        "        \n",
        "        q_idx = []\n",
        "        for x in question.split(' '):\n",
        "            if x in word2idx:\n",
        "                q_idx.append(word2idx[x])\n",
        "            else:\n",
        "                q_idx.append(word2idx['<UNKNOWN>'])   # out-of-vocabulary (OOV)\n",
        "        \n",
        "        # <PADDING>을 삽입한다.\n",
        "        if len(q_idx) < MAX_SEQUENCE_LEN:\n",
        "            q_idx.extend([word2idx['<PADDING>']] * (MAX_SEQUENCE_LEN - len(q_idx)))\n",
        "        else:\n",
        "            q_idx = q_idx[0:MAX_SEQUENCE_LEN]\n",
        "        \n",
        "        answer = genAnswer(np.array(q_idx))\n",
        "        print('A :', answer)\n",
        "\n",
        "chatting(100)\n",
        "\n",
        "# 대화 내용 예시\n",
        "# 가끔 궁금해 : 그 사람도 그럴 거예요.,0\n",
        "# 가끔 뭐하는지 궁금해 : 그 사람도 그럴 거예요.,0\n",
        "# 가끔은 혼자인게 좋다 : 혼자를 즐기세요.,0\n",
        "# 가난한 자의 설움 : 돈은 다시 들어올 거예요.,0\n",
        "# 가만 있어도 땀난다 : 땀을 식혀주세요.,0\n",
        "# 가상화폐 쫄딱 망함 : 어서 잊고 새출발 하세요.,0\n",
        "# 가스불 켜고 나갔어 : 빨리 집에 돌아가서 끄고 나오세요.,0\n",
        "# 가스불 켜놓고 나온거 같아 : 빨리 집에 돌아가서 끄고 나오세요.,0\n",
        "# 가스비 너무 많이 나왔다. : 다음 달에는 더 절약해봐요.,0\n",
        "# 가스비 비싼데 감기 걸리겠어 : 따뜻하게 사세요!,0\n",
        "# 남자친구 교회 데려가고 싶어 : 마음을 열 때까지 설득해보세요.,0\n",
        "# 남자친구 또 운동 갔어 : 운동을 함께 해보세요.,0\n",
        "# 남자친구 생일인데 뭘 줄까 : 평소에 필요한 것 생각해보세요.,0\n",
        "# 남자친구 승진 선물로 뭐가 좋을까? : 평소에 필요했던 게 좋을 것 같아요.,0\n",
        "# 남자친구 오늘 따라 훈훈해 보인다 : 전생에 나라를 구하셨나요.,0\n",
        "# 남자친구 오늘 좀 질린다. : 결단은 빠를수록 좋아요.,0\n",
        "# 남자친구가 나 안 믿어줘 : 거짓말 적당히 하세요.,0\n",
        "# 남자친구가 너무 바빠 : 너무 집착하지 마세요.,0\n",
        "# 남자친구가 너무 운동만 해 : 운동을 함께 해보세요.,0\n",
        "# 남자친구가 너무 잘생겼어 : 전생에 나라를 구하셨나요.,0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}