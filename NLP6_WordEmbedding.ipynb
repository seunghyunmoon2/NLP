{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP6_07/22_딥러닝.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0tDIjWetRfRSZmLp52Dy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP6_WordEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lOzXKxkiKG1",
        "colab_type": "text"
      },
      "source": [
        "# Email classificaion\n",
        "\n",
        "using deeplearning, train the model with train email data set and categorize each test data into 1 of 20 categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z39qqVEUjhUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Email Classification\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "# 빠른 시험을 위해 일부 데이터만 사용한다.\n",
        "x_train = newsgroups_train.data[:2000]\n",
        "x_test = newsgroups_test.data[:500]\n",
        "y_train = newsgroups_train.target[:2000]\n",
        "y_test = newsgroups_test.target[:500]\n",
        "\n",
        "print (\"List of all 20 categories:\")\n",
        "print (newsgroups_train.target_names)\n",
        "print (\"\\nSample Email:\")\n",
        "print (x_train[0])\n",
        "print (\"Sample Target Category:\")\n",
        "print (y_train[0])\n",
        "print (newsgroups_train.target_names[y_train[0]])\n",
        "\n",
        "def preprocessing(text):\n",
        "    # string.punctuation : '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "    text2 = ''\n",
        "    for ch in text:\n",
        "        if ch in string.punctuation:\n",
        "            text2 += ' '\n",
        "        else:\n",
        "            text2 += ch\n",
        "    text2 = ' '.join(text2.split())  # '\\n'을 제거하기 위해 split() 후 다시 합침\n",
        "       \n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text2):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            tokens.append(word.lower())\n",
        "            \n",
        "    stopwds = stopwords.words('english')\n",
        "    tokens = [token for token in tokens if token not in stopwds]\n",
        "    tokens = [word for word in tokens if len(word)>=3]\n",
        "    \n",
        "    #stemmer = PorterStemmer()\n",
        "    #try:\n",
        "    #    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    #except:\n",
        "    #    tokens = tokens\n",
        "        \n",
        "    tagged_corpus = pos_tag(tokens)    \n",
        "    \n",
        "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
        "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def prat_lemmatize(token,tag):\n",
        "        if tag in Noun_tags:\n",
        "            return lemmatizer.lemmatize(token,'n')\n",
        "        elif tag in Verb_tags:\n",
        "            return lemmatizer.lemmatize(token,'v')\n",
        "        else:\n",
        "            return lemmatizer.lemmatize(token,'n')\n",
        "    \n",
        "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
        "\n",
        "    return pre_proc_text\n",
        "\n",
        "x_train_preprocessed  = []\n",
        "for i in x_train:\n",
        "    x_train_preprocessed.append(preprocessing(i))\n",
        "\n",
        "x_test_preprocessed = []\n",
        "for i in x_test:\n",
        "    x_test_preprocessed.append(preprocessing(i))\n",
        "    \n",
        "x_train_preprocessed[0]\n",
        "\n",
        "# building TFIDF vectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),\n",
        "                             stop_words='english',\n",
        "                             max_features= 10000)\n",
        "\n",
        "x_train_2 = vectorizer.fit(x_train_preprocessed)\n",
        "x_train_2 = vectorizer.transform(x_train_preprocessed).todense()\n",
        "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()\n",
        "len(vectorizer.get_feature_names())\n",
        "\n",
        "# Deep Learning modules\n",
        "np.random.seed(1337) \n",
        "nb_classes = len(np.unique(y_train))\n",
        "batch_size = 64\n",
        "nb_epochs = 10\n",
        "\n",
        "Y_train = to_categorical(y_train, nb_classes)\n",
        "Y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(1000, input_shape= (x_train_2.shape[1],)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "print (model.summary())\n",
        "\n",
        "hist = model.fit(x_train_2, Y_train, \n",
        "                 batch_size=batch_size, \n",
        "                 epochs=nb_epochs,\n",
        "                 validation_data = (x_test_2, Y_test))\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "y_train_predclass = model.predict_classes(x_train_2,batch_size=batch_size)\n",
        "y_test_predclass = model.predict_classes(x_test_2,batch_size=batch_size)\n",
        "\n",
        "print (\"Train accuracy:\", np.round(accuracy_score(y_train, y_train_predclass), 3))\n",
        "print (\"Test accuracy:\", np.round(accuracy_score(y_test, y_test_predclass), 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic9bTxgXkwmi",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "List of all 20 categories:\n",
        "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
        "\n",
        "Sample Email:\n",
        "From: lerxst@wam.umd.edu (where's my thing)\n",
        "Subject: WHAT car is this!?\n",
        "Nntp-Posting-Host: rac3.wam.umd.edu\n",
        "Organization: University of Maryland, College Park\n",
        "Lines: 15\n",
        "\n",
        " I was wondering if anyone out there could enlighten me on this car I saw\n",
        "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
        "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
        "the front bumper was separate from the rest of the body. This is \n",
        "all I know. If anyone can tellme a model name, engine specs, years\n",
        "of production, where this car is made, history, or whatever info you\n",
        "have on this funky looking car, please e-mail.\n",
        "\n",
        "Thanks,\n",
        "- IL\n",
        "   ---- brought to you by your neighborhood Lerxst ----\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Sample Target Category:\n",
        "7\n",
        "rec.autos\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "dense (Dense)                (None, 1000)              10001000  \n",
        "_________________________________________________________________\n",
        "activation (Activation)      (None, 1000)              0         \n",
        "_________________________________________________________________\n",
        "dropout (Dropout)            (None, 1000)              0         \n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 20)                20020     \n",
        "_________________________________________________________________\n",
        "activation_1 (Activation)    (None, 20)                0         \n",
        "=================================================================\n",
        "Total params: 10,021,020\n",
        "Trainable params: 10,021,020\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "None\n",
        "Train on 2000 samples, validate on 500 samples\n",
        "Epoch 1/10\n",
        "2000/2000 [==============================] - 4s 2ms/sample - loss: 2.8591 - val_loss: 2.6482\n",
        "Epoch 2/10\n",
        "2000/2000 [==============================] - 2s 1ms/sample - loss: 1.9795 - val_loss: 1.8801\n",
        "Epoch 3/10\n",
        "2000/2000 [==============================] - 2s 1ms/sample - loss: 0.9325 - val_loss: 1.3282\n",
        "Epoch 4/10\n",
        "2000/2000 [==============================] - 2s 1ms/sample - loss: 0.3953 - val_loss: 1.1097\n",
        "Epoch 5/10\n",
        "2000/2000 [==============================] - 2s 1ms/sample - loss: 0.1820 - val_loss: 1.0185\n",
        "Epoch 6/10\n",
        "2000/2000 [==============================] - 2s 1ms/sample - loss: 0.0980 - val_loss: 0.9770\n",
        "Epoch 7/10\n",
        "2000/2000 [==============================] - 2s 1ms/sample - loss: 0.0585 - val_loss: 0.9563\n",
        "Epoch 8/10\n",
        "2000/2000 [==============================] - 2s 1ms/sample - loss: 0.0395 - val_loss: 0.9434\n",
        "Epoch 9/10\n",
        "2000/2000 [==============================] - 2s 1ms/sample - loss: 0.0288 - val_loss: 0.9348\n",
        "Epoch 10/10\n",
        "2000/2000 [==============================] - 2s 1ms/sample - loss: 0.0221 - val_loss: 0.9280\n",
        "\n",
        "\n",
        "\n",
        "Figures now render in the Plots pane by default. To make them also appear inline in the Console, uncheck \"Mute Inline Plotting\" under the Plots pane options menu. \n",
        "\n",
        "\n",
        " Train accuracy: 1.0\n",
        "Test accuracy: 0.726\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422YKd5Wkz4H",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding\n",
        "\n",
        "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. ... Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISAxRIfQbO1t",
        "colab_type": "text"
      },
      "source": [
        "## get comments from IMDB and tell if each comment is either in positive or negative tone. 0: negative, 1: positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npjQAWRabhiu",
        "colab_type": "text"
      },
      "source": [
        "### WE with CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEXjvHkFEPUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMDB 감정 분류 : Word Embedding & CNN\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# set parameters\n",
        "max_features = 6000 # max_features : 최대 단어수\n",
        "max_length = 400\n",
        "\n",
        "# 학습 데이터는 자주 등장하는 단어 6,000개로 구성한다.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "print(len(x_train), 'train observations')\n",
        "print(len(x_test), 'test observations')\n",
        "print(x_train[0])  # 6,000 이하의 word index로 구성돼 있다.\n",
        "\n",
        "wind = imdb.get_word_index()\n",
        "revind = dict((v,k) for k,v in wind.items())\n",
        "\n",
        "def decode(sent_list):\n",
        "    new_words = []\n",
        "    for i in sent_list:\n",
        "        # 0 : padding, 1 : 문서 시작, 2 : OOV로 사용함.\n",
        "        # 실제 word index에서 3을 빼야함.\n",
        "        # revind에서 i-3을 조회하고, 없으면 '*'로 채우라는 의미.\n",
        "        new_words.append(revind.get(i-3, '*'))\n",
        "    comb_words = \" \".join(new_words)\n",
        "    return comb_words\n",
        "\n",
        "# 문장의 시작은 항상 '*'로 시작할 것임. 중간에 있는 '*'는 OOV일 것임.\n",
        "decode(x_train[0])\n",
        "   \n",
        "# Pad sequences for computational efficiency\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "print(x_train[0])\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "# 각 문장의 OOV 개수 확인.\n",
        "(x_train[0] == 2).sum()\n",
        "(x_train[1] == 2).sum()\n",
        "\n",
        "# Deep Learning architecture parameters\n",
        "batch_size = 32\n",
        "embedding_dims = 60\n",
        "num_kernels = 260        # convolution filter 개수\n",
        "kernel_size = 3          # convolution filter size\n",
        "hidden_dims = 300\n",
        "epochs = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_dims, input_length=max_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(num_kernels, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "print(model.summary())\n",
        "\n",
        "# 학습\n",
        "hist = model.fit(x_train, y_train, \n",
        "                 batch_size=batch_size, \n",
        "                 epochs=epochs,\n",
        "                 validation_data = (x_test, y_test))\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 성능 확인\n",
        "y_train_predclass = model.predict_classes(x_train, batch_size=batch_size)\n",
        "y_test_predclass = model.predict_classes(x_test, batch_size=batch_size)\n",
        "\n",
        "y_train_predclass.shape = y_train.shape\n",
        "y_test_predclass.shape = y_test.shape\n",
        "\n",
        "print ((\"Train accuracy:\"),(np.round(accuracy_score(y_train,y_train_predclass),3)))  \n",
        "print ((\"Test accuracy:\"),(np.round(accuracy_score(y_test,y_test_predclass),3)))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leIoeDifa9WX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train accuracy: 0.999\n",
        "Test accuracy: 0.886"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SRHDzCVbs_8",
        "colab_type": "text"
      },
      "source": [
        "### WE with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvJ0YOP6bESE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMDB 감정 분류 : Word Embedding & Bidirectional LSTM\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Bidirectional, LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "max_features = 6000\n",
        "max_length = 400\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "xInput = Input(batch_shape=(None, max_length))\n",
        "xEmbed = Embedding(max_features, 60, input_length = max_length)(xInput)\n",
        "xLstm = Bidirectional(LSTM(64))(xEmbed)\n",
        "xOutput = Dense(1, activation='sigmoid')(xLstm)\n",
        "model = Model(xInput, xOutput)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "embedmodel = Model(xInput,xEmbed)\n",
        "\n",
        "\n",
        "# 학습\n",
        "hist = model.fit(x_train, y_train, \n",
        "                 batch_size=32, \n",
        "                 epochs=10,\n",
        "                 validation_data = (x_test, y_test))\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "y_hat = model.predict(x_test, batch_size=32)\n",
        "y_hat_class = np.round(y_hat, 0)\n",
        "y_hat_class.shape = y_test.shape\n",
        "\n",
        "print ((\"Test accuracy:\"),(np.round(accuracy_score(y_test,y_hat_class),3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eLf_IUa9ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test accuracy: 0.851"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXJ_8kIbvos",
        "colab_type": "text"
      },
      "source": [
        "### Cleaner Code with better explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7snlc6w3anx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMDB Classification using Word Embedding and Conv1D\n",
        "# ----------------------------------------------------\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "max_features = 6000    # max_features : 최대 단어수\n",
        "max_length = 400       # 한 개 리뷰 문서의 최대 단어 길이\n",
        "\n",
        "# IMDB 데이터를 읽어온다.\n",
        "# IMDB 데이터에 사용된 총 단어의 종류는 88,584개 (vocabulary 크기)이다.\n",
        "# IMDB 학습데이터와 시험데이터에는 빈도가 높은 단어 6,000개의 index가 표시돼 있다.\n",
        "# vocabulary의 6,000번째 이후 데이터는 out-of-vocabulary 표시인 '2'가 표시돼 있다.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# 첫 번째 리뷰 문서 x_train[0]의 내용을 확인한다.\n",
        "# 0 : padding, 1 : start, 2 : OOV, 3 : Invalid를 의미한다.\n",
        "print(x_train[0])  # 6,000 이하의 word index로 구성돼 있다.\n",
        "\n",
        "# vocabulary를 생성한다.\n",
        "# word2idx : {'단어' : idx} 구조\n",
        "# idx2word : {idx : '단어'} 구조\n",
        "word2idx = imdb.get_word_index()\n",
        "idx2word = dict((v,k) for k,v in word2idx.items())\n",
        "\n",
        "# volcaburary idx는 1부터 시작한다. idx2word[1] = 'the'\n",
        "# x_train에는 단어들이 vocabulary의 index로 표시돼 있다.\n",
        "# 그러나 idx2word에는 padding=0, start=1, OOV=2, Invalid=3은 포함돼 있지 않다.\n",
        "# idx2word의 idx를 3증가 시키고, 아래와 같이 0, 1, 2, 3을 추가한다.\n",
        "idx2word = dict((v+3, k) for k, v in word2idx.items())\n",
        "idx2word[0] = '<PAD>'  # padding 문자 표시\n",
        "idx2word[1] = '<START>'  # start 문자 표시\n",
        "idx2word[2] = '<OOV>'  # OOV 문자 표시\n",
        "idx2word[3] = '<INV>'  # Invalid 문자 표시\n",
        "word2idx = dict((k, v) for v, k in idx2word.items())\n",
        "\n",
        "# 숫자로 표시된 x_train을 실제 단어로 변환해서 육안으로 확인해 본다.\n",
        "# 학습과는 무관하다.\n",
        "def decode(review):\n",
        "    x = [idx2word[s] for s in review]\n",
        "    return ' '.join(x)\n",
        "decode(x_train[0])\n",
        "####### 여기까지가 주어진 데이터에 관한 부분이다.\n",
        "   \n",
        "\n",
        "# 1개 리뷰 문서의 단어 개수를 max_length = 400으로 맞춘다.\n",
        "# 400개 보다 작으면 padding = 0을 추가하고, 400개 보다 크면 뒷 부분을 자른다.\n",
        "# sequence()에 인자 전달해서 패딩을 뒤쪽에 줄 수도 있다.\n",
        "# RNN의 경우 패딩이 앞에 있는게 유리 - sequantial하기에 0이 훈련중인 단어 앞에(멀리) 있어 vanishing gradient에 유리\n",
        "# CNN의 경우 별 상관 없다.\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "# Deep Learning architecture parameters\n",
        "batch_size = 32\n",
        "embedding_dims = 60\n",
        "num_kernels = 260        # convolution filter 개수\n",
        "kernel_size = 3          # convolution filter size\n",
        "hidden_dims = 300\n",
        "epochs = 10\n",
        "\n",
        "xInput = Input(batch_shape = (None, max_length))\n",
        "emb = Embedding(max_features, embedding_dims)(xInput)\n",
        "emb = Dropout(0.5)(emb)\n",
        "conv = Conv1D(num_kernels, kernel_size, padding='valid', activation='relu', strides=1)(emb)\n",
        "conv = GlobalMaxPooling1D()(conv)\n",
        "ffn = Dense(hidden_dims)(conv)\n",
        "ffn = Dropout(0.5)(ffn)\n",
        "ffn = Activation('relu')(ffn)\n",
        "ffn = Dense(1)(ffn)\n",
        "yOutput = Activation('sigmoid')(ffn)\n",
        "\n",
        "model = Model(xInput, yOutput)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "print(model.summary())\n",
        "\n",
        "# 학습\n",
        "hist = model.fit(x_train, y_train, \n",
        "                 batch_size=batch_size, \n",
        "                 epochs=epochs,\n",
        "                 validation_data = (x_test, y_test))\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 성능 확인\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "print (\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# 1. 특정 단어의 Embedding vector를 확인한다.\n",
        "# ------------------------------------------\n",
        "# Embedding layer의 W를 읽어온다.\n",
        "# 이것이 6,000개 단어에 대한 word embedding vector가 된다.\n",
        "w_emb = np.array(model.layers[1].get_weights())       # shape = (1, 6000, 60)\n",
        "w_emb = w_emb.reshape(max_features, embedding_dims)   # shape = (6000, 60)\n",
        "\n",
        "# father - mother - daughter - son 간의 거리를 측정한다.\n",
        "father = w_emb[word2idx['father']]\n",
        "mother = w_emb[word2idx['mother']]\n",
        "daughter = w_emb[word2idx['daughter']]\n",
        "son = w_emb[word2idx['son']]\n",
        "euclidean_distances([father, mother, daughter, son])\n",
        "\n",
        "# 2. 특정 문장의 Embedding vector를 확인한다.\n",
        "# ------------------------------------------\n",
        "embModel = Model(xInput, emb)\n",
        "m = embModel.predict(x_train[0].reshape(1, max_length))\n",
        "m.shape # (1, 400, 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m2uEajWax0P",
        "colab_type": "text"
      },
      "source": [
        "```euclidean_distances([father, mother, daughter, son])```\n",
        "\n",
        "shows how close 'father' vector is to 'mother', 'son' and 'daughter' vectors\n",
        "```\n",
        "array([[0.        , 1.0991806 , 1.023643  , 0.8721574 ],\n",
        "       [1.0991806 , 0.        , 0.96199644, 1.1721474 ],\n",
        "       [1.023643  , 0.96199644, 0.        , 1.2364552 ],\n",
        "       [0.8721574 , 1.1721474 , 1.2364552 , 0.        ]], dtype=float32)\n",
        "```\n",
        "1. father-son      : 0.8721574\n",
        "2. father-daughter : 1.023643\n",
        "3. father-mother   : 1.0991806"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fsyBtnTbRNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}