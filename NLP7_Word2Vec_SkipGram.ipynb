{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP7_07/22.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOKyyKSddxzFltpSsJ3l38y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP7_Word2Vec_SkipGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tBX2Z2LGPyK",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec\n",
        "\n",
        "Word2vec is a group of related models that are used to produce word embeddings.   \n",
        "\n",
        "Word2Vec은 \n",
        "1. 코퍼스를 학습해 단어들이 벡터화를 통해 관계를 갖도록 하는 기술이다. \n",
        "2. 특정 목적이 없이 범용적으로 사용할 수 있게 벡터화한다. \n",
        "3. 사후적이 아니고 사전에 학습해 단어를 수치 벡터로 표현한다. \n",
        "4. 단어의 주변단어를 참조하여 해당 단어를 수치화한다. 해당 단어는 인접 단어들과 관계가 맺어지고 인접 단어들 간에는 단어 벡터의 유사도가 높다.\n",
        "5. 주변 단어를 참조하기 때문에 distributed representation이라 한다.\n",
        "6. 대표적인 Word2Vec에는 **CBOW**와 **SkipGram**이 있다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVVu2-UB7Emj",
        "colab_type": "text"
      },
      "source": [
        "# Skipgram\n",
        "\n",
        "Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOY2LOAP-Ilz",
        "colab_type": "text"
      },
      "source": [
        "## CBOW vs Skipgram\n",
        "\n",
        "In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle.   \n",
        "While in the Skip-gram model, the distributed representation of the input word is used to predict the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iq1FJ0v_plC",
        "colab_type": "text"
      },
      "source": [
        "![nlp7-1](https://drive.google.com/uc?export=view&id=1rTbqDv7HJ6is21an90xuoYBq2zm4O5FV)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tp-qgZK69ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Skipgram 예시\n",
        "# 소설 alice in wonderland에 사용된 단어들을 2차원 feature로 vector화 한다.\n",
        "# -----------------------------------------------------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "import collections\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 전처리\n",
        "def preprocessing(text):\n",
        "    text2 = \"\".join([\" \" if ch in string.punctuation else ch for ch in text])\n",
        "    tokens = nltk.word_tokenize(text2)\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    \n",
        "    stopwds = stopwords.words('english')\n",
        "    tokens = [token for token in tokens if token not in stopwds]\n",
        "    \n",
        "    tokens = [word for word in tokens if len(word)>=3]\n",
        "    \n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    tagged_corpus = pos_tag(tokens)    \n",
        "    \n",
        "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
        "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def prat_lemmatize(token,tag):\n",
        "        if tag in Noun_tags:\n",
        "            return lemmatizer.lemmatize(token,'n')\n",
        "        elif tag in Verb_tags:\n",
        "            return lemmatizer.lemmatize(token,'v')\n",
        "        else:\n",
        "            return lemmatizer.lemmatize(token,'n')\n",
        "    \n",
        "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
        "\n",
        "    return pre_proc_text\n",
        "\n",
        "# 소설 alice in wonderland를 읽어온다.\n",
        "lines = []\n",
        "fin = open(\"./dataset/alice_in_wonderland.txt\", \"r\")\n",
        "for line in fin:\n",
        "    if len(line) == 0:\n",
        "        continue\n",
        "    lines.append(preprocessing(line))\n",
        "fin.close()\n",
        "\n",
        "# 단어들이 사용된 횟수를 카운트 한다.\n",
        "counter = collections.Counter()\n",
        "\n",
        "for line in lines:\n",
        "    for word in nltk.word_tokenize(line):\n",
        "        counter[word.lower()] += 1\n",
        "\n",
        "# 사전을 구축한다.\n",
        "# 가장 많이 사용된 단어를 1번으로 시작해서 번호를 부여한다.\n",
        "word2idx = {w:(i+1) for i,(w,_) in enumerate(counter.most_common())}\n",
        "idx2word = {v:k for k,v in word2idx.items()}\n",
        "\n",
        "# Trigram으로 학습 데이터를 생성한다.\n",
        "xs = []     # 입력 데이터\n",
        "ys = []     # 출력 데이터\n",
        "for line in lines:\n",
        "    # 사전에 부여된 번호로 단어들을 표시한다.\n",
        "    embedding = [word2idx[w.lower()] for w in nltk.word_tokenize(line)]\n",
        "    \n",
        "    # Trigram으로 주변 단어들을 묶는다.\n",
        "    triples = list(nltk.trigrams(embedding))\n",
        "    \n",
        "    # 왼쪽 단어, 중간 단어, 오른쪽 단어로 분리한다.\n",
        "    w_lefts = [x[0] for x in triples]\n",
        "    w_centers = [x[1] for x in triples]\n",
        "    w_rights = [x[2] for x in triples]\n",
        "    \n",
        "    # 입력 (xs)      출력 (xy)\n",
        "    # ---------    -----------\n",
        "    # 중간 단어 --> 왼쪽 단어\n",
        "    # 중간 단어 --> 오른쪽 단어\n",
        "    xs.extend(w_centers)\n",
        "    ys.extend(w_lefts)\n",
        "    xs.extend(w_centers)\n",
        "    ys.extend(w_rights)\n",
        "\n",
        "# 학습 데이터를 one-hot 형태로 바꾸고, 학습용과 시험용으로 분리한다.\n",
        "vocab_size = len(word2idx) + 1  # 사전의 크기\n",
        "\n",
        "ohe = OneHotEncoder(categories = [range(vocab_size)])\n",
        "X = ohe.fit_transform(np.array(xs).reshape(-1, 1)).todense()\n",
        "Y = ohe.fit_transform(np.array(ys).reshape(-1, 1)).todense()\n",
        "Xtrain, Xtest, Ytrain, Ytest, xstr, xsts = train_test_split(X, Y, xs, test_size=0.2)\n",
        "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
        "\n",
        "# 딥러닝 모델을 생성한다.\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "input_layer = Input(shape = (Xtrain.shape[1],), name=\"input\")\n",
        "first_layer = Dense(300, activation='relu',name = \"first\")(input_layer)\n",
        "first_dropout = Dropout(0.5, name=\"firstdout\")(first_layer)\n",
        "second_layer = Dense(2, activation='relu', name=\"second\")(first_dropout)\n",
        "third_layer = Dense(300,activation='relu', name=\"third\")(second_layer)\n",
        "third_dropout = Dropout(0.5,name=\"thirdout\")(third_layer)\n",
        "fourth_layer = Dense(Ytrain.shape[1], activation='softmax', name = \"fourth\")(third_dropout)\n",
        "\n",
        "model = Model(input_layer, fourth_layer)\n",
        "model.compile(optimizer = \"rmsprop\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "# 학습\n",
        "hist = model.fit(Xtrain, Ytrain, \n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 epochs=NUM_EPOCHS,\n",
        "                 validation_data = (Xtest, Ytest))\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# Extracting Encoder section of the Model for prediction of latent variables\n",
        "encoder = Model(input_layer, second_layer)\n",
        "\n",
        "# Predicting latent variables with extracted Encoder model\n",
        "reduced_X = encoder.predict(Xtest)\n",
        "\n",
        "\n",
        "# 시험 데이터의 단어들에 대한 2차원 latent feature인 reduced_X를\n",
        "# 데이터 프레임으로 정리한다.\n",
        "final_pdframe = pd.DataFrame(reduced_X)\n",
        "final_pdframe.columns = [\"xaxis\",\"yaxis\"]\n",
        "final_pdframe[\"word_indx\"] = xsts\n",
        "final_pdframe[\"word\"] = final_pdframe[\"word_indx\"].map(idx2word)\n",
        "\n",
        "# 데이터 프레임에서 100개를 샘플링한다.\n",
        "rows = final_pdframe.sample(n = 100)\n",
        "labels = list(rows[\"word\"])\n",
        "xvals = list(rows[\"xaxis\"])\n",
        "yvals = list(rows[\"yaxis\"])\n",
        "\n",
        "# 샘플링된 100개 단어를 2차원 공간상에 배치한다.\n",
        "# 거리가 가까운 단어들은 서로 관련이 높은 것들이다.\n",
        "plt.figure(figsize=(15, 15))  \n",
        "\n",
        "for i, label in enumerate(labels):\n",
        "    x = xvals[i]\n",
        "    y = yvals[i]\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(label,xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
        "                 ha='right', va='bottom', fontsize=15)\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urBshtquE8hS",
        "colab_type": "text"
      },
      "source": [
        "![nlp7-2](https://drive.google.com/uc?export=view&id=1_i-rr_YWcT53YqxxTW5osMR9_AHZB8p7)\n",
        "![nlp7-3](https://drive.google.com/uc?export=view&id=1bi0xclWPtgd76qUjPaFndSEst79XqoYY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I7msW3xE0kB",
        "colab_type": "text"
      },
      "source": [
        "model.summary()\n",
        "```\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "input (InputLayer)           [(None, 1787)]            0         \n",
        "_________________________________________________________________\n",
        "first (Dense)                (None, 300)               536400    \n",
        "_________________________________________________________________\n",
        "firstdout (Dropout)          (None, 300)               0         \n",
        "_________________________________________________________________\n",
        "second (Dense)               (None, 2)                 602       \n",
        "_________________________________________________________________\n",
        "third (Dense)                (None, 300)               900       \n",
        "_________________________________________________________________\n",
        "thirdout (Dropout)           (None, 300)               0         \n",
        "_________________________________________________________________\n",
        "fourth (Dense)               (None, 1787)              537887    \n",
        "=================================================================\n",
        "Total params: 1,075,789\n",
        "Trainable params: 1,075,789\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ntJzElpQTys",
        "colab_type": "text"
      },
      "source": [
        "# Skipgram negative sampling\n",
        "\n",
        "Negative sampling allows us to only modify a small percentage of the weights, rather than all of them for each training sample. We do this by slightly modifying our problem. Instead of trying to predict the probability of being a nearby word for all the words in the vocabulary, we try to predict the probability that our training sample words are neighbors or not.\n",
        "\n",
        "So instead of having one giant softmax — classifying among 10,000 classes, we have now turned it into 10,000 binary classification problem.\n",
        "\n",
        "\n",
        "We further simplify the problem by randomly selecting a small number of “negative” words k(a hyper-parameter, let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ADkI9QTQ2UX",
        "colab_type": "text"
      },
      "source": [
        "![7-4](https://drive.google.com/uc?export=view&id=113XE-cSnbF6cOjEKzIINPiFgB1iDzXu1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDLOVKcKRTjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google's pre-trained Word2Vec 사용 예시\n",
        "# download : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
        "# ---------------------------------------------------------------------------\n",
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "# Load Google's pre-trained Word2Vec model.\n",
        "path = 'd:/GoogleWord2Vec/GoogleNews-vectors-negative300.bin'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "        path, binary=True)\n",
        "\n",
        "cat = model['cat']\n",
        "dog = model['dog']\n",
        "human = model['human']\n",
        "\n",
        "np.round(cat, 3)\n",
        "\n",
        "# 유클리디언 거리 유사도\n",
        "np.sqrt(np.sum((cat - dog) ** 2))\n",
        "np.sqrt(np.sum((cat - human) ** 2))\n",
        "\n",
        "# 코사인 유사도\n",
        "def cosDistance(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "cosDistance(cat, dog)\n",
        "cosDistance(cat, human)\n",
        "\n",
        "model.similarity('cat', 'dog')\n",
        "model.similarity('cat', 'human')\n",
        "\n",
        "model.similarity('father', 'daughter')\n",
        "model.similarity('mother', 'daughter')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dQY-LXfToST",
        "colab_type": "text"
      },
      "source": [
        "- output   \n",
        "![npl7-5](https://drive.google.com/uc?export=view&id=1BFEK0mHkgIJbpkpZLSk_qODADI4SR-gW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_HnXOlITxUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}