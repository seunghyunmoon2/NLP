{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP5_7/22.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU1vBQwj8fmWiB/w/BQkW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP5_LDA/TextRank/Sentimentanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IXKMUC84bHd",
        "colab_type": "text"
      },
      "source": [
        "# LDA - gensim\n",
        "\n",
        "> I will be using the Latent Dirichlet Allocation (LDA) from Gensim package along with the Mallet’s implementation (via Gensim). Mallet has an efficient implementation of the LDA. It is known to run faster and gives better topics segregation.\n",
        "\n",
        "> We will also extract the volume and percentage contribution of each topic to get an idea of how important a topic is.\n",
        "\n",
        "[reference](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYb8ax7bKAp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Latent Dirichlet Allocation (LDA) using gensim\n",
        "# ----------------------------------------------\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel as LDA\n",
        "\n",
        "#from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# news data를 읽어와서 저장해 둔다.\n",
        "#newsData = fetch_20newsgroups(shuffle=True, \n",
        "#                              random_state=1, \n",
        "#                              remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "#with open('./dataset/news.data', 'wb') as f:\n",
        "#    pickle.dump(newsData , f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# 저장된 news data를 읽어온다.\n",
        "with open('./dataset/news.data', 'rb') as f:\n",
        "    newsData  = pickle.load(f)\n",
        "\n",
        "# 첫 번째 news를 조회해 본다.\n",
        "news = newsData.data\n",
        "print(len(news))\n",
        "print(news[0])\n",
        "\n",
        "# news 별로 분류된 target을 확인해 본다.\n",
        "print(newsData.target_names)\n",
        "print(len(newsData.target_names))\n",
        "\n",
        "# preprocessing.\n",
        "# 1. 영문자가 아닌 문자를 모두 제거한다.\n",
        "news1 = []\n",
        "for doc in news:\n",
        "    news1.append(re.sub(\"[^a-zA-Z]\", \" \", doc))\n",
        "\n",
        "# 2. 불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 \n",
        "# 단어를 제거한다\n",
        "stop_words = stopwords.words('english')\n",
        "news2 = []\n",
        "for doc in news1:\n",
        "    doc1 = []\n",
        "    for w in doc.split():\n",
        "        w = w.lower()\n",
        "        if len(w) > 3 and w not in stop_words:\n",
        "            doc1.append(w)\n",
        "    news2.append(doc1)\n",
        "    \n",
        "print(news2[0])\n",
        "\n",
        "# doc2bow 생성\n",
        "vocab = corpora.Dictionary(news2)\n",
        "dict(list(vocab.items())[:10])\n",
        "news_bow = [vocab.doc2bow(s) for s in news2]\n",
        "print(news_bow[0])\n",
        "\n",
        "# Latent Dirichlet Allocation (LDA)\n",
        "# ---------------------------------\n",
        "model = LDA(news_bow, \n",
        "            num_topics = len(newsData.target_names), \n",
        "            id2word=vocab)\n",
        "\n",
        "# 문서 별 Topic 번호를 확인한다. (문서 10개만 확인)\n",
        "doc_topic = model.get_document_topics(news_bow)\n",
        "for i in range(10):\n",
        "    dp = np.array(doc_topic[i])\n",
        "    most_likely_topic = int(dp[np.argmax(dp[:, 1]), 0])\n",
        "    print('문서-{:d} : topic = {:d}'.format(i, most_likely_topic))\n",
        "    \n",
        "# topic_term 행렬에서 topic 별로 중요 단어를 표시한다\n",
        "topic_term = model.get_topic_terms(0, topn=10)\n",
        "for i in range(len(newsData.target_names)):\n",
        "    topic_term = model.get_topic_terms(i, topn=10)\n",
        "    idx = [idx for idx, score in topic_term]\n",
        "    print('토픽-{:2d} : '.format(i+1), end='')\n",
        "    for n in idx:\n",
        "        print('{:s} '.format(vocab[n]), end='')\n",
        "    print()\n",
        "\n",
        "# 문서별로 분류된 코드를 확인해 본다.\n",
        "# x, y : 문서 번호\n",
        "def checkTopic(x, y):\n",
        "    print(\"문서 %d의 topic = %s\" % (x, newsData.target_names[newsData.target[x]]))\n",
        "    print(\"문서 %d의 topic = %s\" % (y, newsData.target_names[newsData.target[y]]))\n",
        "\n",
        "checkTopic(2, 5)\n",
        "checkTopic(7, 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDsIH3tZKu7H",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "11314\n",
        "Well i'm not sure about the story nad it did seem biased. What\n",
        "I disagree with is your statement that the U.S. Media is out to\n",
        "ruin Israels reputation. That is rediculous. The U.S. media is\n",
        "the most pro-israeli media in the world. Having lived in Europe\n",
        "I realize that incidences such as the one described in the\n",
        "letter have occured. The U.S. media as a whole seem to try to\n",
        "ignore them. The U.S. is subsidizing Israels existance and the\n",
        "Europeans are not (at least not to the same degree). So I think\n",
        "that might be a reason they report more clearly on the\n",
        "atrocities.\n",
        "\tWhat is a shame is that in Austria, daily reports of\n",
        "the inhuman acts commited by Israeli soldiers and the blessing\n",
        "received from the Government makes some of the Holocaust guilt\n",
        "go away. After all, look how the Jews are treating other races\n",
        "when they got power. It is unfortunate.\n",
        "\n",
        "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
        "20\n",
        "['well', 'sure', 'story', 'seem', 'biased', 'disagree', 'statement', 'media', 'ruin', 'israels', 'reputation', 'rediculous', 'media', 'israeli', 'media', 'world', 'lived', 'europe', 'realize', 'incidences', 'described', 'letter', 'occured', 'media', 'whole', 'seem', 'ignore', 'subsidizing', 'israels', 'existance', 'europeans', 'least', 'degree', 'think', 'might', 'reason', 'report', 'clearly', 'atrocities', 'shame', 'austria', 'daily', 'reports', 'inhuman', 'acts', 'commited', 'israeli', 'soldiers', 'blessing', 'received', 'government', 'makes', 'holocaust', 'guilt', 'away', 'look', 'jews', 'treating', 'races', 'power', 'unfortunate']\n",
        "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 4), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1)]\n",
        "문서-0 : topic = 0\n",
        "문서-1 : topic = 8\n",
        "문서-2 : topic = 18\n",
        "문서-3 : topic = 7\n",
        "문서-4 : topic = 3\n",
        "문서-5 : topic = 8\n",
        "문서-6 : topic = 11\n",
        "문서-7 : topic = 8\n",
        "문서-8 : topic = 2\n",
        "문서-9 : topic = 11\n",
        "토픽- 1 : people would armenian armenians government think said well even right \n",
        "토픽- 2 : filename orbit navy naval system earth lunar shar nuclear program \n",
        "토픽- 3 : drive disk card system hard drivers mouse also controller tape \n",
        "토픽- 4 : game entries period play request season first list send year \n",
        "토픽- 5 : space program would nasa data information also research national technology \n",
        "토픽- 6 : available information mail thanks software would version like know windows \n",
        "토픽- 7 : team games hockey year game play players teams kings last \n",
        "토픽- 8 : would going know president people think said like well time \n",
        "토픽- 9 : would people jesus know like believe think many time even \n",
        "토픽-10 : caps nords jets vpic batting strawberry impulse model salary reactions \n",
        "토픽-11 : file program windows files size entry info line remark know \n",
        "토픽-12 : would good time could like think much also back know \n",
        "토픽-13 : output entry widget char input rules stream build define open \n",
        "토픽-14 : printf easter kent screens allah doug symbol allocation cheers said \n",
        "토픽-15 : ground wire runs year neutral york panel like rockefeller wiring \n",
        "토픽-16 : plastic know like would yankees anyone price good enough paint \n",
        "토픽-17 : shipping sale offer also condition asking price like interested best \n",
        "토픽-18 : chip encryption keys system clipper drive would chips security government \n",
        "토픽-19 : israel jews israeli arab jewish would peace think palestinian time \n",
        "토픽-20 : window scsi display using windows color screen problem mode monitor \n",
        "문서 2의 topic = talk.politics.mideast\n",
        "문서 5의 topic = soc.religion.christian\n",
        "문서 7의 topic = talk.politics.mideast\n",
        "문서 9의 topic = sci.electronics\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xsN8bxyKvGZ",
        "colab_type": "text"
      },
      "source": [
        "## exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb0olX5dLIXI",
        "colab_type": "text"
      },
      "source": [
        "### generate document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YTxmD6ILOkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "theta = np.array([[0.05, 0.15, 0.50, 0.20, 0.10],\n",
        "                  [0.05, 0.05, 0.20, 0.10, 0.60],\n",
        "                  [0.60, 0.15, 0.05, 0.10, 0.10]])\n",
        "\n",
        "beta = np.array([[0.1, 0.1, 0.2, 0.2, 0.2, 0.1, 0.05, 0.05],\n",
        "                 [0.15, 0.25, 0.2, 0.05, 0.05, 0.15, 0.1, 0.05],\n",
        "                 [0.1, 0.2, 0.25, 0.05, 0.25, 0.05, 0.05, 0.05],\n",
        "                 [0.2, 0.2, 0.05, 0.05, 0.15, 0.15, 0.15, 0.05],\n",
        "                 [0.1, 0.2, 0.25, 0.1, 0.1, 0.05, 0.15, 0.05]])\n",
        "\n",
        "w1 = [\"분기\", \"매출\", \"영업이익\", \"영업외이익\", \"실적\", \"증가\", \"감소\", \"계약\"]\n",
        "w2 = [\"수주\", \"계약\", \"발주\", \"호재\", \"기대\", \"매출\", \"실적\", \"약세\"]\n",
        "w3 = [\"주가\", \"약세\", \"하락\", \"부정적\", \"약화\", \"예상\", \"감소\", \"영업이익\"]\n",
        "w4 = [\"원화\", \"환율\", \"영업이익\", \"매출\", \"강세\", \"약세\", \"달러\", \"발행\"]\n",
        "w5 = [\"자본\", \"신주\", \"발행\", \"전환\", \"보통주\", \"기준일\", \"배정\", \"발주\"]\n",
        "words = [w1, w2, w3, w4, w5]\n",
        "\n",
        "sentences = []\n",
        "topic = []\n",
        "for i in range(500):\n",
        "    # theta 1개를 선택한다.\n",
        "    t = np.random.choice([0,1,2])\n",
        "    \n",
        "    # 선택된 theta로 단어 = 50개 짜리 문서를 생성한다.\n",
        "    doc = []\n",
        "    for j in range(50):\n",
        "        # 다항분포로 토픽 1개를 선택한다.\n",
        "        x = np.random.multinomial(1, theta[t])\n",
        "        z = np.argmax(x)  # 토픽 번호\n",
        "        \n",
        "        # z 번째 토픽으로 부터 1개의 단어 (word)를 선택한다.\n",
        "        x = np.random.multinomial(1, beta[z])   # 다항분포 샘플링 (1개)\n",
        "        w = np.argmax(x)  # 단어 번호\n",
        "        word = words[z][w]\n",
        "        doc.append(word)\n",
        "    sentences.append(doc)\n",
        "    \n",
        "with open('dataset/8-6-1.genDoc.pickle', 'wb') as f:\n",
        "    pickle.dump(sentences, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7_LllR8LRDs",
        "colab_type": "text"
      },
      "source": [
        "###  predict LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqRyRAlLLZrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Latent Dirichlet Allocation (LDA) using gensim\n",
        "# ----------------------------------------------\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel as LDA\n",
        "\n",
        "with open('dataset/8-6-1.genDoc.pickle', 'rb') as f:\n",
        "    newsData  = pickle.load(f)\n",
        "\n",
        "# doc2bow 생성\n",
        "vocab = corpora.Dictionary(newsData)\n",
        "news_bow = [vocab.doc2bow(s) for s in newsData]\n",
        "\n",
        "# Latent Dirichlet Allocation (LDA)\n",
        "model = LDA(news_bow, num_topics = 5, id2word=vocab)\n",
        "\n",
        "# 문서 별 Topic 번호를 확인한다. (문서 10개만 확인)\n",
        "doc_topic = model.get_document_topics(news_bow)\n",
        "for i in range(10):\n",
        "    dp = np.array(doc_topic[i])\n",
        "    most_likely_topic = int(dp[np.argmax(dp[:, 1]), 0])\n",
        "    print('문서-{:d} : topic = {:d}'.format(i, most_likely_topic))\n",
        "    \n",
        "# topic_term 행렬에서 topic 별로 중요 단어를 표시한다\n",
        "for i in range(5):\n",
        "    topic_term = model.get_topic_terms(i, topn=6)\n",
        "    idx = [idx for idx, score in topic_term]\n",
        "    print('토픽-{:2d} : '.format(i+1), end='')\n",
        "    for n in idx:\n",
        "        print('{:s} '.format(vocab[n]), end='')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB4rZWwLLsL3",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "문서-0 : topic = 1\n",
        "문서-1 : topic = 2\n",
        "문서-2 : topic = 1\n",
        "문서-3 : topic = 1\n",
        "문서-4 : topic = 1\n",
        "문서-5 : topic = 0\n",
        "문서-6 : topic = 0\n",
        "문서-7 : topic = 4\n",
        "문서-8 : topic = 2\n",
        "문서-9 : topic = 4\n",
        "토픽- 1 : 실적 영업외이익 영업이익 발행 계약 매출 \n",
        "토픽- 2 : 발행 신주 전환 배정 보통주 자본 \n",
        "토픽- 3 : 약화 하락 약세 발행 발주 계약 \n",
        "토픽- 4 : 약세 발행 하락 약화 신주 배정 \n",
        "토픽- 5 : 실적 매출 영업이익 약세 영업외이익 계약 \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQT5kh8v4bsx",
        "colab_type": "text"
      },
      "source": [
        "# PaperRank algorithm\n",
        "\n",
        "> PageRank (PR) is an algorithm used by Google Search to rank web pages in their search engine results. PageRank was named after Larry Page, one of the founders of Google. PageRank is a way of measuring the importance of website pages.\n",
        "\n",
        "![PR](https://drive.google.com/uc?export=view&id=1mN27Gdo4KrgOc-yOC9LtyvF2-XLNvAPW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLShe4gA4-oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 'summarizer' module provides functions for summarizing texts. \n",
        "# Summarizing is based on ranks of text sentences using a variation \n",
        "# of the TextRank algorithm.\n",
        "#\n",
        "# Federico Barrios, et, al., 2016, Variations of the Similarity Function \n",
        "# of TextRank for Automated Summarization, https://arxiv.org/abs/1602.03606\n",
        "#\n",
        "# Barrios는 tfidf 대신 BM25, BM25+를 사용했고, cosine similarity를 사용했다.\n",
        "# gensim.summarizer도 Barrios의 TextRank를 사용한다.\n",
        "from gensim.summarization.summarizer import summarize\n",
        "\n",
        "text = \\\n",
        "'''Rice Pudding - Poem by Alan Alexander Milne\n",
        "What is the matter with Mary Jane?\n",
        "She's crying with all her might and main,\n",
        "And she won't eat her dinner - rice pudding again -\n",
        "What is the matter with Mary Jane?\n",
        "What is the matter with Mary Jane?\n",
        "I've promised her dolls and a daisy-chain,\n",
        "And a book about animals - all in vain -\n",
        "What is the matter with Mary Jane?\n",
        "What is the matter with Mary Jane?\n",
        "She's perfectly well, and she hasn't a pain;\n",
        "But, look at her, now she's beginning again! -\n",
        "What is the matter with Mary Jane?\n",
        "What is the matter with Mary Jane?\n",
        "I've promised her sweets and a ride in the train,\n",
        "And I've begged her to stop for a bit and explain -\n",
        "What is the matter with Mary Jane?\n",
        "What is the matter with Mary Jane?\n",
        "She's perfectly well and she hasn't a pain,\n",
        "And it's lovely rice pudding for dinner again!\n",
        "What is the matter with Mary Jane?\n",
        "'''\n",
        "# ratio (float, optional) – Number between 0 and 1 that determines the \n",
        "# proportion of the number of sentences of the original text to be chosen \n",
        "# for the summary.\n",
        "s = summarize(text, ratio = 0.2)\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzRRFYBI7RhS",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "And she won't eat her dinner - rice pudding again -\n",
        "I've promised her dolls and a daisy-chain,\n",
        "I've promised her sweets and a ride in the train,\n",
        "And it's lovely rice pudding for dinner again!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HL3OsO64b1R",
        "colab_type": "text"
      },
      "source": [
        "# TextRank algorithm\n",
        "\n",
        "> TextRank is a general purpose, graph based ranking algorithm for NLP. TextRank is an automatic summarisation technique. Graph-based ranking algorithms are a way for deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.\n",
        "\n",
        "![textalgo](https://drive.google.com/uc?export=view&id=1STAL53o1DiWMhOJM7D56odyWEX2YdYBC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXQlCdpl7cEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 인공지능이 생성한 논문 사이트에서 논문 한 편을 읽어와서 주요 문장을\n",
        "# 추출한다.\n",
        "from gensim.summarization import summarize\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = 'http://scigen.csail.mit.edu/scicache/269/scimakelatex.25977.Admoni.Moskalskaia.Schendels.html'\n",
        "r = requests.get(url)\n",
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "data = soup.get_text()\n",
        "print(data[:2000])\n",
        "\n",
        "pos1 = data.find('Introduction') + len(\"Introduction\")\n",
        "pos2 = data.find(\"Related Work\")\n",
        "\n",
        "text = data[pos1:pos2].strip()\n",
        "summary = summarize(text, ratio=0.1)\n",
        "print(text)\n",
        "\n",
        "print(\"PAPER URL: \\n{}\\n\".format(url))\n",
        "print(\"GENERATED SUMMARY: \\n{}\".format(summary))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQeClx634cFs",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "GENERATED SUMMARY: \n",
        "We emphasize that our heuristic develops\n",
        "a cycle of four phases: allowance, evaluation, investigation, and\n",
        "motivate the need for wide-area networks [2].\n",
        "can be applied to the exploration of local-area networks.\n",
        "same lines, we prove the development of linked lists.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mtyQIYc9ZCI",
        "colab_type": "text"
      },
      "source": [
        "# Anaphora Resolution\n",
        "\n",
        "> Anaphora resolution (AR) which most commonly appears as pronoun resolution is the problem of resolving references to earlier or later items in the discourse. These items are usually noun phrases representing objects in the real world called referents but can also be verb phrases, whole sentences or paragraphs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d89rvbZNZJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Anaphora resolution 예시\n",
        "import nltk\n",
        "from nltk.chunk import tree2conlltags\n",
        "from nltk.corpus import names\n",
        "import random\n",
        "\n",
        "# name의 마지막 철자를 리턴한다.\n",
        "def feature(word):\n",
        "    return {'last(1)' : word[-1]}\n",
        "\n",
        "# name corpus를 읽어온다.\n",
        "males = [(name, 'male') for name in names.words('male.txt')]\n",
        "females = [(name, 'female') for name in names.words('female.txt')]\n",
        "\n",
        "print(males[:10])\n",
        "print(females[:10])\n",
        "\n",
        "combined = males + females\n",
        "random.shuffle(combined)\n",
        "\n",
        "# supervised learning용 학습  데이터를 생성한다.\n",
        "# 이름의 마지막 철자로 성별 (male or female)을 학습하기 위한 것이다.\n",
        "training = [(feature(name), gender) for (name, gender) in combined]\n",
        "print(training[:10])\n",
        "\n",
        "# Naive Bayes로 학습한다.\n",
        "classifier = nltk.NaiveBayesClassifier.train(training)\n",
        "\n",
        "sentences = [\n",
        "    \"John is a man. He walks\",\n",
        "    \"John and Mary are married. They have two kids\",\n",
        "    \"In order for Ravi to be successful, he should follow John\",\n",
        "    \"John met Mary in Barista. She asked him to order a Pizza\"\n",
        "]\n",
        "\n",
        "# name의 마지막 철자로 성별을 예상한다.\n",
        "def gender(word):\n",
        "    return classifier.classify(feature(word))\n",
        "\n",
        "# 문장을 chunk로 분해해서 사람과 연관된 대명사를 찾는다.\n",
        "for sent in sentences:\n",
        "    chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)), binary=False)\n",
        "    stack = []\n",
        "    print(sent)\n",
        "    items = tree2conlltags(chunks)\n",
        "    for item in items:\n",
        "        if item[1] == 'NNP' and (item[2] == 'B-PERSON' or item[2] == 'O'):\n",
        "            stack.append((item[0], gender(item[0])))\n",
        "        elif item[1] == 'CC':\n",
        "            stack.append(item[0])\n",
        "        elif item[1] == 'PRP':\n",
        "            stack.append(item[0])\n",
        "    print(\"\\t {}\".format(stack))\n",
        "\n",
        "print(items)\n",
        "print(chunks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBMI6eFoNdyg",
        "colab_type": "text"
      },
      "source": [
        "* output\n",
        "```\n",
        "[('Aamir', 'male'), ('Aaron', 'male'), ('Abbey', 'male'), ('Abbie', 'male'), ('Abbot', 'male'), ('Abbott', 'male'), ('Abby', 'male'), ('Abdel', 'male'), ('Abdul', 'male'), ('Abdulkarim', 'male')]\n",
        "[('Abagael', 'female'), ('Abagail', 'female'), ('Abbe', 'female'), ('Abbey', 'female'), ('Abbi', 'female'), ('Abbie', 'female'), ('Abby', 'female'), ('Abigael', 'female'), ('Abigail', 'female'), ('Abigale', 'female')]\n",
        "[({'last(1)': 'e'}, 'female'), ({'last(1)': 'l'}, 'female'), ({'last(1)': 'l'}, 'female'), ({'last(1)': 'd'}, 'male'), ({'last(1)': 'r'}, 'female'), ({'last(1)': 'e'}, 'female'), ({'last(1)': 'n'}, 'male'), ({'last(1)': 'g'}, 'male'), ({'last(1)': 'y'}, 'male'), ({'last(1)': 'd'}, 'male')]\n",
        "John is a man. He walks\n",
        "\t [('John', 'male'), 'He']\n",
        "John and Mary are married. They have two kids\n",
        "\t [('John', 'male'), 'and', ('Mary', 'female'), 'They']\n",
        "In order for Ravi to be successful, he should follow John\n",
        "\t [('Ravi', 'female'), 'he', ('John', 'male')]\n",
        "John met Mary in Barista. She asked him to order a Pizza\n",
        "\t [('John', 'male'), ('Mary', 'female'), 'She', 'him']\n",
        "[('John', 'NNP', 'B-PERSON'), ('met', 'VBD', 'O'), ('Mary', 'NNP', 'O'), ('in', 'IN', 'O'), ('Barista', 'NNP', 'B-GPE'), ('.', '.', 'O'), ('She', 'PRP', 'O'), ('asked', 'VBD', 'O'), ('him', 'PRP', 'O'), ('to', 'TO', 'O'), ('order', 'NN', 'O'), ('a', 'DT', 'O'), ('Pizza', 'NN', 'O')]\n",
        "(S\n",
        "  (PERSON John/NNP)\n",
        "  met/VBD\n",
        "  Mary/NNP\n",
        "  in/IN\n",
        "  (GPE Barista/NNP)\n",
        "  ./.\n",
        "  She/PRP\n",
        "  asked/VBD\n",
        "  him/PRP\n",
        "  to/TO\n",
        "  order/NN\n",
        "  a/DT\n",
        "  Pizza/NN)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whLaJ0qXNRmi",
        "colab_type": "text"
      },
      "source": [
        "# WSD - Word Sense Disabiguation\n",
        "\n",
        "> In natural language processing, word sense disambiguation (WSD) is the problem of determining which \"sense\" (meaning) of a word is activated by the use of the word in a particular context, a process which appears to be largely unconscious in people. WSD is a natural classification problem: Given a word and its possible senses, as defined by a dictionary, classify an occurrence of the word in context into one or more of its sense classes. The features of the context (such as neighboring words) provide the evidence for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFYKtrsBN51G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word Sense Disambiguation (WSD)\n",
        "import nltk\n",
        "\n",
        "def understandWordSenseExamples():\n",
        "    words = ['wind', 'date', 'left']\n",
        "    print(\"-- examples --\")\n",
        "    for word in words:\n",
        "        syns = nltk.corpus.wordnet.synsets(word)\n",
        "        for syn in syns[:2]:\n",
        "            for example in syn.examples()[:2]:\n",
        "                print(\"{} -> {} -> {}\".format(word, syn.name(), example))\n",
        "\n",
        "understandWordSenseExamples()\n",
        "\n",
        "def understandBuiltinWSD():\n",
        "    print(\"-- built-in wsd --\")\n",
        "    maps = [\n",
        "        ('Is it the fish net that you are using to catch fish ?', 'fish', 'n'),\n",
        "        ('Please dont point your finger at others.', 'point', 'n'),\n",
        "        ('I went to the river bank to see the sun rise', 'bank', 'n'),\n",
        "    ]\n",
        "    for m in maps:\n",
        "        print(\"Sense '{}' for '{}' -> '{}'\".format(m[0], m[1], \n",
        "              nltk.wsd.lesk(m[0], m[1], m[2])))\n",
        "\n",
        "understandBuiltinWSD()\n",
        "\n",
        "nltk.corpus.wordnet.synsets('fish')\n",
        "nltk.corpus.wordnet.synset('pisces.n.02').lemma_names()\n",
        "nltk.corpus.wordnet.synset('pisces.n.02').definition()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsmjJAuzN8Vu",
        "colab_type": "text"
      },
      "source": [
        "* output   \n",
        "```\n",
        "# examples \n",
        "wind -> wind.n.01 -> trees bent under the fierce winds\n",
        "wind -> wind.n.01 -> when there is no wind, row\n",
        "wind -> wind.n.02 -> the winds of change\n",
        "date -> date.n.01 -> what is the date today?\n",
        "date -> date.n.02 -> his date never stopped talking\n",
        "left -> left.n.01 -> she stood on the left\n",
        "# built-in wsd \n",
        "Sense 'Is it the fish net that you are using to catch fish ?' for 'fish' -> 'Synset('pisces.n.02')'\n",
        "Sense 'Please dont point your finger at others.' for 'point' -> 'Synset('point.n.25')'\n",
        "Sense 'I went to the river bank to see the sun rise' for 'bank' -> 'Synset('savings_bank.n.02')'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk2o6rwhOArP",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment analysis\n",
        "\n",
        "> Natural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics. The goal is for computers to process or “understand” natural language in order to perform various human like tasks like language translation or answering questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFqL0GFqObj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 감정 분석\n",
        "import nltk\n",
        "import nltk.sentiment.sentiment_analyzer\n",
        "\n",
        "def wordBasedSentiment():\n",
        "    positive_words = ['love', 'hope', 'joy']\n",
        "    text = 'Rainfall this year brings lot of hope and joy to Farmers.'.split()\n",
        "    analysis = nltk.sentiment.util.extract_unigram_feats(text, positive_words)\n",
        "    print(' -- single word sentiment --')\n",
        "    print(analysis)\n",
        "    \n",
        "def multiWordBasedSentiment():\n",
        "    word_sets = [('heavy', 'rains'), ('flood', 'bengaluru')]\n",
        "    text = 'heavy rains cause flash flooding in bengaluru'.split()\n",
        "    analysis = nltk.sentiment.util.extract_bigram_feats(text, word_sets)\n",
        "    print(' -- multi word sentiment --')\n",
        "    print(analysis)\n",
        "\n",
        "def markNegativity(text):\n",
        "    negation = nltk.sentiment.util.mark_negation(text.split())\n",
        "    print(' -- negativity --')\n",
        "    print(negation)\n",
        "\n",
        "wordBasedSentiment()\n",
        "multiWordBasedSentiment()\n",
        "\n",
        "# 주어진 문장에서 부정적 의미를 가진 모든 단어에 대해 접미사 _NEG를 표시한다.\n",
        "markNegativity('Rainfall last year did not bring joy to Farmers')\n",
        "markNegativity(\"I didn't like this movie . It was bad.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLDhIbELOkQv",
        "colab_type": "text"
      },
      "source": [
        "output\n",
        "\n",
        "```\n",
        " -- single word sentiment --\n",
        "{'contains(love)': False, 'contains(hope)': True, 'contains(joy)': True}\n",
        " -- multi word sentiment --\n",
        "{'contains(heavy - rains)': True, 'contains(flood - bengaluru)': False}\n",
        " -- negativity --\n",
        "['Rainfall', 'last', 'year', 'did', 'not', 'bring_NEG', 'joy_NEG', 'to_NEG', 'Farmers_NEG']\n",
        " -- negativity --\n",
        "['I', \"didn't\", 'like_NEG', 'this_NEG', 'movie_NEG', '.', 'It', 'was', 'bad.']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72jQJF5nOcPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VADER-Sentiment-Analysis\n",
        "import nltk\n",
        "import nltk.sentiment.util\n",
        "import nltk.sentiment.sentiment_analyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.downloader.download('vader_lexicon', download_dir='./dataset/')\n",
        "\n",
        "def mySentimentAnalyzer():\n",
        "    def score_feedback(text):\n",
        "        positive_words = ['love', 'genuine', 'liked']\n",
        "        if '_NEG' in ' '.join(nltk.sentiment.util.mark_negation(text.split())):\n",
        "            score = -1\n",
        "        else:\n",
        "            analysis = nltk.sentiment.util.extract_unigram_feats(text.split(), positive_words)\n",
        "            if True in analysis.values():\n",
        "                score = 1\n",
        "            else:\n",
        "                score = 0\n",
        "        return score\n",
        "\n",
        "    feedback = \"\"\"I love the items in this shop, very genuine and quality is well maintained.\n",
        "    I have visited this shop and had samosa, my friends liked it very much.\n",
        "    ok average food in this shop.\n",
        "    Fridays are very busy in this shop, do not place orders during this day.\"\"\"\n",
        "    \n",
        "    print(' -- custom scorer --')\n",
        "    for text in feedback.split(\"\\n\"):\n",
        "        print(\"score = {} for >> {}\".format(score_feedback(text), text))\n",
        "\n",
        "def advancedSentimentAnalyzer():\n",
        "    sentences = [\n",
        "        ':)',\n",
        "        ':(',\n",
        "        'She is so :(',\n",
        "        'I love the way cricket is played by the champions',\n",
        "        'She neither likes coffee nor tea',\n",
        "    ]\n",
        "    \n",
        "    senti = SentimentIntensityAnalyzer()\n",
        "    print(' -- built-in intensity analyser --')\n",
        "    for sentence in sentences:\n",
        "        print('[{}]'.format(sentence), end=' --> ')\n",
        "        kvp = senti.polarity_scores(sentence)\n",
        "        for k in kvp:\n",
        "            print('{} = {}, '.format(k, kvp[k]), end='')\n",
        "        print()\n",
        "\n",
        "mySentimentAnalyzer()\n",
        "advancedSentimentAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bed7btYSOz5P",
        "colab_type": "text"
      },
      "source": [
        "output\n",
        "```\n",
        " -- single word sentiment --\n",
        "{'contains(love)': False, 'contains(hope)': True, 'contains(joy)': True}\n",
        " -- multi word sentiment --\n",
        "{'contains(heavy - rains)': True, 'contains(flood - bengaluru)': False}\n",
        " -- negativity --\n",
        "['Rainfall', 'last', 'year', 'did', 'not', 'bring_NEG', 'joy_NEG', 'to_NEG', 'Farmers_NEG']\n",
        " -- negativity --\n",
        "['I', \"didn't\", 'like_NEG', 'this_NEG', 'movie_NEG', '.', 'It', 'was', 'bad.']\n",
        "```"
      ]
    }
  ]
}