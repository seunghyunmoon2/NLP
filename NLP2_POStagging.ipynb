{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP2-POStagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPY7ZLnVpLjNnw0lUm0qL5T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP2_POStagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zaQGxWSC6cY",
        "colab_type": "text"
      },
      "source": [
        "1. # POS(Part Of Speech) Tagging\n",
        "* '품사' 태깅\n",
        "    * [nlpk_documentation](https://www.nltk.org/book/ch05.html)\n",
        "    * [in_korean](https://excelsior-cjh.tistory.com/71)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yr4NmTvDlPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#POS TAGGING\n",
        "import nltk\n",
        "\n",
        "#see how it works\n",
        "text = \"And now for the first time in forever there will be music\"\n",
        "token = nltk.word_tokenize(text)\n",
        "nltk.pos_tag(token)\n",
        "\"\"\"\n",
        "[('And', 'CC'),\n",
        " ('now', 'RB'),\n",
        " ('for', 'IN'),\n",
        " ('the', 'DT'),\n",
        " ('first', 'JJ'),\n",
        " ('time', 'NN'),\n",
        " ('in', 'IN'),\n",
        " ('forever', 'NN'),\n",
        " ('there', 'EX'),\n",
        " ('will', 'MD'),\n",
        " ('be', 'VB'),\n",
        " ('music', 'NN')]\"\"\"\n",
        "print(type(nltk.pos_tag(token))) #list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkxLjkEkFIne",
        "colab_type": "text"
      },
      "source": [
        "## 한국판\n",
        "1. 국립국어원의 세종(?)무시기   \n",
        "2. konlpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBAs3siEFnNQ",
        "colab_type": "text"
      },
      "source": [
        "## Tagged Corpus\n",
        "\n",
        "* there's many different tagged corpus from different organizations(schools, etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0gZcjL6GRfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Penn treebank\n",
        "print(nltk.corpus.treebank.tagged_words()[:20])\n",
        "\n",
        "#Brown Corpus\n",
        "print(nltk.corpus.brown.tagged_words()[:20])\n",
        "print(nltk.corpus.treebank.tagged_words(tagset='universal')[:20])\n",
        "\n",
        "#NPS Chat\n",
        "#they have a tag for emojis (':P' , 'UH')\n",
        "print(nltk.corpus.nps_chat.tagged_words()[:20])\n",
        "\n",
        "#CONLL Corpus2000\n",
        "print(nltk.corpus.conll2000.tagged_words()[:20])\n",
        "\n",
        "#frequency distribution of tags in Brown Corpus\n",
        "#from nltk.corpus import brown\n",
        "brown_news_tagged = nltk.corpus.brown.tagged_words(categories='news',tagset='universal')\n",
        "tag_fd = nltk.FreqDist(tag for (word,tag) in brown_news_tagged)\n",
        "tag_fd.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCb4b6D6R1YX",
        "colab_type": "text"
      },
      "source": [
        "2. # Hidden Markov Model\n",
        "\n",
        "* for a better result of pos tagging, hmm is used to calculate the probability of the components of given sentences of which we will need to **tag**\n",
        "    * a sentence, `for example they can fish`, has many numbers of cases.\n",
        "    * i.e.        **N V N**, **N V V**\n",
        "\n",
        "* only for `sequential` data\n",
        "    * comes along with LSTM models later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnv1UGAoinOv",
        "colab_type": "text"
      },
      "source": [
        "1. ## simple introduction to hmm with forward, backward, Viterbi and Baum-Welch Algorithm\n",
        "* there are two days : *rainy days*, *sunny days*\n",
        "* we can do three options to do each day : *cleaning*, *going shopping*, *going walking*\n",
        "* say, we want to know the probability of day 4, **raining**&**walking**, given the history of the last three days.\n",
        "\n",
        "* ## might add some graphics later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IldTEFi4A8C",
        "colab_type": "text"
      },
      "source": [
        "![copywright to 아마추어퀀트](https://drive.google.com/uc?export=view&id=17hEqQIJDwMMnDzysgrFxpItDwIPInQ5C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2but2utYkoL2",
        "colab_type": "text"
      },
      "source": [
        "2. ## Forward Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMjcDvHbkoOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"!pip install hmmlearn\"\"\"\n",
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# 히든 상태 정의\n",
        "states = [\"Rainy\", \"Sunny\"]\n",
        "nState = len(states)\n",
        "\n",
        "# 관측 데이터 정의\n",
        "observations = [\"Walk\", \"Shop\", \"Clean\"]\n",
        "nObervation = len(observations)\n",
        "\n",
        "# HMM 모델 빌드\n",
        "model = hmm.MultinomialHMM(n_components=nState)\n",
        "model.startprob_ = np.array([0.6, 0.4])\n",
        "model.transmat_ = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
        "model.emissionprob_ = np.array([[0.1, 0.4, 0.5], [0.6, 0.3, 0.1]])\n",
        "\n",
        "# 관측 데이터 (Observations)\n",
        "X = np.array([[0, 2, 1]]).T  # Walk, Clean, Shop\n",
        "\n",
        "# Forwad(/Backward) algorithm으로 x가 관측될 likely probability 계산\n",
        "logL = model.score(X)\n",
        "p = np.exp(logL)\n",
        "print(\"\\nProbability of [Walk, Clean, Shop] = %.4f%s\" % (p*100, '%'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k0IZs_vkoQE",
        "colab_type": "text"
      },
      "source": [
        "* output\n",
        "\n",
        "```\n",
        "In [13]: logL\n",
        "Out[13]: -3.472543018732704\n",
        "\n",
        "Probability of [Walk, Clean, Shop] = 3.1038%\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEgTKQupkoSM",
        "colab_type": "text"
      },
      "source": [
        "3. ## Viterbi Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTQ5Fw3J68Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# 히든 상태 정의\n",
        "states = [\"Rainy\", \"Sunny\"]\n",
        "nState = len(states)\n",
        "\n",
        "# 관측 데이터 정의\n",
        "observations = [\"Walk\", \"Shop\", \"Clean\"]\n",
        "nObervation = len(observations)\n",
        "\n",
        "# HMM 모델 빌드\n",
        "model = hmm.MultinomialHMM(n_components=nState)\n",
        "model.startprob_ = np.array([0.6, 0.4])\n",
        "model.transmat_ = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
        "model.emissionprob_ = np.array([[0.1, 0.4, 0.5], [0.6, 0.3, 0.1]])\n",
        "\n",
        "# 관측 데이터 (Observations)\n",
        "X = np.array([[0, 2, 1, 0]]).T\n",
        "\n",
        "# Viterbi 알고리즘으로 히든 상태 시퀀스 추정 (Decode)\n",
        "logprob, Z = model.decode(X, algorithm=\"viterbi\")\n",
        "\n",
        "# 결과 출력\n",
        "print(\"\\n  Obervation Sequence :\", \", \".join(map(lambda x: observations[int(x)], X)))\n",
        "print(\"Hidden State Sequence :\", \", \".join(map(lambda x: states[int(x)], Z)))\n",
        "print(\"Probability = %.6f\" % np.exp(logprob))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF3Pmmix7H9B",
        "colab_type": "text"
      },
      "source": [
        "* output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0RpwZXE6-x3",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "Obervation Sequence : Walk, Clean, Shop, Walk\n",
        "Hidden State Sequence : Sunny, Rainy, Rainy, Sunny\n",
        "Probability = 0.002419\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnAqRZOo7Fqp",
        "colab_type": "text"
      },
      "source": [
        "## Baum-Welch Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJdEm5oQ-AgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Observation 시퀀스만을 이용하여, 초기 확률, Transition, Emmision 확률을 \n",
        "# 추정하고, 히든 상태 시퀀스를 추정한다. (Baum Welch 알고리즘)\n",
        "#\n",
        "# 2019.3.27 아마추어퀀트 (blog.naver.com/chunjein)\n",
        "# ----------------------------------------------------------------------\n",
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "nState = 2\n",
        "pStart = [0.6, 0.4]\n",
        "pTran = [[0.7, 0.3], [0.2, 0.8]]\n",
        "pEmit = [[0.1, 0.4, 0.5], [0.6, 0.3, 0.1]]\n",
        "\n",
        "# 1. 주어진 확률 분포대로 관측 데이터 시퀀스를 생성한다.\n",
        "# ---------------------------------------------------\n",
        "# 히든 상태 선택. 확률 = [0.6, 0.4]\n",
        "s = np.argmax(np.random.multinomial(1, pStart, size=1))\n",
        "\n",
        "X = []      # Obervation 시퀀스\n",
        "Z = []      # 히든 상태 시퀀스\n",
        "for i in range(5000):\n",
        "    # Walk, Shop, Clean ?\n",
        "    a = np.argmax(np.random.multinomial(1, pEmit[s], size=1))\n",
        "    X.append(a)\n",
        "    Z.append(s)\n",
        "    \n",
        "    # 히든 상태 천이\n",
        "    s = np.argmax(np.random.multinomial(1, pTran[s], size=1))\n",
        "\n",
        "X = np.array(X)\n",
        "X = np.reshape(X, [len(X), 1])\n",
        "Z = np.array(Z)\n",
        "\n",
        "# 2. Observation 시퀀스만을 이용하여, 초기 확률, Transition, Emmision 확률을 추정하고,\n",
        "#    히든 상태 시퀀스를 추정한다. (Baum Welch 알고리즘)\n",
        "# EM 알고리즘은 local optimum에 빠질 수 있으므로, 5번 반복하여 로그 우도값이 가장\n",
        "# 작은 결과를 채택한다.\n",
        "# ---------------------------------------------------------------------------------\n",
        "zHat = np.zeros(len(Z))\n",
        "minprob = 999999999\n",
        "for k in range(5):\n",
        "    model = hmm.MultinomialHMM(n_components=nState, tol=0.0001, n_iter=10000)\n",
        "    model = model.fit(X)\n",
        "    predZ = model.predict(X)\n",
        "    logprob = -model.score(X)\n",
        "    \n",
        "    if logprob < minprob:\n",
        "        zHat = predZ\n",
        "        T = model.transmat_\n",
        "        E = model.emissionprob_\n",
        "        minprob = logprob\n",
        "    print(\"k = %d, logprob = %.2f\" % (k, logprob))\n",
        "\n",
        "# 3. 1 단계에서 생성한 Z와 추정한 zHat의 정확도를 측정한다.\n",
        "# --------------------------------------------------------\n",
        "# 함수 동작 중에 [Rainy, Sunny]의 인덱싱이 0,1 or 1,0 둘 중 뭐가 되는지 모르기에\n",
        "# 0.5이하라면 switch해준다.\n",
        "accuracy = (Z == zHat).sum() / len(Z)\n",
        "if accuracy < 0.5:\n",
        "    T = np.fliplr(np.flipud(T))\n",
        "    E = np.flipud(E)\n",
        "    zHat = 1 - zHat\n",
        "    print(\"flipped\")\n",
        "\n",
        "accuracy = (Z == zHat).sum() / len(Z)\n",
        "print(\"\\naccuracy = %.2f %s\" % (accuracy * 100, '%'))\n",
        "\n",
        "# 추정 결과를 출력한다\n",
        "print(\"\\nlog prob = %.2f\" % minprob)\n",
        "print(\"\\nstart prob :\\n\", model.startprob_)\n",
        "print(\"\\ntrans prob :\\n\",T)\n",
        "print(\"\\nemiss prob :\\n\", E)\n",
        "print(\"\\niteration = \", model.monitor_.iter)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hdLlw7g9-Wh",
        "colab_type": "text"
      },
      "source": [
        "* output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqMDlwgL9oGG",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "k = 0, logprob = 5353.05\n",
        "k = 1, logprob = 5353.05\n",
        "k = 2, logprob = 5353.05\n",
        "k = 3, logprob = 5353.05\n",
        "k = 4, logprob = 5353.05\n",
        "flipped\n",
        "\n",
        "accuracy = 77.02 %   #굉장히 높은 편\n",
        "\n",
        "log prob = 5353.05\n",
        "\n",
        "start prob :\n",
        " [1.00e+000 5.66e-172]\n",
        "\n",
        "trans prob :  #위 코드의 매트릭스와 비슷하다.\n",
        " [[0.68 0.32]\n",
        " [0.21 0.79]]\n",
        "\n",
        "emiss prob :  #위 코드의 매트릭스와 비슷하다.\n",
        " [[0.09 0.42 0.49]\n",
        " [0.61 0.28 0.11]]\n",
        "\n",
        "iteration =  352\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80XOOeIF91G2",
        "colab_type": "text"
      },
      "source": [
        "# Back to NLTK CORPUS tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXr0rn0MW_dD",
        "colab_type": "text"
      },
      "source": [
        "## some tagging exercises\n",
        "\n",
        "\n",
        "read pos of brown corpus and estimate parameters of hmm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYdAjBcqW7Hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagged_words = []\n",
        "all_tags = []\n",
        "\n",
        "for sent in nltk.corpus.brown.tagged_sents(tagset='universal'):\n",
        "    tagged_words.append(('START','START'))\n",
        "    all_tags.append('START')\n",
        "    for (word, tag) in sent:\n",
        "        all_tags.append(tag)\n",
        "        tagged_words.append((tag,word))\n",
        "    tagged_words.append(('END','END'))\n",
        "    all_tags.append('END')\n",
        "    \n",
        "# Transition probability (Bigram)\n",
        "cfd_tags = nltk.ConditionalFreqDist(nltk.bigrams(all_tags))\n",
        "cpd_tags = nltk.ConditionalProbDist(cfd_tags,nltk.MLEProbDist)\n",
        "print('Count(\\'DET\\',\\'NOUN\\') =',cfd_tags['DET']['NOUN']) # # of times appear noun after an article\n",
        "print('P(\\'NOUN\\'|\\'DET\\') =',cpd_tags['DET'].prob('NOUN')) # # probability\n",
        "\n",
        "# Emission prob\n",
        "cfd_tagwords = nltk.ConditionalFreqDist(tagged_words)\n",
        "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords,nltk.MLEProbDist)\n",
        "\n",
        "print('Count(\\'DET\\',\\'the\\') =',cfd_tagwords['DET']['the']) # # of times the appears(?)\n",
        "print('P(\\'the\\'|\\'DET\\') =',cpd_tagwords['DET'].prob('the')) # prob of the"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbBDgyF8YWgR",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "Count('DET','NOUN') = 85838\n",
        "P('NOUN'|'DET') = 0.6264678621213117\n",
        "\n",
        "Count('DET','the') = 62710\n",
        "P('the'|'DET') = 0.45767375327509324\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejkR1zqdYHNN",
        "colab_type": "text"
      },
      "source": [
        "## pos positioned before NOUNs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZvH3EkkYUGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_tag_pairs = nltk.bigrams(brown_news_tagged)\n",
        "bigram = [(a,b) for (a,b) in word_tag_pairs]\n",
        "print(bigram[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INJ5bx0eZTPP",
        "colab_type": "text"
      },
      "source": [
        "> the output is bigrams!\n",
        "```\n",
        "[(('The', 'DET'), ('Fulton', 'NOUN')), (('Fulton', 'NOUN'), ('County', 'NOUN')), (('County', 'NOUN'), ('Grand', 'ADJ')), (('Grand', 'ADJ'), ('Jury', 'NOUN')), (('Jury', 'NOUN'), ('said', 'VERB')), (('said', 'VERB'), ('Friday', 'NOUN')), (('Friday', 'NOUN'), ('an', 'DET')), (('an', 'DET'), ('investigation', 'NOUN')), (('investigation', 'NOUN'), ('of', 'ADP')), (('of', 'ADP'), (\"Atlanta's\", 'NOUN'))]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQW7Gd1iZXFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "brown_news_tagged = nltk.corpus.brown.tagged_words(categories='news', tagset='universal')\n",
        "word_tag_pairs = nltk.bigrams(brown_news_tagged)\n",
        "noun_preceders = [a[1] for (a,b) in word_tag_pairs if b[1] =='NOUN']\n",
        "fdist = nltk.FreqDist(noun_preceders)\n",
        "print([tag for (tag,_) in fdist.most_common()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2J37I4YabhM",
        "colab_type": "text"
      },
      "source": [
        "> this will print in order what pos was most placed before one `noun` in the news article.\n",
        "output:\n",
        "```\n",
        "['NOUN', 'DET', 'ADJ', 'ADP', '.', 'VERB', 'CONJ', 'NUM', 'ADV', 'PRT', 'PRON', 'X']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvFuux6spVqu",
        "colab_type": "text"
      },
      "source": [
        "## N-Gram Tagging\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKq3VHWkpb-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Hhuxz0xPO3",
        "colab_type": "text"
      },
      "source": [
        "### # Unigram Tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8dllZGLxUe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#crete two sents to compare. one already tagged by the package the other by 'UnigramTagger'\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "brown_sents = brown.sents(categories='news')\n",
        "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents) # fitting\n",
        "print(unigram_tagger.tag(brown_sents[2007])) # predict(?)\n",
        "\n",
        "unigram_tagger.evaluate(brown_tagged_sents) # 0.9349006503968017\n",
        "\n",
        "# divide dataset to train and test set. then evaluate  POS tag of test set\n",
        "size = int(len(brown_tagged_sents)*0.9)\n",
        "size  # 4160\n",
        "\n",
        "train_sents = brown_tagged_sents[:size]\n",
        "test_sents = brown_tagged_sents[size:]\n",
        "unigram_tagger = nltk.UnigramTagger(train_sents) #fitting\n",
        "unigram_tagger.evaluate(test_sents) #  0.8121200039868434"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLm9hnbRxZKd",
        "colab_type": "text"
      },
      "source": [
        "### Bigram Tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSWo1Ko4xb-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bigram Tagger\n",
        "bigram_tagger = nltk.BigramTagger(train_sents)\n",
        "bigram_tagger.tag(brown_sents[2007])\n",
        "unseen_sent = brown_sents[4203]\n",
        "print(bigram_tagger.tag(unseen_sent))\n",
        "\n",
        "# evaluate. in case of Bigram, it is relatively law because when NNS VBG sequence does\n",
        "#not exist, P(VBG|NNS)=0, sparse proble, it will affect the whole PIE(P) calculation.\n",
        "# trade off between accuracy and coverage.\n",
        "bigram_tagger.evaluate(test_sents) # 0.10206319146815508\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vg_LbVfxe4h",
        "colab_type": "text"
      },
      "source": [
        "### Combining Tagger (Backoff Tagger)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07SXJhjaxj_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combingin Tagger\n",
        "# 1. try bagging the token with the bigram tagger\n",
        "# 2. if the bigram tagger is unable to find a tag for the token, try the unigram tagger\n",
        "# 3. if the unigram tagger is also unable to find a tag, use a default tagger.\n",
        "\n",
        "t0 = nltk.DefaultTagger('NN') # tag it NOUN\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff = t0)\n",
        "t2 = nltk.BigramTagger(train_sents, backoff = t1)\n",
        "t3 = nltk.TrigramTagger(train_sents, backoff = t2)\n",
        "t3.evaluate(test_sents) # 0.843317053722715 remarkably improved from 0.1020.....\n",
        "\n",
        "\n",
        "# HMM Tagger\n",
        "trainer = nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
        "hmm_tagger = trainer.train_supervised(train_sents)\n",
        "hmm_tagger.evaluate(test_sents) # 0.3166550383733679"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqApKtD7biHK",
        "colab_type": "text"
      },
      "source": [
        "## unknown words\n",
        "\n",
        "문제야. 알아야하는데.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YzENrQufefX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unknown word tagging.\n",
        "text = 'I go to school in the klasdfas'\n",
        "token = text.split()\n",
        "print(unigram_tagger.tag(token))\n",
        "\"\"\"\n",
        "[('I', 'PPSS'), ('go', 'VB'), ('to', 'TO'), ('school', 'NN'), ('in', 'IN'), ('the', 'AT'), ('klasdfas', None)]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbcvUk_vxAWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(bigram_tagger.tag(token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffducgZuw4Qa",
        "colab_type": "text"
      },
      "source": [
        "`[('I', 'PPSS'), ('go', 'VB'), ('to', 'TO'), ('school', None), ('in', None), ('the', None), ('klasdfas', None)]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INCp51AZw2ZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(nltk.pos_tag(token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW3S2hJkwpA3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```[('I', 'PRP'), ('go', 'VBP'), ('to', 'TO'), ('school', 'NN'), ('in', 'IN'), ('the', 'DT'), ('klasdfas', 'NN')]```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdjmn9NXfexB",
        "colab_type": "text"
      },
      "source": [
        "> Unigram Tagger only affects on unknown word\n",
        "Bigram Tagger affects on unknown word as well as adjacent(or other) words   \n",
        "\n",
        "> pos_tag() tags the unknown word as 'NN' (noun) based on default setting of context read?  \n",
        "\n",
        "> note that Unigram and Bigram is not well trained as the train data set was small\n",
        "and pos_tag is well trained with sufficient amount of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64-de9uKan7V",
        "colab_type": "text"
      },
      "source": [
        "## nltk.pos_tag(token)\n",
        "\n",
        "기존 만든 태거(pos_tag)를 대체할 필요가 있을때가 있다.  \n",
        "직접 만들기 위해 배우는 것.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jqNhD-Dfe3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}