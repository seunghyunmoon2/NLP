{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP8_7/31.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhSOjgB0CAH4YWIGGDShuD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP8_Text_Generation/DMN(QnA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnyLmBM2WNyB",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation using LSTM\n",
        "\n",
        "Here, we used LSTM because the sequence affects the learning. Also, instead of giving **one-hot encoding** to the output, **we do multinomial()** so that arguments with lesser probability can also be selected during the generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnQTnEb7V2_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM을 이용한 세익스피어 저서의 텍스트 자동 생성\n",
        "# ----------------------------------------------\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "path = './dataset/shakespeare_final.txt'\n",
        "text = open(path).read().lower()\n",
        "\n",
        "print('corpus length:', len(text))\n",
        "characters = sorted(list(set(text)))\n",
        "print('total chars:', len(characters))\n",
        "\n",
        "char2indices = dict((c, i) for i, c in enumerate(characters))\n",
        "indices2char = dict((i, c) for i, c in enumerate(characters))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "# Converting indices into vectorized format\n",
        "X = np.zeros((len(sentences), maxlen, len(characters)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(characters)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char2indices[char]] = 1\n",
        "    y[i, char2indices[next_chars[i]]] = 1\n",
        "\t\n",
        "#Model Building\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
        "print (model.summary())\n",
        "\n",
        "# Function to convert prediction into index\n",
        "def pred_indices(preds, metric=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / metric\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds/np.sum(exp_preds)\n",
        "    probs = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probs)\n",
        "\n",
        "# Train & Evaluate the Model\n",
        "for iteration in range(1, 30):\n",
        "    print('-' * 40)\n",
        "    print('Iteration', iteration)\n",
        "    model.fit(X, y,batch_size=128,epochs=1)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "\n",
        "    for diversity in [0.2, 0.7,1.2]:\n",
        "        print('\\n----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x = np.zeros((1, maxlen, len(characters)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x[0, t, char2indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x, verbose=0)[0]\n",
        "            next_index = pred_indices(preds, diversity)\n",
        "            pred_char = indices2char[next_index]\n",
        "\n",
        "            generated += pred_char\n",
        "            sentence = sentence[1:] + pred_char\n",
        "\n",
        "            sys.stdout.write(pred_char)\n",
        "            sys.stdout.flush()\n",
        "        print(\"\\nOne combination completed \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYWY_uL-XVLl",
        "colab_type": "text"
      },
      "source": [
        "output from the final iteration\n",
        "```\n",
        "----------------------------------------\n",
        "Iteration 29\n",
        "Train on 193798 samples\n",
        "193798/193798 [==============================] - 105s 544us/sample - loss: 1.2409\n",
        "\n",
        "----- diversity: 0.2\n",
        "----- Generating with seed: \"of my love,\n",
        "    and lack not to lose sti\"\n",
        "of my love,\n",
        "    and lack not to lose still the strong.\n",
        "    the particess is a soldier's faster?\n",
        "  cleopatra. who have the seem of the soldiers of him.\n",
        "    the self to a man and some strong.\n",
        "    which i will be thee a sun in thee as the strong\n",
        "    of the rememund's through the care of the fortune,\n",
        "    and some for my soldiers and the complete with partice\n",
        "    the messenger. and the particess of the complete works of ephesus. i have fortu\n",
        "One combination completed \n",
        "\n",
        "\n",
        "----- diversity: 0.7\n",
        "----- Generating with seed: \"of my love,\n",
        "    and lack not to lose sti\"\n",
        "of my love,\n",
        "    and lack not to lose still person, and i know\n",
        "  thou my fulvil'd would make the faston and bare bear and emption,\n",
        "    that can shall dear in this laid to his deed?\n",
        "    were i be mainter to you, my times some mean's sense,\n",
        "    all the name, to the world right, and take thee.\n",
        "    scanth their other.\n",
        "  cleopatra. not speak thee, amables, the better wrink even of imperier-\n",
        "    to hear my lov'd to the concitites, and this his\n",
        "One combination completed \n",
        "\n",
        "\n",
        "----- diversity: 1.2\n",
        "----- Generating with seed: \"of my love,\n",
        "    and lack not to lose sti\"\n",
        "of my love,\n",
        "    and lack not to lose stight. no, as'd death.\n",
        "    your much very your involadyes.\n",
        "  appoaarihier. the a mother gaun.\n",
        "  cleopatra. well- snoth from my feed, away!\n",
        "    ix-\n",
        "    my loves for purse of ewel;  hy', with parry, trims thos\n",
        "    and i two a shave young\n",
        " patiaus that for his worst, wenden thee.\n",
        "  goiserace.\n",
        "  cleopatra. my so?' he shall in trength.\n",
        "  caesar. i am say; if i in heares, and our dis,in as shall\n",
        "    by my\n",
        "One combination completed \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWfIyPF5gsTt",
        "colab_type": "text"
      },
      "source": [
        "# DMN (Dynamic Memory Network)\n",
        "\n",
        "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REZfEIopZUSS",
        "colab_type": "text"
      },
      "source": [
        "![nlp8-1](https://drive.google.com/uc?export=view&id=13nWfHSHeMbwna8-z5zystgz7DtI5CuiZ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g51f2dCZW3J",
        "colab_type": "text"
      },
      "source": [
        "![nlp8-2](https://drive.google.com/uc?export=view&id=11EQOdvLNK_H7mj74RRpgWEegi1xXVnZl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XulAJByYaa2",
        "colab_type": "text"
      },
      "source": [
        "## Question and Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etx1obbyYT3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dynamic Memory Network을 이용한 Q&A 데이터 학습\n",
        "# ----------------------------------------------\n",
        "import collections\n",
        "import itertools\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import LSTM, Permute\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Add, Concatenate, Dot\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 문서 내용 예시 :\n",
        "# 1 Mary moved to the bathroom.\\n\n",
        "# 2 Daniel went to the garden.\\n\n",
        "# 3 Where is Mary?\\tbathroom\\t1\n",
        "#\n",
        "# Return:\n",
        "# Stories = ['Mary moved to the bathroom.\\n', 'John went to the hallway.\\n']\n",
        "# questions = 'Where is Mary? '\n",
        "# answers = 'bathroom'\n",
        "#----------------------------------------------------------------------------\n",
        "def get_data(infile):\n",
        "    stories, questions, answers = [], [], []\n",
        "    story_text = []\n",
        "    fin = open(Train_File, \"r\") \n",
        "    for line in fin:\n",
        "        lno, text = line.split(\" \", 1)\n",
        "        if \"\\t\" in text:\n",
        "            question, answer, _ = text.split(\"\\t\")\n",
        "            stories.append(story_text)\n",
        "            questions.append(question)\n",
        "            answers.append(answer)\n",
        "            story_text = []\n",
        "        else:\n",
        "            story_text.append(text)\n",
        "    fin.close()\n",
        "    return stories, questions, answers\n",
        "\n",
        "Train_File = \"./dataset/qa1_single-supporting-fact_train.txt\"\n",
        "Test_File = \"./dataset/qa1_single-supporting-fact_test.txt\"\n",
        "\n",
        "# get the data\n",
        "data_train = get_data(Train_File)\n",
        "data_test = get_data(Test_File)\n",
        "print(\"\\n\\nTrain observations:\",len(data_train[0]),\"Test observations:\", len(data_test[0]),\"\\n\\n\")\n",
        "\n",
        "# Building Vocab dictionary from Train & Test data\n",
        "dictnry = collections.Counter()\n",
        "for stories, questions, answers in [data_train, data_test]:\n",
        "    for story in stories:\n",
        "        for sent in story:\n",
        "            for word in nltk.word_tokenize(sent):\n",
        "                dictnry[word.lower()] +=1\n",
        "    for question in questions:\n",
        "        for word in nltk.word_tokenize(question):\n",
        "            dictnry[word.lower()]+=1\n",
        "    for answer in answers:\n",
        "        for word in nltk.word_tokenize(answer):\n",
        "            dictnry[word.lower()]+=1\n",
        "\n",
        "word2indx = {w:(i+1) for i,(w,_) in enumerate(dictnry.most_common())}\n",
        "word2indx[\"PAD\"] = 0\n",
        "indx2word = {v:k for k,v in word2indx.items()}\n",
        "\n",
        "vocab_size = len(word2indx)\n",
        "print(\"vocabulary size:\",len(word2indx))\n",
        "print(word2indx)\n",
        "\n",
        "# compute max sequence length for each entity\n",
        "story_maxlen = 0\n",
        "question_maxlen = 0\n",
        "\n",
        "for stories, questions, answers in [data_train, data_test]:\n",
        "    for story in stories:\n",
        "        story_len = 0\n",
        "        for sent in story:\n",
        "            swords = nltk.word_tokenize(sent)\n",
        "            story_len += len(swords)\n",
        "        if story_len > story_maxlen:\n",
        "            story_maxlen = story_len\n",
        "            \n",
        "    for question in questions:\n",
        "        question_len = len(nltk.word_tokenize(question))\n",
        "        if question_len > question_maxlen:\n",
        "            question_maxlen = question_len\n",
        "            \n",
        "print (\"Story maximum length:\", story_maxlen, \"Question maximum length:\", question_maxlen)\n",
        "\n",
        "# Converting data into Vectorized form\n",
        "def data_vectorization(data, word2indx, story_maxlen, question_maxlen):\n",
        "    Xs, Xq, Y = [], [], []\n",
        "    stories, questions, answers = data\n",
        "    for story, question, answer in zip(stories, questions, answers):\n",
        "        xs = [[word2indx[w.lower()] for w in nltk.word_tokenize(s)] for s in story]\n",
        "        xs = list(itertools.chain.from_iterable(xs))\n",
        "        xq = [word2indx[w.lower()] for w in nltk.word_tokenize(question)]\n",
        "        Xs.append(xs)\n",
        "        Xq.append(xq)\n",
        "        Y.append(word2indx[answer.lower()])\n",
        "    return pad_sequences(Xs, maxlen=story_maxlen), pad_sequences(Xq, maxlen=question_maxlen),\\\n",
        "           to_categorical(Y, num_classes=len(word2indx))\n",
        "\n",
        "           \n",
        "Xstrain, Xqtrain, Ytrain = data_vectorization(data_train, word2indx, story_maxlen, question_maxlen)\n",
        "Xstest, Xqtest, Ytest = data_vectorization(data_test, word2indx, story_maxlen, question_maxlen)\n",
        "\n",
        "print(\"Train story\",Xstrain.shape,\"Train question\", Xqtrain.shape,\"Train answer\", Ytrain.shape)\n",
        "print( \"Test story\",Xstest.shape, \"Test question\",Xqtest.shape, \"Test answer\",Ytest.shape)\n",
        "\n",
        "# Model Parameters\n",
        "EMBEDDING_SIZE = 128\n",
        "LATENT_SIZE = 64\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 40\n",
        "\n",
        "# Inputs\n",
        "story_input = Input(shape=(story_maxlen,))\n",
        "question_input = Input(shape=(question_maxlen,))\n",
        "\n",
        "# Story encoder embedding\n",
        "story_encoder = Embedding(input_dim=vocab_size,\n",
        "                          output_dim=EMBEDDING_SIZE, \n",
        "                          input_length=story_maxlen)(story_input)\n",
        "story_encoder = Dropout(0.2)(story_encoder)\n",
        "\n",
        "# Question encoder embedding\n",
        "question_encoder = Embedding(input_dim=vocab_size,\n",
        "                             output_dim=EMBEDDING_SIZE,\n",
        "                             input_length=question_maxlen)(question_input)\n",
        "question_encoder = Dropout(0.3)(question_encoder)\n",
        "\n",
        "# Match between story and question\n",
        "# story_encoder = [None, 14, 128], question_encoder = [None, 4, 128]\n",
        "# match = [None, 14, 4]\n",
        "match = Dot(axes=[2, 2])([story_encoder, question_encoder])\n",
        "\n",
        "# Encode story into vector space of question\n",
        "story_encoder_c = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=question_maxlen,\n",
        "                            input_length=story_maxlen)(story_input)\n",
        "story_encoder_c = Dropout(0.3)(story_encoder_c)\n",
        "\n",
        "# Combine match and story vectors\n",
        "response = Add()([match, story_encoder_c])\n",
        "response = Permute((2, 1))(response)\n",
        "\n",
        "# Combine response and question vectors to answers space\n",
        "answer = Concatenate()([response, question_encoder])\n",
        "answer = LSTM(LATENT_SIZE)(answer)\n",
        "answer = Dropout(0.2)(answer)\n",
        "answer = Dense(vocab_size)(answer)\n",
        "output = Activation(\"softmax\")(answer)\n",
        "\n",
        "model = Model(inputs=[story_input, question_input], outputs=output)\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "print (model.summary())\n",
        "\n",
        "# Model Training\n",
        "history = model.fit([Xstrain, Xqtrain], [Ytrain],\n",
        "                    batch_size = BATCH_SIZE, \n",
        "                    epochs = NUM_EPOCHS,\n",
        "                    validation_data=([Xstest, Xqtest], [Ytest]))\n",
        "\t\t\t\t\t\n",
        "# loss plot\n",
        "plt.title(\"Episodic Memory Q & A Loss\")\n",
        "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], color=\"r\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n",
        "# get predictions of labels\n",
        "ytest = np.argmax(Ytest, axis=1)\n",
        "Ytest_ = model.predict([Xstest, Xqtest])\n",
        "ytest_ = np.argmax(Ytest_, axis=1)\n",
        "\n",
        "# Select Random questions and predict answers\n",
        "NUM_DISPLAY = 10\n",
        "   \n",
        "for i in random.sample(range(Xstest.shape[0]),NUM_DISPLAY):\n",
        "    story = \" \".join([indx2word[x] for x in Xstest[i].tolist() if x != 0])\n",
        "    question = \" \".join([indx2word[x] for x in Xqtest[i].tolist()])\n",
        "    label = indx2word[ytest[i]]\n",
        "    prediction = indx2word[ytest_[i]]\n",
        "    print(story, question, label, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abr2SqmLaZnS",
        "colab_type": "text"
      },
      "source": [
        "output\n",
        "```\n",
        "Epoch 40/40\n",
        "10000/10000 [==============================] - 1s 148us/sample - loss: 0.4214 - val_loss: 0.3947\n",
        "mary moved to the bedroom . sandra went back to the garden . where is sandra ? garden garden\n",
        "mary went to the office . sandra journeyed to the hallway . where is john ? garden office\n",
        "mary went back to the bathroom . mary went to the office . where is daniel ? kitchen garden\n",
        "sandra journeyed to the kitchen . mary journeyed to the bathroom . where is john ? hallway office\n",
        "mary travelled to the kitchen . john journeyed to the bathroom . where is john ? bathroom bathroom\n",
        "john went back to the bathroom . mary went back to the bedroom . where is sandra ? bedroom bedroom\n",
        "daniel went to the bathroom . mary journeyed to the bathroom . where is john ? office office\n",
        "mary went to the garden . daniel went to the garden . where is mary ? garden garden\n",
        "daniel travelled to the hallway . daniel journeyed to the office . where is sandra ? office office\n",
        "john went to the bedroom . daniel went back to the bedroom . where is john ? bedroom bedroom\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AnZwlOAagie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}