{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP11_8_4_Kaggle_BOWmeetsBagsOfPopcorns.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5MF5f7nFY7i8AzE6X3cJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP11_Kaggle_BOWmeetsBagsOfPopcorns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTOEx17FAgGm",
        "colab_type": "text"
      },
      "source": [
        "# Kaggle's Bags of Words Meets Bags of Popcorns\n",
        "\n",
        "영화리뷰 데이터 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB4y4gHkAmcN",
        "colab_type": "text"
      },
      "source": [
        "## 영화리뷰 데이터 전처리 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2igWFhpv1A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 영화리뷰 데이터 전처리\n",
        "# 데이터 : Kaggle의 Bags of Words Meets Bags of Popcorns\n",
        "# ------------------------------------------------------\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# 4.1 장에서 사용할 데이터인 영화 리뷰 데이터를 불러온다\n",
        "train_data = pd.read_csv('dataset/4-1.labeledTrainData.tsv', \n",
        "                         header = 0, delimiter = '\\t', quoting = 3)\n",
        "train_data.head()\n",
        "\n",
        "# 전처리 작업\n",
        "def preprocessing(review, stops, remove_stopwords = False): \n",
        "    # 1. HTML 태그 제거\n",
        "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
        "\n",
        "    # 2. 영어가 아닌 특수문자들을 공백(\" \")으로 바꾸기\n",
        "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
        "\n",
        "    # 3. 대문자들을 소문자로 바꾸고 공백단위로 텍스트들 나눠서 리스트로 만든다.\n",
        "    words = review_text.lower().split()\n",
        "\n",
        "    if remove_stopwords: \n",
        "        # 4. 불용어 제거       \n",
        "        # 불용어가 아닌 단어들로 이루어진 새로운 리스트 생성\n",
        "        words = [w for w in words if not w in stops]\n",
        "        \n",
        "        # 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다.\n",
        "        clean_review = ' '.join(words)\n",
        "\n",
        "    else: # 불용어를 제거하지 않을 때\n",
        "        clean_review = ' '.join(words)\n",
        "\n",
        "    return clean_review\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "clean_train_reviews = []\n",
        "for review in train_data['review']:\n",
        "    r = preprocessing(review, stops, remove_stopwords = True)\n",
        "    clean_train_reviews.append(r)\n",
        "\n",
        "# 전처리한 데이터 확인\n",
        "clean_train_reviews[0]\n",
        "\n",
        "# 전처리가 완료된 리뷰 데이터를 데이터프레임으로 구성한다. (학습할 데이터)\n",
        "clean_train_df = pd.DataFrame({'review': clean_train_reviews, \n",
        "                               'sentiment': train_data['sentiment']})\n",
        "\n",
        "# 리뷰 데이터를 워드 인덱스로 표현한다.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_reviews)\n",
        "text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
        "print(text_sequences[0])\n",
        "len(text_sequences[0])\n",
        "\n",
        "# 리뷰 데이터의 길이를 통일시킨다.\n",
        "# 길이는 중간값인 174로 하고, 리뷰 데이터의 워드 인덱스 길이가 174보다 작으면 \n",
        "# 뒷 부분을 0으로 패딩하고, 174보다 크면 뒷 부분을 버린다.\n",
        "MAX_SEQUENCE_LENGTH = 174 \n",
        "\n",
        "train_inputs = pad_sequences(text_sequences, \n",
        "                             maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                             padding='post',\n",
        "                             truncating='post')\n",
        "print('Shape of train data: ', train_inputs.shape)\n",
        "\n",
        "print(train_inputs[0])\n",
        "print(\"길이 = \", len(train_inputs[0]))\n",
        "\n",
        "# 리뷰 문서의 라벨 (1 or 0)을 가져온다\n",
        "train_labels = np.array(train_data['sentiment'])\n",
        "print('Shape of label tensor:', train_labels.shape)\n",
        "train_labels[0]\n",
        "\n",
        "TRAIN_INPUT_DATA = 'train_input.npy'\n",
        "TRAIN_LABEL_DATA = 'train_label.npy'\n",
        "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
        "\n",
        "# 전처리가 완료된 학습 데이터를 파일에 저장해 둔다\n",
        "TRAIN_INPUT_DATA = '4-1.train_input.npy'\n",
        "TRAIN_LABEL_DATA = '4-1.train_label.npy'\n",
        "TRAIN_CLEAN_DATA = '4-1.train_clean.csv'\n",
        "DATA_IN_PATH = './dataset/'\n",
        "\n",
        "# 전처리 된 데이터를 넘파이 형태로 저장\n",
        "np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n",
        "\n",
        "# 정제된 텍스트를 csv 형태로 저장\n",
        "clean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX5D9m56wJXL",
        "colab_type": "text"
      },
      "source": [
        "- output\n",
        "```\n",
        "[404, 70, 419, 8815, 506, 2456, 115, 54, 873, 516, 178, 18686, 178, 11242, 165, 78, 14, 662, 2457, 117, 92, 10, 499, 4074, 165, 22, 210, 581, 2333, 1194, 11242, 71, 4826, 71, 635, 2, 253, 70, 11, 302, 1663, 486, 1144, 3265, 8815, 411, 793, 3342, 17, 441, 600, 1500, 15, 4424, 1851, 998, 146, 342, 1442, 743, 2424, 4, 8815, 418, 70, 637, 69, 237, 94, 541, 8815, 26055, 26056, 120, 1, 8815, 323, 8, 47, 20, 323, 167, 10, 207, 633, 635, 2, 116, 291, 382, 121, 15535, 3315, 1501, 574, 734, 10013, 923, 11578, 822, 1239, 1408, 360, 8815, 221, 15, 576, 8815, 22224, 2274, 13426, 734, 10013, 27, 28606, 340, 16, 41, 18687, 1500, 388, 11243, 165, 3962, 8815, 115, 627, 499, 79, 4, 8815, 1430, 380, 2163, 114, 1919, 2503, 574, 17, 60, 100, 4875, 5100, 260, 1268, 26057, 15, 574, 493, 744, 637, 631, 3, 394, 164, 446, 114, 615, 3266, 1160, 684, 48, 1175, 224, 1, 16, 4, 8815, 3, 507, 62, 25, 16, 640, 133, 231, 95, 7426, 600, 3439, 8815, 37248, 1864, 1, 128, 342, 1442, 247, 3, 865, 16, 42, 1487, 997, 2333, 12, 549, 386, 717, 6920, 12, 41, 16, 158, 362, 4392, 3388, 41, 87, 225, 438, 207, 254, 117, 3, 18688, 18689, 316, 1356]\n",
        "Shape of train data:  (25000, 174)\n",
        "[  404    70   419  8815   506  2456   115    54   873   516   178 18686\n",
        "   178 11242   165    78    14   662  2457   117    92    10   499  4074\n",
        "   165    22   210   581  2333  1194 11242    71  4826    71   635     2\n",
        "   253    70    11   302  1663   486  1144  3265  8815   411   793  3342\n",
        "    17   441   600  1500    15  4424  1851   998   146   342  1442   743\n",
        "  2424     4  8815   418    70   637    69   237    94   541  8815 26055\n",
        " 26056   120     1  8815   323     8    47    20   323   167    10   207\n",
        "   633   635     2   116   291   382   121 15535  3315  1501   574   734\n",
        " 10013   923 11578   822  1239  1408   360  8815   221    15   576  8815\n",
        " 22224  2274 13426   734 10013    27 28606   340    16    41 18687  1500\n",
        "   388 11243   165  3962  8815   115   627   499    79     4  8815  1430\n",
        "   380  2163   114  1919  2503   574    17    60   100  4875  5100   260\n",
        "  1268 26057    15   574   493   744   637   631     3   394   164   446\n",
        "   114   615  3266  1160   684    48  1175   224     1    16     4  8815\n",
        "     3   507    62    25    16   640]\n",
        "길이 =  174\n",
        "Shape of label tensor: (25000,)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPfmrJGWAdiK",
        "colab_type": "text"
      },
      "source": [
        "## Tfidf와 Logistic Regression을 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tWfO4ckwJje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tfidf와 Logistic Regression을 이용한 영화리뷰 데이터 분류\n",
        "# 데이터 : Kaggle의 Bags of Words Meets Bags of Popcorns\n",
        "# --------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "TRAIN_CLEAN_DATA = '4-1.train_clean.csv'\n",
        "DATA_IN_PATH = './dataset/'\n",
        "\n",
        "# 전처리가 완료된 학습 데이터를 읽어온다.\n",
        "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
        "reviews = list(train_data['review'])\n",
        "sentiments = list(train_data['sentiment'])\n",
        "\n",
        "# 리뷰 문서를 tfidf로 변환한다.\n",
        "# min_df : float in range [0.0, 1.0] or int (default=1)\n",
        "# When building the vocabulary ignore terms that have a document frequency \n",
        "# strictly lower than the given threshold. This value is also called cut-off \n",
        "# in the literature. If float, the parameter represents a proportion of \n",
        "# documents, integer absolute counts. This parameter is ignored if vocabulary\n",
        "# is not None.\n",
        "#\n",
        "# analyzer : string, {‘word’, ‘char’, ‘char_wb’} or callable\n",
        "# Whether the feature should be made of word or character n-grams. \n",
        "# Option ‘char_wb’ creates character n-grams only from text inside word \n",
        "# boundaries; n-grams at the edges of words are padded with space.\n",
        "#\n",
        "# ngram_range : tuple (min_n, max_n) (default=(1, 1))\n",
        "# The lower and upper boundary of the range of n-values for different n-grams\n",
        "# to be extracted.\n",
        "# All values of n such that min_n <= n <= max_n will be used.\n",
        "# ngram_range(1, 2) means unigrams and bigrams, (2, 2) means only bigrams.\n",
        "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"word\", sublinear_tf=True, \n",
        "                             ngram_range=(1,2), max_features=1000) \n",
        "\n",
        "X = vectorizer.fit_transform(reviews)\n",
        "y = np.array(sentiments)\n",
        "\n",
        "# 학습 데이터와 시험 데이터로 분리한다.\n",
        "RANDOM_SEED = 42\n",
        "TEST_SPLIT = 0.2\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, \n",
        "                                                    random_state=RANDOM_SEED)\n",
        "\n",
        "# Logistic Regression으로 학습 데이터를 학습한다\n",
        "lgs = LogisticRegression(class_weight='balanced', solver='lbfgs') \n",
        "lgs.fit(X_train, y_train)\n",
        "\n",
        "# 시험 데이터로 학습 성능을 평가한다\n",
        "predicted = lgs.predict(X_eval)\n",
        "print(predicted[:20])\n",
        "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcvlvJaswOJu",
        "colab_type": "text"
      },
      "source": [
        "- output\n",
        "```\n",
        "[0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1]\n",
        "Accuracy: 0.862800\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFvC2Wz4Abwp",
        "colab_type": "text"
      },
      "source": [
        "## Word2Vec과 Logistic Regression을 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqDH3XVYwSu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word2Vec과 Logistic Regression을 이용한 영화리뷰 데이터 분류\n",
        "# 데이터 : Kaggle의 Bags of Words Meets Bags of Popcorns\n",
        "# pip install gensim\n",
        "# ----------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import word2vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "TRAIN_CLEAN_DATA = '4-1.train_clean.csv'\n",
        "DATA_IN_PATH = './dataset/'\n",
        "\n",
        "# 전처리가 완료된 학습 데이터를 읽어온다.\n",
        "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
        "reviews = list(train_data['review'])\n",
        "sentiments = list(train_data['sentiment'])\n",
        "\n",
        "sentences = []\n",
        "for review in reviews:\n",
        "    sentences.append(review.split())\n",
        "\n",
        "model_name = '4-1.300features.word2vec'\n",
        "num_features = 300      # 워드 벡터 특징값 수\n",
        "min_word_count = 40     # 단어에 대한 최소 빈도수\n",
        "num_workers = 4         # 프로세스 개수\n",
        "context = 10            # 컨텍스트 윈도우 크기\n",
        "downsampling = 1e-3     # 다운 샘플링 비율\n",
        "model_saved = True\n",
        "\n",
        "if model_saved:\n",
        "    model = word2vec.Word2Vec.load(DATA_IN_PATH + model_name)\n",
        "else:\n",
        "    # gensim 패키지를 이용하여 단어를 vector화 한다 (Word2Vec)\n",
        "    model = word2vec.Word2Vec(sentences,\n",
        "                              workers = num_workers,\n",
        "                              size = num_features,\n",
        "                              min_count = min_word_count,\n",
        "                              window = context,\n",
        "                              sample = downsampling)\n",
        "    model.save(DATA_IN_PATH + model_name)\n",
        "\n",
        "# model을 확인해 본다.\n",
        "keys = list(model.wv.vocab.keys())[:20]\n",
        "print(keys)\n",
        "\n",
        "# 단어 'stuff'의 vector를 확인한다. 길이 = 300개\n",
        "model.wv['stuff']\n",
        "\n",
        "# 단어 유사도를 측정해 본다.\n",
        "model.wv.similarity(\"dog\", \"cat\")\n",
        "model.wv.similarity(\"dog\", \"cake\")\n",
        "\n",
        "np.dot(model.wv['dog'], model.wv['cat'])\n",
        "np.dot(model.wv['dog'], model.wv['cake'])\n",
        "\n",
        "model.wv.most_similar(\"dog\")\n",
        "\n",
        "# 1개 문장을 300개 feature로 vector화 한다.\n",
        "# 문장 = ['dog', 'eat']라면 word.wv['dog']의 300개 vector와 word.wv['eag']의\n",
        "# 300개 vector의 평균을 계산한다.\n",
        "def get_features(words, model, num_features):\n",
        "    feature_vector = np.zeros((num_features),dtype=np.float32)\n",
        "\n",
        "    num_words = 0\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "\n",
        "    for w in words:\n",
        "        if w in index2word_set:\n",
        "            num_words += 1\n",
        "            feature_vector = np.add(feature_vector, model.wv[w])\n",
        "\n",
        "    feature_vector = np.divide(feature_vector, num_words)\n",
        "    return feature_vector\n",
        "\n",
        "# reviews 문장들을 각각 300개 feature로 vector화 한다.\n",
        "def get_dataset(reviews, model, num_features):\n",
        "    dataset = list()\n",
        "\n",
        "    for s in reviews:\n",
        "        dataset.append(get_features(s, model, num_features))\n",
        "\n",
        "    reviewFeatureVecs = np.stack(dataset)\n",
        "    \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "test_data_vecs = get_dataset(sentences, model, num_features)\n",
        "\n",
        "# 학습 데이터와 시험 데이터로 분리한다.\n",
        "RANDOM_SEED = 42\n",
        "TEST_SPLIT = 0.2\n",
        "X = test_data_vecs\n",
        "y = np.array(sentiments)\n",
        "\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, \n",
        "                                                    random_state=RANDOM_SEED)\n",
        "\n",
        "# Logistic Regression으로 학습 데이터를 학습한다\n",
        "lgs = LogisticRegression(class_weight='balanced', solver='newton-cg') \n",
        "lgs.fit(X_train, y_train)\n",
        "\n",
        "# 시험 데이터로 학습 성능을 평가한다\n",
        "predicted = lgs.predict(X_eval)\n",
        "print(predicted[:20])\n",
        "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRzw8kWgwnO9",
        "colab_type": "text"
      },
      "source": [
        "- output\n",
        "\n",
        "```\n",
        "['stuff', 'going', 'moment', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties']\n",
        "\n",
        "C:\\Users\\student\\.conda\\envs\\Python_practice_M\\lib\\site-packages\\scipy\\optimize\\linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
        "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
        "\n",
        "C:\\Users\\student\\.conda\\envs\\Python_practice_M\\lib\\site-packages\\sklearn\\utils\\optimize.py:204: UserWarning: Line Search failed\n",
        "  warnings.warn('Line Search failed')\n",
        "[0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0]\n",
        "Accuracy: 0.864000\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoMiBu77AXaI",
        "colab_type": "text"
      },
      "source": [
        "## Doc2Vec과 Logistic Regression을 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fzETxhcAu2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Doc2Vec과 Logistic Regression을 이용한 영화리뷰 데이터 분류\n",
        "# 데이터 : Kaggle의 Bags of Words Meets Bags of Popcorns\n",
        "# pip install gensim\n",
        "# ----------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "TRAIN_CLEAN_DATA = '4-1.train_clean.csv'\n",
        "DATA_IN_PATH = './dataset/'\n",
        "\n",
        "# 전처리가 완료된 학습 데이터를 읽어온다.\n",
        "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
        "reviews = list(train_data['review'])\n",
        "sentiments = list(train_data['sentiment'])\n",
        "\n",
        "sentences = []\n",
        "for review in reviews:\n",
        "    sentences.append(review.split())\n",
        "\n",
        "model_name = '4-1.300features.doc2vec'\n",
        "model_saved = True\n",
        "\n",
        "if model_saved:\n",
        "    model = Doc2Vec.load(DATA_IN_PATH + model_name)\n",
        "else:\n",
        "    # gensim 패키지를 이용하여 문장을 vector화 한다 (Doc2Vec)\n",
        "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]\n",
        "    model = Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.00025, \n",
        "                    min_count=10, workers=4, dm =1)\n",
        "    model.build_vocab(documents)\n",
        "    model.train(documents, total_examples=model.corpus_count, epochs=10)\n",
        "    model.save(DATA_IN_PATH + model_name)\n",
        "\n",
        "# model을 확인해 본다.\n",
        "keys = list(model.wv.vocab.keys())[:20]\n",
        "print(keys)\n",
        "\n",
        "# 단어 'stuff'의 vector를 확인한다. 길이 = 300개\n",
        "model.wv['stuff']\n",
        "\n",
        "# 단어 유사도를 측정해 본다.\n",
        "model.wv.similarity(\"dog\", \"cat\")\n",
        "model.wv.similarity(\"dog\", \"cake\")\n",
        "\n",
        "np.dot(model.wv['dog'], model.wv['cat'])\n",
        "np.dot(model.wv['dog'], model.wv['cake'])\n",
        "\n",
        "model.wv.most_similar(\"dog\")\n",
        "\n",
        "# 첫 번째 문장의 vector (300 개)\n",
        "model.docvecs[0]\n",
        "\n",
        "# 새로운 문장의 vector를 추정한다.\n",
        "new_sentence = model.infer_vector([\"system\", \"response\", \"cpu\", \"compute\"])\n",
        "\n",
        "# 학습 데이터와 시험 데이터로 분리한다.\n",
        "RANDOM_SEED = 42\n",
        "TEST_SPLIT = 0.2\n",
        "X = [model.docvecs[i] for i in range(len(sentences))]\n",
        "y = np.array(sentiments)\n",
        "\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, \n",
        "                                                    random_state=RANDOM_SEED)\n",
        "\n",
        "# Logistic Regression으로 학습 데이터를 학습한다\n",
        "lgs = LogisticRegression(class_weight='balanced', solver='newton-cg') \n",
        "lgs.fit(X_train, y_train)\n",
        "\n",
        "# 시험 데이터로 학습 성능을 평가한다\n",
        "predicted = lgs.predict(X_eval)\n",
        "print(predicted[:20])\n",
        "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJUZTZb8U9MK",
        "colab_type": "text"
      },
      "source": [
        "- output\n",
        "\n",
        "```\n",
        "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought']\n",
        "[0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1]\n",
        "Accuracy: 0.845200\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmgyN50dThrA",
        "colab_type": "text"
      },
      "source": [
        "## Word2Vec - Doc2Vec 의 결과 비교\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OFCQZz_Trq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 단어 'stuff'의 vector를 확인한다. 길이 = 300개\n",
        "model.wv['stuff']\n",
        "\n",
        "# 단어 유사도를 측정해 본다.\n",
        "model.wv.similarity(\"dog\", \"cat\")\n",
        "model.wv.similarity(\"dog\", \"cake\")\n",
        "\n",
        "#np.dot(model.wv['dog'], model.wv['cat'])\n",
        "#np.dot(model.wv['dog'], model.wv['cake'])\n",
        "\n",
        "model.wv.most_similar(\"dog\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC9wrJtiT93T",
        "colab_type": "text"
      },
      "source": [
        "- Word2vec\n",
        "\n",
        "```\n",
        "0.7890252\n",
        "\n",
        "0.29079747\n",
        "\n",
        "[('rat', 0.8182035684585571),\n",
        " ('chicken', 0.8072102665901184),\n",
        " ('dude', 0.7966217994689941),\n",
        " ('eat', 0.7944402694702148),\n",
        " ('eats', 0.7918289303779602),\n",
        " ('cat', 0.7890251874923706),\n",
        " ('underwear', 0.7868368625640869),\n",
        " ('drink', 0.7856504917144775),\n",
        " ('butt', 0.7746052742004395),\n",
        " ('drinking', 0.7740159034729004)]\n",
        "```\n",
        "---\n",
        "- Doc2Vec\n",
        "\n",
        "```\n",
        "0.5822584\n",
        "\n",
        "0.05612491\n",
        "\n",
        "[('chicken', 0.592168390750885),\n",
        " ('cat', 0.5822584629058838),\n",
        " ('puppy', 0.5540207624435425),\n",
        " ('cats', 0.5450679063796997),\n",
        " ('eat', 0.5350972414016724),\n",
        " ('dogs', 0.5316440463066101),\n",
        " ('bunny', 0.5241804122924805),\n",
        " ('worm', 0.5236088633537292),\n",
        " ('bite', 0.5121653079986572),\n",
        " ('bike', 0.5107408761978149)]\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCiJYNEsph0X",
        "colab_type": "text"
      },
      "source": [
        "# gensim.models.doc2vec 들여다보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzt_Kn8qpl2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "samples = ['너 오늘 이뻐 보인다.',\n",
        "          '나는 오늘 기분이 더러워',\n",
        "          '끝내주는데, 좋은 일이 있나봐',\n",
        "          '나 좋은 일이 생겼어',\n",
        "          '아 오늘 진짜 짜증나',\n",
        "          '환상적인데, 정말 좋은 것 같아']\n",
        "\n",
        "sentences = [s.split() for s in samples]\n",
        "\n",
        "# 문장마다 Paragraph ID assign\n",
        "documents = [TaggedDocument(doc, [f'd{i}']) \\\n",
        "             for i, doc in enumerate(sentences)]\n",
        "    \n",
        "# [f'doc-{i}' for i in range(10)]\n",
        "\n",
        "\n",
        "# PV- DM 모델을 생성한다.\n",
        "model = Doc2Vec(vector_size=5, alpha=0.025, min_alpha=0.0025, min_count=1, dm =1)#if dm =0: PV-DBOW로 학습하다.\n",
        " \n",
        "\n",
        "# train model (PV-DM 모델을 학습한다.)\n",
        "model.build_vocab(documents)\n",
        "model.train(documents, total_examples=len(samples),epochs=100)\n",
        "\n",
        "\n",
        "# word vector 확인해본다.\n",
        "model.wv['오늘']\n",
        "#array([-0.03693939, -0.04885605, -0.05943887,  0.05963083,  0.0555545 ], dtype=float32)\n",
        "\n",
        "\n",
        "# Paragraph vector 확인해본다.\n",
        "# 너 오늘 이뻐 보인다.\n",
        "model.docvecs[0]\n",
        "#array([-0.05443092, -0.04604649, -0.05194417, -0.05120996,  0.02557259], dtype=float32)\n",
        "\n",
        "\n",
        "# sentences에 대한 각각의 paragraph vector를 확인해본다.\n",
        "# word2vec 에서는 문장이 2차원 구조의 매트릭스. 문장을 얻기위해 평균을 내더라.\n",
        "# doc2vec 문장이 벡터로 나와버림.\n",
        "model.docvecs.vectors_docs\n",
        "#array([[-0.05443092, -0.04604649, -0.05194417, -0.05120996,  0.02557259],\n",
        "#       [ 0.08317213,  0.0322421 , -0.08741622,  0.08744427,  0.0764193 ],\n",
        "#       [ 0.02685709, -0.03138647,  0.00841046, -0.07948915, -0.07992024],\n",
        "#       [ 0.02179648,  0.03237828, -0.07769433, -0.04575829, -0.05825642],\n",
        "#       [ 0.05020343, -0.07546338, -0.10376928, -0.01349617,  0.00015175],\n",
        "#       [-0.04932483, -0.08273362,  0.00135178,  0.0470594 ,  0.00538549]],\n",
        "#      dtype=float32)\n",
        "\n",
        "\n",
        "# 새로운 문장에 대한 벡터 추론\n",
        "model.infer_vector('오늘 좋은 일 있을 것 같아'.split())\n",
        "#array([ 0.09398302,  0.06397881, -0.08147865, -0.0935128 , -0.03737117], dtype=float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEKTscXKKW3I",
        "colab_type": "text"
      },
      "source": [
        "### Doc2Vec과 DL을 이용-2 \n",
        "\n",
        "LR대신 Deeplearning을 이용한다. - FFN을 쓴다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5Z18DKtjCC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "TRAIN_CLEAN_DATA = '4-1.train_clean.csv'\n",
        "DATA_IN_PATH = './dataset/'\n",
        "\n",
        "# 전처리가 완료된 학습 데이터를 읽어온다.\n",
        "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
        "reviews = list(train_data['review'])\n",
        "sentiments = list(train_data['sentiment'])\n",
        "\n",
        "sentences = []\n",
        "for review in reviews:\n",
        "    sentences.append(review.split())\n",
        "\n",
        "model_name = '4-1.300features.doc2vec'\n",
        "model_saved = True\n",
        "\n",
        "if model_saved:\n",
        "    model = Doc2Vec.load(DATA_IN_PATH + model_name)\n",
        "else:\n",
        "    # gensim 패키지를 이용하여 문장을 vector화 한다 (Doc2Vec)\n",
        "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]\n",
        "    model = Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.00025, \n",
        "                    min_count=10, workers=4, dm =1)\n",
        "    model.build_vocab(documents)\n",
        "    model.train(documents, total_examples=model.corpus_count, epochs=10)\n",
        "    model.save(DATA_IN_PATH + model_name)\n",
        "\n",
        "# model을 확인해 본다.\n",
        "keys = list(model.wv.vocab.keys())[:20]\n",
        "print(keys)\n",
        "\n",
        "# 단어 'stuff'의 vector를 확인한다. 길이 = 300개\n",
        "model.wv['stuff']\n",
        "\n",
        "# 단어 유사도를 측정해 본다.\n",
        "model.wv.similarity(\"dog\", \"cat\")\n",
        "model.wv.similarity(\"dog\", \"cake\")\n",
        "\n",
        "np.dot(model.wv['dog'], model.wv['cat'])\n",
        "np.dot(model.wv['dog'], model.wv['cake'])\n",
        "\n",
        "model.wv.most_similar(\"dog\")\n",
        "\n",
        "# 첫 번째 문장의 vector (300 개)\n",
        "model.docvecs[0]\n",
        "\n",
        "# 새로운 문장의 vector를 추정한다.\n",
        "new_sentence = model.infer_vector([\"system\", \"response\", \"cpu\", \"compute\"])\n",
        "\n",
        "# 학습 데이터와 시험 데이터로 분리한다.\n",
        "RANDOM_SEED = 42\n",
        "TEST_SPLIT = 0.2\n",
        "X = np.array([model.docvecs[i] for i in range(len(sentences))])\n",
        "y = np.array(sentiments)\n",
        "\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, \n",
        "                                                    random_state=RANDOM_SEED)\n",
        "\n",
        "\n",
        "Xcopy = np.array(X.copy())\n",
        "input_layer = Input(batch_shape=(None, Xcopy.shape[1])) # 300\n",
        "#rnn_layers = LSTM(50)(input_layer)\n",
        "hidden_layer = Dense(400, activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_layer)\n",
        "hidden_layer2 = Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.05))(hidden_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)\n",
        "\n",
        "model = Model(input_layer, output_layer)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0005))\n",
        "model.summary()\n",
        "\n",
        "#model.fit(np.array(X_train).reshape(-1, 1, 300), np.array(y_train).reshape(-1, 1, 300), \\\n",
        "#          batch_size = 500, epochs = 30)\n",
        "    \n",
        "#xInput = Input(batch_shape=(None, trainX.shape[1]))\n",
        "#xHidden = Dense(8, kernel_regularizer=regularizers.l2(0.05), activation='relu')(xInput)\n",
        "#yOutput = Dense(trainY.shape[1], kernel_regularizer=regularizers.l2(0.05), activation='sigmoid')(xHidden)\n",
        "#model = Model(xInput, yOutput)\n",
        "#model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.01))\n",
        "\n",
        "# 학습한다\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=50)# validation_data = (testX, testY), \n",
        "\n",
        "# 학습이 완료되면 testX를 넣어서 출력값을 확인한다.\n",
        "# textX의 출력값 (추정값)과 testY (실제값)를 이용하여 정확도를 측정한다.\n",
        "yHat = model.predict(X_eval)\n",
        "testYhat = np.where(yHat > 0.5, 1, 0)\n",
        "accuracy = 100 * (y_eval == testYhat).mean()\n",
        "print(accuracy)\n",
        "# 50.008056 \n",
        "# 확연히 낮아졌다. vs 0.845200\n",
        "# 더 높게 나오기도 한다더라..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}