{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNMYZs6hS83Y4sYzWzYe9Mn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl_Nu_8eqWUb",
        "colab_type": "text"
      },
      "source": [
        "# NLTKpackage\n",
        "corpus. introduction dataset(as I call it) to start learning NLP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sj_e0Klq29P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import webtext\n",
        "from urllib import request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuWgCPomq6v-",
        "colab_type": "text"
      },
      "source": [
        "## gutengerg-books의 emma가져오고 놀아보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1nHZB8brS2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textId = nltk.corpus.gutenberg.fileids()\n",
        "\n",
        "text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "text[:100]\n",
        "\n",
        "print(text[:200])\n",
        "\n",
        "print('문자개수 = ',len(text))\n",
        "\n",
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "len(emma)\n",
        "\n",
        "\n",
        "#gutenberg.org 에서   nltk.download() - book 에 없는 파일을 읽어올 때. 그냥 url로 접근한다.\n",
        "url = \"http://www.gutenberg.org/files/12345/12345.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf8')\n",
        "print(text[:100])\n",
        "\n",
        "#text is a raw document. i.e. text[0] = 'E'  -> returns a list(?) of alphabets.\n",
        "#word_tokenize는 list로 returns.\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"토큰타입 = \",type(tokens),' lenth = ',len(tokens))\n",
        "print(tokens[:20])\n",
        "\n",
        "#nltk.Text returns type of class'nltk.text.Text'\n",
        "nltkText = nltk.Text(tokens)\n",
        "print('type of document = ', type(nltkText),' lenth = ',len(nltkText))\n",
        "print(nltkText[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr3d918SrtRJ",
        "colab_type": "text"
      },
      "source": [
        "## Webtext\n",
        "for more coloquial\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5eS87larxd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import webtext\n",
        "\n",
        "# lookup file ID or Webtext corpus\n",
        "textId = webtext.fileids()\n",
        "\n",
        "# will play with the script of 'Pirates of Carribean'\n",
        "# read txt file by alphabet\n",
        "text = webtext.raw('pirates.txt')\n",
        "print(text[0])\n",
        "print(text[:1000])\n",
        "print(len(text))\n",
        "print(text[1000:])\n",
        "\n",
        "\n",
        "#read txt file by word\n",
        "word = webtext.words('pirates.txt')\n",
        "print(word)\n",
        "print('# of words = ',len(word))\n",
        "\n",
        "#read by sentence\n",
        "sentence = webtext.sents('pirates.txt')\n",
        "print(sentence[0])\n",
        "print(sentence[1])\n",
        "print(sentence[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POsjzKUgr1YW",
        "colab_type": "text"
      },
      "source": [
        "## general data from internet\n",
        " called 'Chat data' (idontknowwhy)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSG8yizjr_Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import nps_chat\n",
        "\n",
        "# lookup file ids\n",
        "textId = nps_chat.fileids()\n",
        "print(textId)\n",
        "\n",
        "#read a chat session xml file\n",
        "text = nps_chat.raw('10-19-20s_706posts.xml')\n",
        "print(text[:1000])\n",
        "#got curious about what the other will say\n",
        "print(text[:2000])\n",
        "\n",
        "#get data from post tag.\n",
        "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
        "chatroom[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL6M1OAKr7I3",
        "colab_type": "text"
      },
      "source": [
        "## Brown Corpus\n",
        "first e-documents categorized well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab-qK3WSsCtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read Brown Corpus data\n",
        "from nltk.corpus import brown\n",
        "\n",
        "#file ids - theres alot\n",
        "print(brown.fileids())\n",
        "\n",
        "\n",
        "# by category\n",
        "print(brown.categories())\n",
        "\n",
        "# lookup news txt. this includes 품사(wordclass?)\n",
        "# after each word represents its wordclass?\n",
        "news = brown.raw(categories='news')\n",
        "print(news[:1000])\n",
        "\n",
        "#by word\n",
        "news = brown.words(categories='news')\n",
        "print(news)\n",
        "\n",
        "# by word by fileid\n",
        "cg22 = brown.words(fileids=['cg22'])\n",
        "print(cg22)\n",
        "\n",
        "\n",
        "# word distribution by genre\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (genre,word)\n",
        "    for genre in brown.categories()\n",
        "    for word in brown.words(categories=genre))\n",
        "\n",
        "cfd  # output:   <ConditionalFreqDist with 15 conditions>\n",
        "print(cfd.conditions())\n",
        "\"\"\"\n",
        "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
        "\"\"\"\n",
        "\n",
        "# can retrieve frequencydistribution like DF works\n",
        "cfd['science_fiction']\n",
        "# output: FreqDist({',': 791, '.': 786, 'the': 652, 'of': 321, ...})\n",
        "\n",
        "genres=['news','science_fiction','romance']\n",
        "modals=['can','could','may','might','must','will']\n",
        "cfd.tabulate(conditions=genres, samples=modals)\n",
        "#COULD this MEAN somthing?\n",
        "\"\"\"               can could   may might  must  will \n",
        "           news    93    86    66    38    50   389 \n",
        "science_fiction    16    49     4    12     8    16 \n",
        "        romance    74   193    11    51    45    43 \n",
        "\"\"\" \n",
        "# further, will learn LDA(topic model) which guesses the topic by the dist of words."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5YthTTfsI8h",
        "colab_type": "text"
      },
      "source": [
        "## Reuter corpus\n",
        "categorized news documents. two data sets(?)for training, for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKG9YnZ9sLSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read \n",
        "from nltk.corpus import reuters\n",
        "\n",
        "#fileid\n",
        "print(reuters.fileids()[:10])\n",
        "print(len(reuters.fileids()))\n",
        "\n",
        "# category list\n",
        "print(reuters.categories())\n",
        "\n",
        "# read one 'raw' text\n",
        "text = reuters.raw('training/9865')\n",
        "print(text)\n",
        "\n",
        "# topic lookup\n",
        "print(reuters.categories('training/9865'))\n",
        "print(reuters.categories('training/9880'))\n",
        "print(reuters.categories(['training/9865','training/9880']))\n",
        "\"\"\"\n",
        "['barley', 'corn', 'grain', 'wheat']\n",
        "['money-fx']\n",
        "['barley', 'corn', 'grain', 'money-fx', 'wheat']\n",
        "\"\"\"\n",
        "\n",
        "#find doc by topic\n",
        "print(reuters.fileids('cpu'))\n",
        "print(reuters.fileids(['cpu','naphtha']))\n",
        "\"\"\"\n",
        "['test/21245', 'training/5388', 'training/5460', 'training/5485']\n",
        "['test/17880', 'test/18480', 'test/19497', 'test/19903', 'test/21245', 'training/5388', 'training/5460', 'training/5485', 'training/6535', 'training/7397']\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# read two documents with same topic by words\n",
        "# we might bring something meaningful from this later\n",
        "print(reuters.words('training/5388'))\n",
        "print(reuters.words('training/5460'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# presidential inaugural.\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "#file ids\n",
        "print(inaugural.fileids())\n",
        "\n",
        "# read one of them by 'raw'\n",
        "print(inaugural.raw('1789-Washington.txt'))\n",
        "\n",
        "# Fun graph drawing session.\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (target, fileid[:4])\n",
        "    for fileid in inaugural.fileids()\n",
        "    for w in inaugural.words(fileid)\n",
        "    for target in ['america','citizen']\n",
        "    if w.lower().startswith(target))\n",
        "cfd['america']\n",
        "cfd['citizen']\n",
        "cfd.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L26zMXT9DugM",
        "colab_type": "text"
      },
      "source": [
        "- output: <matplotlib.axes._subplots.AxesSubplot at 0x27e97503648>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rJwm_qlDXwJ",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://drive.google.com/uc?export=view&id=1UWbwGGYyHLHgmEF-6i7ucQoaTiHZ7q3Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXDd0XDDq8BG",
        "colab_type": "text"
      },
      "source": [
        "##  WORDLIST\n",
        "all(?) existing words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVLeP5MXtUTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordlist = nltk.corpus.words.words()\n",
        "print('# of words :',len(wordlist)) #  # of words : 236736\n",
        "print(wordlist[:20])  #['A', 'a', 'aa', 'aal', 'aalii', ...\n",
        "print(wordlist[-20:]) # ..., 'yes', 'yesterday', 'you', 'young']\n",
        "\n",
        "\n",
        "# NAMES\n",
        "# retrieve a list of names\n",
        "names = nltk.corpus.names\n",
        "print(names.fileids())\n",
        "\"\"\"['female.txt', 'male.txt']\"\"\"\n",
        "\n",
        "maleNames = names.words('male.txt')\n",
        "femaleNames = names.words('female.txt')\n",
        "print(maleNames[:20])\n",
        "print(femaleNames[:20])\n",
        "\n",
        "\n",
        "# distribution of the last letters of male/female names\n",
        "# see any pattern?\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (fileid, name[-1])\n",
        "    for fileid in names.fileids()\n",
        "    for name in names.words(fileid))\n",
        "f = [w for w in cfd]\n",
        "cfd.plot()\n",
        "cfd[f[0]] #female last letter\n",
        "#FreqDist({'a': 1773, 'e': 1432, 'y': 461, ...\n",
        "cfd[f[1]] #male last letter\n",
        "#FreqDist({'n': 478, 'e': 468, 'y': 332, ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd1APshLtYyn",
        "colab_type": "text"
      },
      "source": [
        "## STOPWORDS - 불용어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cs4vdhJtV6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords as sss\n",
        "\n",
        "# stopwords in English\n",
        "StopWords = sss.words('english')\n",
        "print(len(StopWords))\n",
        "print(StopWords)\n",
        "print(sss.fileids())\n",
        "print(sss.words('spanish'))\n",
        "\n",
        "# see text without stopwords\n",
        "text = nltk.corpus.gutenberg.words('austen-sense.txt')\n",
        "print(text[:15])\n",
        "print(len(text)) #141576\n",
        "removed = [w for w in text if w.lower() not in StopWords]\n",
        "print(removed[:15])\n",
        "print(len(removed)) #74829"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHJLRX-ftdL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WORDNET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe6-Byw3tlN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import Wordnet\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXcqgXoutsLd",
        "colab_type": "text"
      },
      "source": [
        "## 어휘의 원형"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4TE8yLTtmQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 어휘의 원형\n",
        "wn.synsets('working')\n",
        "\"\"\"\n",
        "Synset('working.n.01'),\n",
        " Synset('work.v.01'),\n",
        " Synset('work.v.02'),\n",
        " Synset('work.v.03'),\n",
        " Synset('function.v.01'), ...\n",
        " \"\"\"\n",
        "\n",
        "# lookup synsets\n",
        "wn.synsets('car')\n",
        "\"\"\"\n",
        "[Synset('car.n.01'),\n",
        " Synset('car.n.02'),\n",
        " Synset('car.n.03'),\n",
        " Synset('car.n.04'),\n",
        " Synset('cable_car.n.01')]\"\"\"\n",
        "\n",
        "for synset in wn.synsets('car'):\n",
        "    print(\"Lemma : {}\".format(synset.name()))\n",
        "    print(\"Lemma : {}\".format(synset.definition()))\n",
        "    print(\"Lemma_names : {}\".format(synset.lemma_names()))\n",
        "    print()\n",
        "\"\"\"Lemma : car.n.01\n",
        "Lemma : a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
        "Lemma_names : ['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
        "\n",
        "Lemma : car.n.02\n",
        "Lemma : a wheeled vehicle adapted to the rails of railroad\n",
        "Lemma_names : ['car', 'railcar', 'railway_car', 'railroad_car']\n",
        "\n",
        "Lemma : car.n.03\n",
        "Lemma : the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
        "Lemma_names : ['car', 'gondola']\n",
        "\n",
        "Lemma : car.n.04\n",
        "Lemma : where passengers ride up and down\n",
        "Lemma_names : ['car', 'elevator_car']\n",
        "\n",
        "Lemma : cable_car.n.01\n",
        "Lemma : a conveyance for passengers or freight on a cable railway\n",
        "Lemma_names : ['cable_car', 'car']\n",
        "\"\"\"\n",
        "\n",
        "# lemma \n",
        "wn.lemma('work.v.02.work').synset()  #Synset('work.v.02')\n",
        "\n",
        "# hyponyms - 하위어\n",
        "\n",
        "wn.synset('tree.n.01').hyponyms()\n",
        "wn.synset('entity.n.01').hyponyms()\n",
        "\n",
        "# hypernyms - 상위어\n",
        "printer = wn.synset('printer.n.03')\n",
        "printer  #Synset('printer.n.03')\n",
        "\n",
        "printer.hypernyms()\n",
        "\n",
        "printer.root_hypernyms() # find the root of tree\n",
        "\n",
        "printer.hypernym_paths() # find the path from tree\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CjoDPqDtzfM",
        "colab_type": "text"
      },
      "source": [
        "##  Lexical relation. (어휘관계)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StY7Sv00t04g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wn.synsets('tree')\n",
        "\n",
        "#meronym\n",
        "wn.synset('tree.n.01').part_meronyms() # obtain parts\n",
        "\n",
        "wn.synset('tree.n.01').substance_meronyms() # obtain substances\n",
        "\n",
        "\n",
        "#holonym #denotes a membership to something\n",
        "wn.synset('atom.n.01').part_holonyms() \n",
        "\n",
        "wn.synset('hydrogen.n.01').substance_holonyms() \n",
        "\n",
        "#entailment #denotes how verbs are involved - 함의\n",
        "wn.synset('eat.v.01').entailments()\n",
        "#output: [Synset('chew.v.01'), Synset('swallow.v.01')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCLC27Gft_R9",
        "colab_type": "text"
      },
      "source": [
        "### similarity measure\n",
        "only in wordnet in this case. we use other methods for out KR NLP usages.\n",
        "besides path_similarity, there's Leacock-Chodorow similarity, Wu-Palmer similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmbysCDhuRcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## semantic similarity - only in wordnet. we use other methods for out KR NLP usages.\n",
        "truck = wn.synset('truck.n.01')\n",
        "limousine = wn.synset('limousine.n.01')\n",
        "\n",
        "### 공통 상위어\n",
        "truck.lowest_common_hypernyms(limousine)\n",
        "#output: [Synset('motor_vehicle.n.01')]\n",
        "\n",
        "\n",
        "## similarity measure   #other than path_similarity, Leacock-Chodorow, Wu-Palmer similarity\n",
        "#여기에서 어떤녀석인지 찾는다.\n",
        "wn.synsets('dog')\n",
        "\n",
        "dog = wn.synset('dog.n.01')\n",
        "wolf = wn.synset('wolf.n.01')\n",
        "cat = wn.synset('cat.n.01')\n",
        "human = wn.synset('homo.n.02')\n",
        "\n",
        "dog.path_similarity(cat) #0.2\n",
        "dog.path_similarity(wolf) #0.3333333333333333\n",
        "dog.path_similarity(human) #0.14285714285714285"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DfxmjbuTbN",
        "colab_type": "text"
      },
      "source": [
        "- about how\n",
        "following are images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht-weogPC9wz",
        "colab_type": "text"
      },
      "source": [
        "![nlp1](https://drive.google.com/uc?export=view&id=1Y-Styryh5pohgeeM1UC8UAWYSTYLFYrP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFYKeSP2E4eU",
        "colab_type": "text"
      },
      "source": [
        "![nlp2](https://drive.google.com/uc?export=view&id=1970l0nhqsjDsPk_UNi7b-aXQPNSuIl9G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1iJaZuHFE1m",
        "colab_type": "text"
      },
      "source": [
        "![nlp3](https://drive.google.com/uc?export=view&id=1qJFeDhrTB3Dm3xs8umsUkaFZE3NKWrhJ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65-TIN0C_bc",
        "colab_type": "text"
      },
      "source": [
        "### exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w33k57bsC_nN",
        "colab_type": "text"
      },
      "source": [
        "![exercise](https://drive.google.com/uc?export=view&id=19hFjEaYz-6oeZM1GUyLxMjNGTMFl-5Gq)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1VJ3WKGC_xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 지프의 법칙 (Zipf's Law)\n",
        "# f(w) : 단어 w의 빈도\n",
        "# rank(w) : 단어 w의 빈도 순위\n",
        "# f(w) ~ 1/rank(w)\n",
        "# log(f) = -1 * log(rank) : 기울기가 -1인 직선\n",
        "import nltk\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "text = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
        "fdist = nltk.FreqDist(text)\n",
        "print(fdist)\n",
        "\n",
        "x = np.arange(1, len(fdist) + 1)\n",
        "y = sorted(list(fdist.values()), reverse=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y)\n",
        "plt.xlim(1, 100)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "logx = np.log(x)\n",
        "logy = np.log(y)\n",
        "plt.plot(logx, logy)\n",
        "plt.show()\n",
        "\n",
        "print(\"기울기 = \", np.polyfit(logx, logy, 1)[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}