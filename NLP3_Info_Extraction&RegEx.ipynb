{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP3_Info_Extraction&RegEx.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8v2tqSlDXN31Thnsqm7Q0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP3_Info_Extraction%26RegEx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SwlWXERITqW",
        "colab_type": "text"
      },
      "source": [
        "# Information Extraction from Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaeS11NgIrSL",
        "colab_type": "text"
      },
      "source": [
        "## Regular Expression\n",
        "\n",
        "정규표현식\n",
        "* [정규식을 만들때 유용한 사이트 입니다. 정규식을 작성하고 적용되는 샘플 텍스트를 입력하면, 정규식의 의미와 샘플 텍스트에서 적용되는 문자를 알려주고, 많이 쓰이는 정규식또한 정리가 되있습니다.](https://regex101.com/)  \n",
        "* [cheatsheet부터 예시들까지 잘 나와있어서 정규표현식 공부하기 좋은것 같습니다](https://regexr.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85FCR410L72J",
        "colab_type": "text"
      },
      "source": [
        "## exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulg1KhQyESjW",
        "colab_type": "text"
      },
      "source": [
        "### play with a list of English words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgGw4TGlEHTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RE\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "\n",
        "# make list of English words\n",
        "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
        "\n",
        "\n",
        "# find from a string w any english word that starts with w and ends with ed\n",
        "v = [w for w in wordlist if re.search('^w.+ed$', w)]\n",
        "print(v[:20]) # ['waged', 'waistcoated', 'waisted', ... , 'wasted', 'watched']\n",
        "\n",
        "\n",
        "# textonyms(전화기문자판.. ;;;)\n",
        "v2 = [w for w in wordlist if re.search('^[abc][def][ghi]$',w)]\n",
        "print(v2) #beg \n",
        "v3 = [w for w in wordlist if re.search('^[ghi][mno][jlk][def]$',w)]\n",
        "print(v3) #['gold', 'golf', 'hold', 'hole']\n",
        "\n",
        "\n",
        "# NPS Chat corpus\n",
        "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
        "v4 = [w for w in chat_words if re.search('^m+i+n+e+$',w)]\n",
        "print(v4)\n",
        "# ['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine', 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n",
        "\n",
        "\n",
        "# start or end with either a or h with more than 1 repetition\n",
        "v5 = [w for w in chat_words if re.search('^[ah]+$',w)]\n",
        "print(v5[:15])\n",
        "# ['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G9d5AnyEOLJ",
        "colab_type": "text"
      },
      "source": [
        "### Penn Treebank corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPSVemEWEcC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wsj = set(nltk.corpus.treebank.words())\n",
        "\n",
        "v = [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$',w)]\n",
        "print(v[:20])\n",
        "# ['1.637', '494.50', '8.04', '2.75', '9.75', '143.80', '141.9', '80.8', '3.5', '8.25', '7.55', '415.6', '7.45', '38.3', '3.80', '5.6', '13.73', '6.20', '9.8', '47.5']\n",
        "\n",
        "\n",
        "v = [w for w in wsj if re.search('^[A-Z]+\\$$',w)]\n",
        "print(v) #['C$', 'US$']\n",
        "# at least 3 numbers.    ====== {3,} : no more that 3 numbers. ======= {3,4}: interval\n",
        "\n",
        "\n",
        "v= [w for w in wsj if re.search('^[[0-9]{,3}$',w)]\n",
        "print(v[:20])\n",
        "# ['353', '119', '913', '46', '57', '960', '184', '138', '26', '700', '190', '148', '34', '69', '37', '132', '73', '48', '203', '692']\n",
        "\n",
        "\n",
        "v= [w for w in wsj if re.search('^[0-9]+-[a-z]{3,6}$',w)]\n",
        "print(v[:20])\n",
        "# ['300-day', '12-member', '10-year', '237-seat', '36-minute', '15-day', '240-page', '10-day', '20-stock', '500-stock', '50-state', '150-point', '12-year', '12-point', '520-lawyer', '52-week', '90-day', '30-share', '27-year', '21-month']\n",
        "\n",
        "\n",
        "v = [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$',w)]\n",
        "print(v)\n",
        "# ['bread-and-butter', 'machine-gun-toting', 'savings-and-loan', 'father-in-law', 'black-and-white']\n",
        "\n",
        "\n",
        "v = [w for w in wsj if re.search('(ed|ing|nt)$',w)] # words end with ed, ing, nt\n",
        "print(v[30:])\n",
        "# [reacted', 'angered', 'happening', 'Datapoint', 'penchant', 'reciting', 'unwilling']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USbXxDR0Eu_K",
        "colab_type": "text"
      },
      "source": [
        "### re.findall(pattern, sentence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYvISIe-E3As",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
        "fd = nltk.FreqDist(vs for word in wsj\n",
        "                       for vs in re.findall(r'[aeiou]{2,}',word))\n",
        "\n",
        "print(fd)\n",
        "# FreqDist({'io': 549, 'ea': 476, 'ie': 331, 'ou': 329, 'ai': 261, 'ia': 253, 'ee': 217, 'oo': 174, 'ua': 109, 'au': 106, ...})\n",
        "\n",
        "\n",
        "print(fd.most_common(12))\n",
        "# [('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253), ('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KbWQzH4FBH_",
        "colab_type": "text"
      },
      "source": [
        "### '?' expression\n",
        "\n",
        "'?' Matches 0 or 1 of the preceding token, effectively making it optional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihU3_SOsFD6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re.search(r'ab?','ac') # same expression re.search('ab?','ac') # is it not? r'smth' means it is a raw string. not like \\\\\n",
        "# output: <re.Match object; span=(0, 1), match='a'>\n",
        "\n",
        "# empty list\n",
        "x = [x for x in ['ac','c'] if re.search(r'ab?','c')]\n",
        "\n",
        "# condition is always true\n",
        "x = [x for x in ['ac','c','abc'] if re.search(r'ab?','ac')]\n",
        "['ac', 'c', 'abc']\n",
        "\n",
        "# if x meets the condition\n",
        "x = [x for x in ['ac','c','abc'] if re.search(r'ab?',x)]\n",
        "['ac', 'abc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wve2Jp_8FLm4",
        "colab_type": "text"
      },
      "source": [
        "### /w /b /W /B\n",
        "\n",
        "[useful_link](https://stackoverflow.com/questions/11874234/difference-between-w-and-b-regular-expression-meta-characters)\n",
        "> \\w is not a word boundary, it matches any word character, including underscores: [a-zA-Z0-9_]. \\b is a word boundary, that is, it matches the position between a word and a non-alphanumeric character: \\W or [^\\w].\n",
        "\n",
        "> \\w stands for \"word character\", usually [A-Za-z0-9_]. Notice the inclusion of the underscore and digits.\n",
        "\n",
        "> \\B is the negated version of \\b. \\B matches at every position where \\b does not. Effectively, \\B matches at any position between two word characters as well as at any position between two non-word characters.\n",
        "\n",
        "> \\W is short for [^\\w], the negated version of \\w."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRH_orWIFt7o",
        "colab_type": "text"
      },
      "source": [
        "### re.finditer()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWqNT8qNFzrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# re.finditer()\n",
        "text = 'Diwali is a festival of lights, Holi is a festival of colors!'\n",
        "pattern = 'festival'\n",
        "\n",
        "for match in re.finditer(pattern,text):\n",
        "    s = match.start()\n",
        "    e = match.end()\n",
        "    print('found %s in idx %d:%d'%(text[s:e],s,e))\n",
        "# found festival in idx 12:20\n",
        "# found festival in idx 42:50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9q4y9AhF1T0",
        "colab_type": "text"
      },
      "source": [
        "### find date\n",
        "\n",
        "find a wanted string by using REGEX. a date and from an url in this case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnufc7BWGJHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find date by regex\n",
        "url = 'http://www.telegraph.co.uk/formula-1/2018/10/28/mexican-grand-prix-2017-time-does-start-tv-channel-odds-lewis1/'\n",
        "# \\d indicates a sigle digit number between 0-9\n",
        "date_regex = '/([0-9]{4})/(\\d{2})/(\\d{2})'\n",
        "\n",
        "print(re.findall(date_regex, url)) # [('2018', '10', '28')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vykzIsyGKoQ",
        "colab_type": "text"
      },
      "source": [
        "### .(dot) and ^ in []\n",
        "\n",
        "> # `in []`\n",
        "> negative set  Match any character that is not in the set.\n",
        "\n",
        "pattern : `[^aeiou]  `\n",
        "\n",
        "sentence : `'glib jocks vex dwarves!'`\n",
        "\n",
        "returns `'glb jcks vx dwrvs!'` **including space**\n",
        "\n",
        "\n",
        "> # `.(dot)` \n",
        "> Matches any character except linebreaks.\n",
        "\n",
        "> Equivalent to [^\\n\\r].\n",
        "\n",
        "\n",
        "pattern : `.`\n",
        "\n",
        "sentence : `'glib jocks vex dwarves!'`\n",
        "\n",
        "returns `'glib jocks vex dwarves!'` - **all including space**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nqd1gLvGQW_",
        "colab_type": "text"
      },
      "source": [
        "### Chunking\n",
        "\n",
        "construsting a phrase(구) with a number of POS, this phrase is called chunk\n",
        "like so in the image. # nlp5\n",
        "\n",
        "first from a sentence to POS, then to phrase by chunking.\n",
        "chunking is used for entity detection in information extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iTpIysTIW0o",
        "colab_type": "text"
      },
      "source": [
        "![chinkingintro](https://drive.google.com/uc?export=view&id=1uouFsUrcv45kFYELz-qdmla7sPvjZeXd)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO7-qr9MGpyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = [(\"the\",\"DT\"),(\"little\",\"JJ\"),(\"yellow\",\"JJ\"),(\"dog\",\"NN\"),(\"barked\",\"VBD\"),\n",
        "            (\"at\",\"IN\"),(\"the\",\"DT\"),(\"cat\",\"NN\")]\n",
        "\n",
        "# define a grammar rule for chunking by using REGEX\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\" # <> : tag pattern\n",
        "\n",
        "# create chunk parser\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "\n",
        "# using the created chunk parser, parse sentence\n",
        "result = cp.parse(sentence)\n",
        "\n",
        "print(result)\n",
        "# (S\n",
        "#   (NP the/DT little/JJ yellow/JJ dog/NN)\n",
        "#   barked/VBD\n",
        "#   at/IN\n",
        "#   (NP the/DT cat/NN))\n",
        "\n",
        "\n",
        "result.draw() # nltk method"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq2WNp3vH2k2",
        "colab_type": "text"
      },
      "source": [
        "![chunk1](https://drive.google.com/uc?export=view&id=1Ym4CTcnn8h4PgIVTDiSwpSIZA2tyuGf8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTMSJNbGH0gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply two rules\n",
        "grammar = \"\"\"\n",
        "    NP: {<DT>?<JJ>*<NN>}\n",
        "        {<NNP>+}\n",
        "\"\"\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "sentence = [(\"Rapunzel\",\"NNP\"),(\"the\",\"DT\"),(\"little\",\"JJ\"),(\"yellow\",\"JJ\"),(\"dog\",\"NN\"),(\"barked\",\"VBD\"),\n",
        "            (\"at\",\"IN\"),(\"the\",\"DT\"),(\"cat\",\"NN\")]\n",
        "result = cp.parse(sentence)\n",
        "print(result)\n",
        "result.draw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOBm_gPZH41A",
        "colab_type": "text"
      },
      "source": [
        "![chunk2](https://drive.google.com/uc?export=view&id=1Y2exLnc2onYy0yq9ikB32MfjDLeyCNbP)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ncR37ZjJFu2",
        "colab_type": "text"
      },
      "source": [
        "### Chinking\n",
        "\n",
        "Chinking\n",
        "draws the same result as chunking\n",
        "if \n",
        "1. chucnk a whole sentence \n",
        "2. chink with a certain rule/word \n",
        "3. done\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jz4M7OnJKPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grammar = r\"\"\"\n",
        "    NP: {<.*>+}  # chunk everything\n",
        "        }<VBD|IN>+{  #chink sequences of VBD and IN\n",
        "\"\"\"        # | acts as a Boolean OR operator.\n",
        "\n",
        "sentence = [(\"the\",\"DT\"),(\"little\",\"JJ\"),(\"yellow\",\"JJ\"),(\"dog\",\"NN\"),(\"barked\",\"VBD\"),\n",
        "            (\"at\",\"IN\"),(\"the\",\"DT\"),(\"cat\",\"NN\")]\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(sentence)\n",
        "print(result)\n",
        "result.draw() # same as above result with one rule chunking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USmL4OKRJcBK",
        "colab_type": "text"
      },
      "source": [
        "![chink](https://drive.google.com/uc?export=view&id=1Ym4CTcnn8h4PgIVTDiSwpSIZA2tyuGf8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptMHUKvFJgid",
        "colab_type": "text"
      },
      "source": [
        "### NER named entity recognition\n",
        "\n",
        "![ner1](https://drive.google.com/uc?export=view&id=1ajWf-M5B-juPIQEJgJ0bkX5ddynBAu5c)\n",
        "\n",
        "![ner2](https://drive.google.com/uc?export=view&id=1hNA8gouADCyQ-J_XT0NCoSSRtjdkTfD4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypeV7u_KXmFa",
        "colab_type": "text"
      },
      "source": [
        "# Analzing Sentence Structure\n",
        "\n",
        "문장구조분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBxHDE6ZXwCF",
        "colab_type": "text"
      },
      "source": [
        "## Ambiguity\n",
        "\n",
        "a sentence can be ambiguous.\n",
        "\n",
        "While hunting in Africa, I shot an elephant in my pajamas.\n",
        "\n",
        "we will use `nltk.CFG.fromstring` to set up a grammar rule to apply on the sentence. # cfg stands for Context Free Grammar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAzpSJM2YuP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "groucho_grammar = nltk.CFG.fromstring(\n",
        "    \"\"\"\n",
        "    S -> NP VP\n",
        "    PP -> P NP\n",
        "    NP -> Det N | Det N PP | 'I'\n",
        "    VP -> V NP | VP PP\n",
        "    Det -> 'an' | 'my'\n",
        "    N -> 'elephant' | 'pajamas'\n",
        "    V -> 'shot'\n",
        "    P -> 'in\n",
        "    \"\"\")\n",
        "\n",
        "# see the tree structure of the sentece with the grammar applied\n",
        "sent = ['I','shot','an','elephany','in','my','pajamas']\n",
        "parser = nltk.ChartParser(groucho_grammar)\n",
        "for tree in parser.parse(sent):\n",
        "    print(tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Dn8zQCZdbm",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "(S\n",
        "  (NP I)\n",
        "  (VP\n",
        "    (VP (V shot) (NP (Det an) (N elephant)))\n",
        "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
        "(S\n",
        "  (NP I)\n",
        "  (VP\n",
        "    (V shot)\n",
        "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDt_ltcSaXLF",
        "colab_type": "text"
      },
      "source": [
        "## Formal Language Theory - 형식언어이론\n",
        "\n",
        "Chomsky Hierarhy\n",
        "type 0 : close to human language(natural language)\n",
        "type 3 : regular expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Zi0PLBaysn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}