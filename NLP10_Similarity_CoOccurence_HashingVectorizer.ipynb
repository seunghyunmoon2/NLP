{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP10_8_4_HashingTrick_CoOccurence_HashingVectorizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPzBETQn6N+fV+92i5KUEst",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyunmoon2/NLP/blob/master/NLP10_Similarity_CoOccurence_HashingVectorizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT4I0WewD29c",
        "colab_type": "text"
      },
      "source": [
        "# OneHot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvYZ8XHK0muS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "26ecc2c4-aa66-4a9c-c223-2423da3a82b5"
      },
      "source": [
        "# one-hot encoding 연습\n",
        "# ---------------------\n",
        "import numpy as np\n",
        "\n",
        "data = ['남자', '여자', '아빠', '엄마', '삼촌', '이모']\n",
        "values = np.array(data)\n",
        "print(values)\n",
        "print(sorted(values))\n",
        "\n",
        "# sklearn의 preprocessing을 이용한 one-hot encoding\n",
        "import sklearn.preprocessing as sk\n",
        "\n",
        "label_encoder = sk.LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "\n",
        "# integer encoding\n",
        "print(integer_encoded)\n",
        "\n",
        "# binary encoding\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoder = sk.OneHotEncoder(sparse=False, categories='auto')\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "\n",
        "# Keras를 이용한 one-hot encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "encoded = to_categorical(integer_encoded)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['남자' '여자' '아빠' '엄마' '삼촌' '이모']\n",
            "['남자', '삼촌', '아빠', '엄마', '여자', '이모']\n",
            "[0 4 2 3 1 5]\n",
            "[[1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n",
            "[[1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRmPpMcID51L",
        "colab_type": "text"
      },
      "source": [
        "# word embedding using Hashing Trick"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBJvjWKP37gG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90197ac6-ef1c-4e6d-f044-04414ef3fc4a"
      },
      "source": [
        "# Hashing trick을 이용한 word embedding\n",
        "# -------------------------------------\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import hashing_trick\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "    \n",
        "samples = ['너 오늘 이뻐 보인다', \n",
        "           '나는 오늘 기분이 더러워', \n",
        "           '끝내주는데, 좋은 일이 있나봐', \n",
        "           '나 좋은 일이 생겼어', \n",
        "           '아 오늘 진짜 짜증나', \n",
        "           '환상적인데, 정말 좋은거 같아']\n",
        "labels = [[1], [0], [1], [1], [0], [1]]\n",
        "\n",
        "# hash 테이블로 문서를 수치화한다.\n",
        "VOCAB_SIZE = 10 # vocabulary 크기 (hash table)를 10개로 한정한다.\n",
        "sequences = [hashing_trick(s, VOCAB_SIZE) for s in samples]\n",
        "sequences = np.array(sequences)\n",
        "labels = np.array(labels)\n",
        "print(sequences)\n",
        "\n",
        "# Embedding layer 내부의 출력층 개수임. 단어의 latent feature 개수\n",
        "EMB_SIZE = 8\n",
        "\n",
        "# 딥러닝 모델을 빌드한다.\n",
        "xInput = Input(batch_shape=(None, sequences.shape[1]))\n",
        "embed_input = Embedding(input_dim=VOCAB_SIZE + 1, output_dim=EMB_SIZE)(xInput)\n",
        "embed_input1 = tf.reduce_mean(embed_input, axis=-1)\n",
        "\n",
        "hidden_layer = Dense(128, activation=tf.nn.relu)(embed_input1)\n",
        "output = Dense(1, activation='sigmoid')(hidden_layer)\n",
        "model = Model(xInput, output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01))\n",
        "\n",
        "# 학습\n",
        "model.fit(sequences, labels, epochs=100)\n",
        "\n",
        "# 추정\n",
        "pred = model.predict(sequences)\n",
        "print(np.round(pred, 0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8 7 7 3]\n",
            " [9 7 5 9]\n",
            " [4 1 7 5]\n",
            " [5 1 7 9]\n",
            " [6 7 7 5]\n",
            " [3 6 7 2]]\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6928\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6830\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6696\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6569\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6441\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6309\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6179\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6049\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5916\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5771\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5601\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5389\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.5127\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4818\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4469\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4113\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3771\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3463\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3167\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2881\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2588\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2298\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2016\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1747\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1499\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1272\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1069\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0889\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0734\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0600\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0483\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0387\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0307\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0242\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0189\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0147\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0114\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0089\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0069\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0054\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0043\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0034\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0013\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.3707e-04\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.1365e-04\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.1324e-04\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.3082e-04\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.6260e-04\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.0566e-04\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.5777e-04\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.1720e-04\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.8253e-04\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 3.5274e-04\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2697e-04\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 3.0458e-04\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.8501e-04\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.6781e-04\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.5263e-04\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.3916e-04\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.2716e-04\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.1642e-04\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.0678e-04\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.9810e-04\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.9024e-04\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8310e-04\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7663e-04\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7071e-04\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6527e-04\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6027e-04\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5567e-04\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5146e-04\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4755e-04\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4392e-04\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4053e-04\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3738e-04\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3445e-04\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3169e-04\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2910e-04\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2665e-04\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.2433e-04\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2213e-04\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2005e-04\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1806e-04\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1616e-04\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1434e-04\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1260e-04\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1093e-04\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0933e-04\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0780e-04\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0635e-04\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0493e-04\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0356e-04\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0224e-04\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0095e-04\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.9695e-05\n",
            "[[1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwIsiTOtD_aC",
        "colab_type": "text"
      },
      "source": [
        "# Co-Occurrence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuNdYz_s4Ut0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0bde99bd-b205-47a0-d813-107573b5267a"
      },
      "source": [
        "# co-occurrence encoding 연습\n",
        "# ---------------------------\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "docs = ['성진과 창욱은 야구장에 갔다',\n",
        "        '성진과 태균은 도서관에 갔다',\n",
        "        '성진과 창욱은 공부를 좋아한다']\n",
        "\n",
        "count_model = CountVectorizer(ngram_range=(1,1))\n",
        "x = count_model.fit_transform(docs)\n",
        "\n",
        "# 문서에 사용된 사전을 조회한다.\n",
        "print(count_model.vocabulary_)\n",
        "\n",
        "# co-occurrence 행렬을 조회한다. Compact Sparse Row(CSR) format\n",
        "# (row, col) value\n",
        "print(x)\n",
        "\n",
        "# 행렬 형태로 표시한다.\n",
        "print(x.toarray())\n",
        "print()\n",
        "print(x.T.toarray())\n",
        "\n",
        "#x.T의 의미\n",
        "#          1 2 3  - 문장\n",
        "#갔다    [[1 1 0] - '갔다'라는 단어는 문장-1과 문장-2에 쓰였음.\n",
        "#공부를   [0 0 1] - '공부를'은 문장-3에만 쓰였음.\n",
        "#도서관에 [0 1 0]\n",
        "#성진과   [1 1 1]\n",
        "#야구장에 [1 0 0]\n",
        "#좋아한다 [0 0 1]\n",
        "#창욱은   [1 0 1]\n",
        "#태균은   [0 1 0]]\n",
        "\n",
        "xc = x.T * x # this is co-occurrence matrix in sparse csr format\n",
        "xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
        "print(xc.toarray())\n",
        "\n",
        "#              0       1       2        3        4         5        6       7\n",
        "#             갔다  공부를  도서관에  성진과  야구장에  좋아한다  창욱은  태균은\n",
        "#0 갔다        0       0       1        2        1         0        1       1\n",
        "#1 공부를      0       0       0        1        0         1        1       0\n",
        "#2 도서관에    1       0       0        1        0         0        0       1\n",
        "#3 성진과      2       1       1        0        1         1        2       1\n",
        "#4 야구장에    1       0       0        1        0         0        1       0\n",
        "#5 좋아한다    0       1       0        1        0         0        1       0\n",
        "#6 창욱은      1       1       0        2        1         1        0       0\n",
        "#7 태균은      1       0       1        1        0         0        0       0\n",
        "\n",
        "# ngram_range(min_n = 1, max_n = 2)인 경우\n",
        "#count_model = CountVectorizer(ngram_range=(1,2))\n",
        "#x = count_model.fit_transform(docs)\n",
        "\n",
        "# 문서에 사용된 사전을 조회한다.\n",
        "#print(count_model.vocabulary_)\n",
        "\n",
        "xc = x.T * x # this is co-occurrence matrix in sparse csr format\n",
        "xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
        "print(xc.toarray())\n",
        "\n",
        "# Co-occurrence matrix를 SVD로 분해한다.\n",
        "# C = U.S.VT\n",
        "# numpy를 이용한 SVD 예시\n",
        "import numpy as np\n",
        "C = xc.toarray()\n",
        "U, S, VT = np.linalg.svd(C, full_matrices = True)\n",
        "print(np.round(U, 2), '\\n')\n",
        "print(np.round(S, 2), '\\n')\n",
        "print(np.round(VT, 2), '\\n')\n",
        "\n",
        "# S를 정방행렬로 바꾼다.\n",
        "s = np.diag(S)\n",
        "print(np.round(s, 2))\n",
        "\n",
        "# A = U.s.VT를 계산하고, A와 C가 일치하는지 확인한다.\n",
        "A = np.dot(U, np.dot(s, VT))\n",
        "print(np.round(A, 1))\n",
        "print(C)\n",
        "\n",
        "# sklearn을 이용한 SVD 예시\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# 특이값 (S)이 큰 4개를 주 성분으로 C의 차원을 축소한다.\n",
        "svd = TruncatedSVD(n_components=4, n_iter=7)\n",
        "D = svd.fit_transform(xc.toarray())\n",
        "\n",
        "U = D / svd.singular_values_\n",
        "S = np.diag(svd.singular_values_)\n",
        "VT = svd.components_\n",
        "\n",
        "print(\"\\nU, S, VT :\")\n",
        "print(np.round(U, 2), '\\n')\n",
        "print(np.round(S, 2), '\\n')\n",
        "print(np.round(VT, 2), '\\n')\n",
        "\n",
        "print(\"C를 4개 차원으로 축소 : truncated (U * S)\")\n",
        "print(np.round(D, 2))\n",
        "\n",
        "# U * S * VT 하면 원래 C의 차원과 동일해 진다. U * S가 축소된\n",
        "# 차원을 의미하고, V는 축소된 차원을 원래 차원으로 되돌리는 역할을\n",
        "# 한다 (mapping back)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'성진과': 3, '창욱은': 6, '야구장에': 4, '갔다': 0, '태균은': 7, '도서관에': 2, '공부를': 1, '좋아한다': 5}\n",
            "  (0, 3)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 0)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 2)\t1\n",
            "  (2, 3)\t1\n",
            "  (2, 6)\t1\n",
            "  (2, 1)\t1\n",
            "  (2, 5)\t1\n",
            "[[1 0 0 1 1 0 1 0]\n",
            " [1 0 1 1 0 0 0 1]\n",
            " [0 1 0 1 0 1 1 0]]\n",
            "\n",
            "[[1 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 1 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 1]\n",
            " [0 1 0]]\n",
            "[[0 0 1 2 1 0 1 1]\n",
            " [0 0 0 1 0 1 1 0]\n",
            " [1 0 0 1 0 0 0 1]\n",
            " [2 1 1 0 1 1 2 1]\n",
            " [1 0 0 1 0 0 1 0]\n",
            " [0 1 0 1 0 0 1 0]\n",
            " [1 1 0 2 1 1 0 0]\n",
            " [1 0 1 1 0 0 0 0]]\n",
            "[[0 0 1 2 1 0 1 1]\n",
            " [0 0 0 1 0 1 1 0]\n",
            " [1 0 0 1 0 0 0 1]\n",
            " [2 1 1 0 1 1 2 1]\n",
            " [1 0 0 1 0 0 1 0]\n",
            " [0 1 0 1 0 0 1 0]\n",
            " [1 1 0 2 1 1 0 0]\n",
            " [1 0 1 1 0 0 0 0]]\n",
            "[[-0.44 -0.39  0.41 -0.58  0.35  0.    0.   -0.19]\n",
            " [-0.24 -0.12  0.41  0.29 -0.24  0.62 -0.34  0.35]\n",
            " [-0.24 -0.12 -0.41 -0.29 -0.24 -0.34 -0.62  0.35]\n",
            " [-0.56  0.8  -0.    0.    0.19 -0.    0.    0.02]\n",
            " [-0.27 -0.01 -0.   -0.   -0.7   0.   -0.   -0.66]\n",
            " [-0.24 -0.12  0.41  0.29 -0.24 -0.62  0.34  0.35]\n",
            " [-0.44 -0.39 -0.41  0.58  0.35 -0.    0.   -0.19]\n",
            " [-0.24 -0.12 -0.41 -0.29 -0.24  0.34  0.62  0.35]] \n",
            "\n",
            "[5.27 2.52 1.73 1.73 1.27 1.   1.   0.53] \n",
            "\n",
            "[[-0.44 -0.24 -0.24 -0.56 -0.27 -0.24 -0.44 -0.24]\n",
            " [ 0.39  0.12  0.12 -0.8   0.01  0.12  0.39  0.12]\n",
            " [-0.71  0.   -0.    0.    0.    0.    0.71 -0.  ]\n",
            " [-0.    0.5  -0.5  -0.    0.    0.5  -0.   -0.5 ]\n",
            " [-0.35  0.24  0.24 -0.19  0.7   0.24 -0.35  0.24]\n",
            " [-0.   -0.62  0.34  0.    0.    0.62  0.   -0.34]\n",
            " [-0.    0.34  0.62 -0.    0.   -0.34 -0.   -0.62]\n",
            " [-0.19  0.35  0.35  0.02 -0.66  0.35 -0.19  0.35]] \n",
            "\n",
            "[[5.27 0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   2.52 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   1.73 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   1.73 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   1.27 0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   1.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.53]]\n",
            "[[ 0.  0.  1.  2.  1.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.  1.  1.  0.]\n",
            " [ 1.  0.  0.  1. -0. -0.  0.  1.]\n",
            " [ 2.  1.  1.  0.  1.  1.  2.  1.]\n",
            " [ 1.  0. -0.  1.  0.  0.  1. -0.]\n",
            " [ 0.  1.  0.  1.  0. -0.  1.  0.]\n",
            " [ 1.  1. -0.  2.  1.  1.  0. -0.]\n",
            " [ 1. -0.  1.  1. -0. -0.  0.  0.]]\n",
            "[[0 0 1 2 1 0 1 1]\n",
            " [0 0 0 1 0 1 1 0]\n",
            " [1 0 0 1 0 0 0 1]\n",
            " [2 1 1 0 1 1 2 1]\n",
            " [1 0 0 1 0 0 1 0]\n",
            " [0 1 0 1 0 0 1 0]\n",
            " [1 1 0 2 1 1 0 0]\n",
            " [1 0 1 1 0 0 0 0]]\n",
            "\n",
            "U, S, VT :\n",
            "[[ 0.44 -0.39  0.58  0.41]\n",
            " [ 0.24 -0.12 -0.29  0.41]\n",
            " [ 0.24 -0.12  0.29 -0.41]\n",
            " [ 0.56  0.8  -0.   -0.  ]\n",
            " [ 0.27 -0.01 -0.    0.  ]\n",
            " [ 0.24 -0.12 -0.29  0.41]\n",
            " [ 0.44 -0.39 -0.58 -0.41]\n",
            " [ 0.24 -0.12  0.29 -0.41]] \n",
            "\n",
            "[[5.27 0.   0.   0.  ]\n",
            " [0.   2.52 0.   0.  ]\n",
            " [0.   0.   1.73 0.  ]\n",
            " [0.   0.   0.   1.73]] \n",
            "\n",
            "[[ 0.44  0.24  0.24  0.56  0.27  0.24  0.44  0.24]\n",
            " [ 0.39  0.12  0.12 -0.8   0.01  0.12  0.39  0.12]\n",
            " [-0.   -0.5   0.5   0.    0.   -0.5  -0.    0.5 ]\n",
            " [-0.71 -0.   -0.    0.   -0.   -0.    0.71  0.  ]] \n",
            "\n",
            "C를 4개 차원으로 축소 : truncated (U * S)\n",
            "[[ 2.31 -0.97  1.    0.71]\n",
            " [ 1.24 -0.3  -0.5   0.71]\n",
            " [ 1.24 -0.3   0.5  -0.71]\n",
            " [ 2.97  2.03 -0.   -0.  ]\n",
            " [ 1.44 -0.03 -0.    0.  ]\n",
            " [ 1.24 -0.3  -0.5   0.71]\n",
            " [ 2.31 -0.97 -1.   -0.71]\n",
            " [ 1.24 -0.3   0.5  -0.71]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxfD37DnEFeC",
        "colab_type": "text"
      },
      "source": [
        "# Similarity using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmhvmrMj4XLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fe62e132-2758-4671-9ee3-1a5816acb1ee"
      },
      "source": [
        "# Tfidf를 이용한 유사도 측정 예시\n",
        "# 유사도 : 자카드, 코사인, 유클리디언, 맨하탄 유사도\n",
        "# ------------------------------------------------\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "sent = (\"휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.\", \n",
        "        \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니다.\") \n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sent).toarray()\n",
        "print(np.round(tfidf_matrix, 3))\n",
        "\n",
        "# 1. 자카드 유사도\n",
        "# ----------------\n",
        "sent_1 = set(sent[0].split())\n",
        "sent_2 = set(sent[1].split())\n",
        "print(sent_1)\n",
        "print(sent_2)\n",
        "\n",
        "# 합집합과 교집합을 구한다.\n",
        "hap_set = sent_1 | sent_2\n",
        "gyo_set = sent_1 & sent_2\n",
        "print(hap_set, '\\n')\n",
        "print(gyo_set, '\\n')\n",
        "\n",
        "jaccard = len(gyo_set) / len(hap_set)\n",
        "print(jaccard)\n",
        "\n",
        "# 2. 코사인 유사도\n",
        "# ---------------\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "d = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "print(d)\n",
        "\n",
        "# 3. 유클리디안 유사도\n",
        "# -------------------\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "euclidean_distances(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "\n",
        "# 정규화\n",
        "def l1_normalize(v):\n",
        "    return v / np.sum(v)\n",
        "\n",
        "tfidf_norm_l1 = l1_normalize(tfidf_matrix)\n",
        "d = euclidean_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])\n",
        "print(d)\n",
        "\n",
        "# 4. 맨하탄 유사도\n",
        "# ---------------\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "\n",
        "d = manhattan_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.    0.324 0.    0.    0.324 0.324 0.324 0.324 0.324 0.324 0.    0.231\n",
            "  0.324 0.231 0.    0.    0.231]\n",
            " [0.365 0.    0.365 0.365 0.    0.    0.    0.    0.    0.    0.365 0.259\n",
            "  0.    0.259 0.365 0.365 0.259]]\n",
            "{'중심', '휴일', '오늘', '이', '은', '비', '도', '반가운', '인', '으로', '소식', '폭염', '서쪽', '을', '이어졌는데요,', '있습니다.', '내일'}\n",
            "{'휴일', '피해서', '갑작스런', '인해', '비', '망연자실', '하고', '로', '폭염', '에', '을', '놀러왔다가', '있습니다.'}\n",
            "{'오늘', '은', '갑작스런', '인해', '비', '망연자실', '하고', '로', '서쪽', '을', '있습니다.', '놀러왔다가', '중심', '휴일', '이', '피해서', '도', '반가운', '인', '으로', '소식', '폭염', '이어졌는데요,', '에', '내일'} \n",
            "\n",
            "{'휴일', '비', '폭염', '을', '있습니다.'} \n",
            "\n",
            "0.2\n",
            "[[0.17952266]]\n",
            "[[0.20491229]]\n",
            "[[0.77865927]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opTqTCguEMI3",
        "colab_type": "text"
      },
      "source": [
        "# Similarity using HashingVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO5bQXc_4Y6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "3c0c4a6e-f2e3-48ae-d2f4-3682ec859bc2"
      },
      "source": [
        "# HashingVectorizer를 이용한 유사도 측정 예시\n",
        "# 유사도 : 자카드, 코사인, 유클리디언, 맨하탄 유사도\n",
        "# ------------------------------------------------\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "sent = (\"휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.\", \n",
        "        \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니다.\") \n",
        "\n",
        "# 카운트 기반\n",
        "VOCAB_SIZE = 20\n",
        "hvectorizer = HashingVectorizer(n_features=VOCAB_SIZE,norm=None,alternate_sign=False)\n",
        "hash_matrix = hvectorizer.fit_transform(sent).toarray()\n",
        "print(hash_matrix)\n",
        "\n",
        "# L2 normalization\n",
        "VOCAB_SIZE = 20\n",
        "hvectorizer = HashingVectorizer(n_features=VOCAB_SIZE,norm='l2',alternate_sign=False)\n",
        "hash_matrix = hvectorizer.fit_transform(sent).toarray()\n",
        "print(np.round(hash_matrix, 3))\n",
        "\n",
        "# 1. 자카드 유사도\n",
        "# ----------------\n",
        "sent_1 = set(sent[0].split())\n",
        "sent_2 = set(sent[1].split())\n",
        "print(sent_1)\n",
        "print(sent_2)\n",
        "\n",
        "# 합집합과 교집합을 구한다.\n",
        "hap_set = sent_1 | sent_2\n",
        "gyo_set = sent_1 & sent_2\n",
        "print(hap_set, '\\n')\n",
        "print(gyo_set, '\\n')\n",
        "\n",
        "jaccard = len(gyo_set) / len(hap_set)\n",
        "print(jaccard)\n",
        "\n",
        "# 2. 코사인 유사도\n",
        "# ---------------\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "d = cosine_similarity(hash_matrix[0:1], hash_matrix[1:2])\n",
        "print(d)\n",
        "\n",
        "# 3. 유클리디안 유사도\n",
        "# -------------------\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "euclidean_distances(hash_matrix[0:1], hash_matrix[1:2])\n",
        "\n",
        "# 정규화\n",
        "def l1_normalize(v):\n",
        "    return v / np.sum(v)\n",
        "\n",
        "tfidf_norm_l1 = l1_normalize(hash_matrix)\n",
        "d = euclidean_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])\n",
        "print(d)\n",
        "\n",
        "# 4. 맨하탄 유사도\n",
        "# ---------------\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "\n",
        "d = manhattan_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])\n",
        "print(d)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 2. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 2. 2. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 2. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0.]]\n",
            "[[0.    0.485 0.    0.    0.    0.243 0.243 0.243 0.243 0.    0.    0.\n",
            "  0.485 0.485 0.    0.243 0.    0.    0.    0.   ]\n",
            " [0.    0.555 0.277 0.    0.    0.277 0.277 0.    0.    0.    0.277 0.\n",
            "  0.    0.    0.    0.    0.555 0.277 0.    0.   ]]\n",
            "{'중심', '휴일', '오늘', '이', '은', '비', '도', '반가운', '인', '으로', '소식', '폭염', '서쪽', '을', '이어졌는데요,', '있습니다.', '내일'}\n",
            "{'휴일', '피해서', '갑작스런', '인해', '비', '망연자실', '하고', '로', '폭염', '에', '을', '놀러왔다가', '있습니다.'}\n",
            "{'오늘', '은', '갑작스런', '인해', '비', '망연자실', '하고', '로', '서쪽', '을', '있습니다.', '놀러왔다가', '중심', '휴일', '이', '피해서', '도', '반가운', '인', '으로', '소식', '폭염', '이어졌는데요,', '에', '내일'} \n",
            "\n",
            "{'휴일', '비', '폭염', '을', '있습니다.'} \n",
            "\n",
            "0.2\n",
            "[[0.40360368]]\n",
            "[[0.21149137]]\n",
            "[[0.62427015]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPelytu9ERRi",
        "colab_type": "text"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2OCBbSp4eM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 이해하기 : 탐색적 데이터 분석 (예시)\n",
        "# -----------------------------------------\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# 영화 리뷰 데이터 불러오기\n",
        "data_set = tf.keras.utils.get_file(\n",
        "      fname=\"imdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "\n",
        "# 다운받은 데이터 셋의 경로를 확인한다.\n",
        "#\n",
        "# 아래 path의 하위 폴더\n",
        "# aclimdb/test : 테스트 데이터\n",
        "# aclimdb/train : 학습 데이터\n",
        "# aclimdb/test/neg : negative 데이터 파일 (txt format)\n",
        "# aclimdb/test/pos : positive 데이터 파일 (txt format)\n",
        "data_path = os.path.dirname(data_set)\n",
        "print(data_path)\n",
        "\n",
        "#시험 데이터의 negative review file 이름 몇 개를 조회해 본다.\n",
        "aclimdb = os.path.join(data_path, \"aclimdb/test/neg\")\n",
        "print(aclimdb)\n",
        "\n",
        "file_name = os.listdir(aclimdb)\n",
        "print(file_name[:20])\n",
        "\n",
        "# Negative review 문서 2개의 내용을 확인해 본다\n",
        "for file_path in file_name[:2]:\n",
        "    with open(os.path.join(aclimdb, file_path), \"r\", encoding='utf-8') as file:\n",
        "        print(file.read())\n",
        "        print()\n",
        "\n",
        "# 학습 데이터를 읽어와서 데이터 프레임에 저장한다\n",
        "def directory_data(directory):\n",
        "    data = {}\n",
        "    data[\"review\"] = []\n",
        "    for file_path in os.listdir(directory):\n",
        "        with open(os.path.join(directory, file_path), \"r\", encoding='utf-8') as file:\n",
        "            data[\"review\"].append(file.read())\n",
        "            \n",
        "    return pd.DataFrame.from_dict(data)\n",
        "\n",
        "def data(directory):\n",
        "    pos_df = directory_data(os.path.join(directory, \"pos\"))\n",
        "    neg_df = directory_data(os.path.join(directory, \"neg\"))\n",
        "    pos_df[\"sentiment\"] = 1\n",
        "    neg_df[\"sentiment\"] = 0\n",
        "    \n",
        "    return pd.concat([pos_df, neg_df])\n",
        "\n",
        "train_df = data(os.path.join(os.path.dirname(data_set), \"aclImdb\", \"train\"))\n",
        "test_df = data(os.path.join(os.path.dirname(data_set), \"aclImdb\", \"test\"))\n",
        "train_df.head()\n",
        "reviews = list(train_df['review'])\n",
        "print(reviews[0])\n",
        "\n",
        "# 데이터 분석\n",
        "# 문자열 문장 리스트를 토큰나이즈\n",
        "tokenized_reviews = [r.split() for r in reviews]\n",
        "print(tokenized_reviews[0])\n",
        "\n",
        "# 토큰나이즈 된 리스트에 대한 각 길이를 저장\n",
        "review_len_by_token = [len(t) for t in tokenized_reviews]\n",
        "print(review_len_by_token[:10])\n",
        "\n",
        "# 토큰나이즈 된 것을 붙여서 음절의 길이를 저장 (문자 길이)\n",
        "review_len_by_eumjeol = [len(s.replace(' ', '')) for s in reviews]\n",
        "remove_space = [s.replace(' ', '') for s in reviews]\n",
        "print(remove_space[0])\n",
        "print(review_len_by_eumjeol[:20])\n",
        "\n",
        "# review 문장의 크기 분포를 확인한다. (단어 개수 분포, 문자 개수 분포)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 그래프에 대한 이미지 사이즈 선언\n",
        "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# 히스토그램 선언\n",
        "# bins: 히스토그램 값들에 대한 버켓 범위\n",
        "# range: x축 값의 범위\n",
        "# alpha: 그래프 색상 투명도\n",
        "# color: 그래프 색상\n",
        "# label: 그래프에 대한 라벨\n",
        "plt.hist(review_len_by_token, bins=50, alpha=0.5, color= 'r', label='word')\n",
        "plt.hist(review_len_by_eumjeol, bins=50, alpha=0.5, color='b', label='alphabet')\n",
        "plt.legend()\n",
        "plt.yscale('log', nonposy='clip')\n",
        "\n",
        "# 그래프 제목\n",
        "plt.title('Review Length Histogram')\n",
        "\n",
        "# 그래프 x 축 라벨\n",
        "plt.xlabel('Review Length')\n",
        "\n",
        "# 그래프 y 축 라벨\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()                          # 1\n",
        "\n",
        "# review 문서의 단어 분포의 기술 통계량을 확인한다\n",
        "import numpy as np\n",
        "\n",
        "print('문장 최대길이: {}'.format(np.max(review_len_by_token)))\n",
        "print('문장 최소길이: {}'.format(np.min(review_len_by_token)))\n",
        "print('문장 평균길이: {:.2f}'.format(np.mean(review_len_by_token)))\n",
        "print('문장 길이 표준편차: {:.2f}'.format(np.std(review_len_by_token)))\n",
        "print('문장 중간길이: {}'.format(np.median(review_len_by_token)))\n",
        "\n",
        "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
        "print('제 1 사분위 길이: {}'.format(np.percentile(review_len_by_token, 25)))\n",
        "print('제 3 사분위 길이: {}'.format(np.percentile(review_len_by_token, 75)))\n",
        "\n",
        "# 단어 분포를 박스 플롯으로 확인한다\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# 박스플롯 생성\n",
        "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
        "# labels: 입력한 데이터에 대한 라벨\n",
        "# showmeans: 평균값을 마크함\n",
        "\n",
        "plt.boxplot([review_len_by_token], labels=['token'], showmeans=True)\n",
        "plt.show()                               # 2 \n",
        "\n",
        "# 문자 분포를 박스 플롯으로 확인한다\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.boxplot([review_len_by_eumjeol], labels=['Eumjeol'], showmeans=True)\n",
        "plt.show()                               # 3\n",
        "\n",
        "# 워드 클라우드\n",
        "# conda install -c conda-forge wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = 'black', width = 800, height = 600)\\\n",
        "                .generate(' '.join(train_df['review']))\n",
        "\n",
        "plt.figure(figsize = (15, 10))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()                              # 4\n",
        "\n",
        "# 긍정 부정 분포\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentiment = train_df['sentiment'].value_counts()\n",
        "fig, axe = plt.subplots(ncols=1)\n",
        "fig.set_size_inches(6, 3)\n",
        "sns.countplot(train_df['sentiment'])\n",
        "plt.show()                              # 5\n",
        "\n",
        "print(sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6d2LHsO4jNY",
        "colab_type": "text"
      },
      "source": [
        "- output\n",
        "\n",
        "print(data_path)\n",
        "```\n",
        "C:\\Users\\student\\.keras\\datasets\n",
        "```\n",
        "print(aclimdb)\n",
        "```\n",
        "C:\\Users\\student\\.keras\\datasets\\aclimdb/test/neg\n",
        "```\n",
        "print(file_name[:20])\n",
        "\n",
        "```\n",
        "['0_2.txt', '10000_4.txt', '10001_1.txt', '10002_3.txt', '10003_3.txt', '10004_2.txt', '10005_2.txt', '10006_2.txt', '10007_4.txt', '10008_4.txt', '10009_3.txt', '1000_3.txt', '10010_2.txt', '10011_1.txt', '10012_1.txt', '10013_4.txt', '10014_2.txt', '10015_4.txt', '10016_3.txt', '10017_1.txt']\n",
        "```\n",
        "for file_path in file_name[:2]:\n",
        "    with open(os.path.join(aclimdb, file_path), \"r\", encoding='utf-8') as file:\n",
        "        print(file.read())\n",
        "        print()\n",
        "```\n",
        "Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\n",
        "\n",
        "This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying...<br /><br />Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\n",
        "```\n",
        "print(reviews[0])\n",
        "```\n",
        "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
        "```\n",
        "print(tokenized_reviews[0])\n",
        "```\n",
        "['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"Teachers\".', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', \"High's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"Teachers\".', 'The', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High.', 'A', 'classic', 'line:', 'INSPECTOR:', \"I'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'STUDENT:', 'Welcome', 'to', 'Bromwell', 'High.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched.', 'What', 'a', 'pity', 'that', 'it', \"isn't!\"]\n",
        "```\n",
        "print(review_len_by_token[:10])\n",
        "```\n",
        "[140, 428, 147, 124, 120, 171, 108, 340, 436, 324]\n",
        "```\n",
        "print(remove_space[0])\n",
        "```\n",
        "BromwellHighisacartooncomedy.Itranatthesametimeassomeotherprogramsaboutschoollife,suchas\"Teachers\".My35yearsintheteachingprofessionleadmetobelievethatBromwellHigh'ssatireismuchclosertorealitythanis\"Teachers\".Thescrambletosurvivefinancially,theinsightfulstudentswhocanseerightthroughtheirpatheticteachers'pomp,thepettinessofthewholesituation,allremindmeoftheschoolsIknewandtheirstudents.WhenIsawtheepisodeinwhichastudentrepeatedlytriedtoburndowntheschool,Iimmediatelyrecalled.........at..........High.Aclassicline:INSPECTOR:I'mheretosackoneofyourteachers.STUDENT:WelcometoBromwellHigh.IexpectthatmanyadultsofmyagethinkthatBromwellHighisfarfetched.Whatapitythatitisn't!\n",
        "```\n",
        "print(review_len_by_eumjeol[:20])\n",
        "```\n",
        "[667, 1939, 695, 540, 528, 847, 521, 1845, 2170, 1537, 1299, 388, 1348, 1027, 626, 718, 908, 1554, 631, 676]\n",
        "```\n",
        "\n",
        "- review 문서의 단어 분포의 기술 통계량을 확인한다\n",
        "import numpy as np\n",
        "\n",
        "print('문장 최대길이: {}'.format(np.max(review_len_by_token)))\n",
        "print('문장 최소길이: {}'.format(np.min(review_len_by_token)))\n",
        "print('문장 평균길이: {:.2f}'.format(np.mean(review_len_by_token)))\n",
        "print('문장 길이 표준편차: {:.2f}'.format(np.std(review_len_by_token)))\n",
        "print('문장 중간길이: {}'.format(np.median(review_len_by_token)))\n",
        "\n",
        "- 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
        "print('제 1 사분위 길이: {}'.format(np.percentile(review_len_by_token, 25)))\n",
        "print('제 3 사분위 길이: {}'.format(np.percentile(review_len_by_token, 75)))\n",
        "```\n",
        "문장 최대길이: 2470\n",
        "문장 최소길이: 10\n",
        "문장 평균길이: 233.79\n",
        "문장 길이 표준편차: 173.73\n",
        "문장 중간길이: 174.0   \n",
        "제 1 사분위 길이: 127.0\n",
        "제 3 사분위 길이: 284.0\n",
        "1    12500\n",
        "0    12500\n",
        "```\n",
        "print(sentiment)\n",
        "```\n",
        "Name: sentiment, dtype: int64\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QIEus5vEUer",
        "colab_type": "text"
      },
      "source": [
        "## plt.show() outputs\n",
        "\n",
        "1. ![#1](https://drive.google.com/uc?view=export&id=1GvsxRY7LjOiVdt8qmXZ3sTsA3z2G06f7)\n",
        "2. ![#2](https://drive.google.com/uc?view=export&id=1NyENSE_ERdnI7G1H-JqjiQgWvD7MsBwA)\n",
        "3. ![#3](https://drive.google.com/uc?view=export&id=1glf_BbLGRTUgQnM9gXOGkkyL2Oy9NVmE)\n",
        "4. ![#4](https://drive.google.com/uc?view=export&id=1glf_BbLGRTUgQnM9gXOGkkyL2Oy9NVmE)\n",
        "5. ![#5](https://drive.google.com/uc?view=export&id=1p4HyqMhxYic8mj9pzkMYiXn9uFMJj0e_)"
      ]
    }
  ]
}